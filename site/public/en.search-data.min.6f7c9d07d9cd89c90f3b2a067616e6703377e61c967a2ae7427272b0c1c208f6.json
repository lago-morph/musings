[{"id":0,"href":"/musings/docs/misc/agentic-computing-overview-and-tools/","title":"Agentic Computing Overview And Tools","section":"Miscellaneous","content":"Great question - this is a rapidly evolving space. Let me break down the concepts and tooling landscape.\nWhat \u0026ldquo;Agentic Computing\u0026rdquo; Means#\rTraditional software: You write explicit instructions for every step Agentic computing: You give an AI system a goal, and it figures out the steps\nKey characteristics of AI agents:\nAutonomy: Makes decisions without constant human input Tool use: Can call APIs, search databases, execute code Planning: Breaks down complex goals into subtasks Memory: Maintains context across interactions Iteration: Tries different approaches if initial attempts fail Think of it as the difference between:\nScript: \u0026ldquo;Download file ‚Üí parse JSON ‚Üí insert to database\u0026rdquo; Agent: \u0026ldquo;Get me the latest sales data into our dashboard\u0026rdquo; (it figures out how) Agent Architectures#\rReAct Pattern (Reasoning + Acting)#\rMost common agent loop:\nThought: Agent reasons about what to do next Action: Calls a tool/API Observation: Sees the result Repeat until goal achieved Planning Agents#\rCreate full plan upfront, then execute steps (less flexible but more predictable)\nMulti-Agent Systems#\rMultiple specialized agents collaborate:\nResearcher agent gathers info Analyst agent processes data Writer agent creates output Reflexive Agents#\rSimple stimulus-response without deep reasoning (lightweight, fast)\nAgent Components#\rLLM core: The \u0026ldquo;brain\u0026rdquo; doing reasoning (GPT-4, Claude, etc.)\nTools/Functions: Actions the agent can take (search web, query DB, send email, run code)\nMemory systems:\nShort-term: Conversation context Long-term: Vector databases for retrieval Episodic: Past interaction history Orchestration: The loop controlling agent execution\nGuardrails: Constraints on what agents can/can\u0026rsquo;t do\nEvaluation Dimensions for Agent Tools#\rBased on your DevOps background, here\u0026rsquo;s how I\u0026rsquo;d evaluate agent platforms:\n1. Visibility \u0026amp; Debugging#\rCan you see the agent\u0026rsquo;s reasoning chain? Trace tool calls and responses? Inspect decision points? Replay failed runs? Best-in-class: LangSmith (LangChain\u0026rsquo;s observability platform), Weights \u0026amp; Biases for LLMs\n2. Development Experience#\rVisual vs code-first? How easy to add custom tools? Testing framework? Version control integration? 3. Deployment \u0026amp; Operations#\rSelf-hosted vs cloud? Scaling model (per-execution, always-on)? API access patterns? Cost predictability? 4. Reliability \u0026amp; Control#\rError handling mechanisms? Timeout/retry logic? Human-in-the-loop checkpoints? Determinism vs creativity trade-offs? 5. Integration Ecosystem#\rPre-built tool libraries? Custom tool SDK quality? Data connector availability? Tool Landscape by Use Case#\rFor Learning/Experimentation#\rLangChain/LangGraph (Python/JS)\nPros: Most mature ecosystem, great docs, extensive tooling Cons: Can be complex, lots of abstraction layers Visualization: LangSmith for tracing Good for: Understanding agent patterns deeply AutoGen (Python)\nPros: Multi-agent focus, good for research Cons: Less production-ready, Microsoft-centric Visualization: Built-in logging, GroupChat UI Good for: Agent collaboration experiments Flowise/LangFlow\nPros: Visual builder, instant feedback, low barrier Cons: Less control, harder to version control Visualization: Built-in flow diagram is the visualization Good for: Quick prototyping, non-coders For Production Deployment#\rTemporal + LLM\nPros: Bulletproof reliability, durable execution, great debugging Cons: More DevOps overhead, not AI-native Visualization: Temporal UI shows workflow state Good for: Mission-critical agent workflows LangGraph Cloud\nPros: Deployment abstraction, built-in observability Cons: Vendor lock-in, newer product Visualization: LangSmith integration Good for: Teams already using LangChain Custom orchestration (Inngest, Windmill, etc.)\nPros: Full control, fits your existing stack Cons: Build your own agent patterns Visualization: Platform-dependent Good for: DevOps teams comfortable with infrastructure For Low-Code/Business Users#\rn8n with AI nodes\nPros: Familiar workflow paradigm, self-hostable Cons: Not purpose-built for complex agents Visualization: Workflow canvas Good for: Deterministic workflows with AI steps Stack AI, Relevance AI\nPros: Agent-specific UI, templates Cons: Less customization, cloud-only Visualization: Built-in trace viewers Good for: Quick deployment, less technical teams Debugging \u0026amp; Visualization Tools#\rObservability platforms:\nLangSmith: Trace LangChain agents, evaluate outputs Weights \u0026amp; Biases: Track experiments, compare agent runs Helicone: LLM observability, caching, logging Arize AI: Monitor production LLM apps Development tools:\nLangGraph Studio: Visual debugger for LangGraph agents Promptfoo: Test/evaluate agent prompts systematically OpenLIT: Open-source LLM observability Evaluation frameworks:\nRAGAS: Evaluate RAG pipelines DeepEval: Unit tests for LLM outputs TruLens: Feedback functions for agent quality Recommended Learning Path#\rGiven your technical background:\nStart with LangChain tutorial - understand ReAct pattern hands-on Build simple agent - web search + calculator tools Add LangSmith - see what tracing/debugging looks like Try LangGraph - for more complex stateful agents Experiment with multi-agent - AutoGen or CrewAI Consider production patterns - how would you deploy this in your DevOps context? Key Questions for Your Use Case#\rDeterministic vs autonomous: Do you need predictable steps or creative problem-solving? Human oversight: How much do you trust the agent to act independently? Failure tolerance: What happens if the agent makes a wrong decision? Scale: One-off tasks vs high-volume operations? For DevOps specifically, promising agent applications:\nIncident response: Gather logs, analyze patterns, suggest fixes Infrastructure optimization: Analyze metrics, recommend changes Documentation generation: From code/configs to human-readable docs Automated troubleshooting: Debug issues across distributed systems What kind of agentic workflow are you thinking about building? That would help me point you to the most relevant tools and patterns.\n"},{"id":1,"href":"/musings/docs/misc/agno-vs-langchain/","title":"Agno Vs Langchain","section":"Miscellaneous","content":"Great! Now let me fetch the actual README.md file:I can see the README content embedded in the GitHub page I fetched. Based on what I can see from your repository, here\u0026rsquo;s my updated analysis:\nUpdated Framework Recommendation#\rYour project is a conference data extraction and analysis system that:\nExtracts presentation data from CNCF KubeCon conferences (Sched.com) Uses a 4-task architecture (Discovery, Extraction, AI Processing, Issue Resolution) Needs web scraping, AI-powered content analysis, and quality assurance Plans for future analytics and trend analysis Given this context, here\u0026rsquo;s my revised comparison:\nLangChain/LangGraph ‚úÖ RECOMMENDED#\rWhy it\u0026rsquo;s the best fit:\nYour 4-task architecture maps perfectly to LangGraph\u0026rsquo;s stateful workflow design Excellent document loaders for YouTube transcripts, PDFs, PPTX files Built-in retry/error handling crucial for web scraping reliability LangSmith observability helps debug the complex multi-task pipeline Strong RAG capabilities for future trend analysis features Production-ready for the scale you need (multiple conferences, thousands of presentations) Specific advantages for your use case:\nCheckpoint/resume capabilities essential for long-running extraction tasks Graph-based orchestration handles your 4 independent tasks elegantly Rich ecosystem for content extraction and processing Works well with YAML storage format you\u0026rsquo;re using Agno#\rWhy it\u0026rsquo;s less suitable:\nToo new/untested for a production data extraction pipeline Lacks mature document processing integrations you need Insufficient error recovery patterns for web scraping Smaller ecosystem means you\u0026rsquo;d build more from scratch Alternative: AutoGen#\rPros:\nMulti-agent design aligns with your 4-task architecture Good for collaborative problem-solving (troubleshooting agents) Microsoft backing means good stability Cons:\nLess mature document processing than LangChain More research-oriented, less production tooling Steeper learning curve for practical workflows Final Recommendation#\rFor your conference-summaries project: Use LangChain/LangGraph\nThe combination of robust document processing, stateful workflow orchestration, production-ready error handling, and observability tools makes it the clear choice for a data extraction and analysis pipeline at your scale.\n"},{"id":2,"href":"/musings/docs/misc/serverless-function-workflow-tools-for-kubernetes/","title":"Core Concepts","section":"Miscellaneous","content":"Serverless/Function Workflows on Kubernetes#\rThis is about running event-driven, scale-to-zero workloads where functions are triggered by events and orchestrated into workflows. Great for cost efficiency and dynamic scaling.\nCore Concepts#\rTraditional workflows: Always-running services waiting for work Serverless workflows: Functions that:\nSpin up on-demand Execute and terminate Scale to zero when idle Pay only for execution time Perfect for: Bursty workloads, event-driven agents, API endpoints with variable traffic\nMain Players in K8s#\rKnative ‚≠ê Industry Standard#\r# What it is K8s-native serverless platform (Google-backed, now CNCF)Architecture:\nKnative Serving: HTTP-triggered functions, auto-scaling Knative Eventing: Event-driven workflows via pub/sub License: Apache 2.0 Installation:\n# Install Knative Serving kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.12.0/serving-crds.yaml kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.12.0/serving-core.yaml # Install Knative Eventing kubectl apply -f https://github.com/knative/eventing/releases/download/knative-v1.12.0/eventing-crds.yaml kubectl apply -f https://github.com/knative/eventing/releases/download/knative-v1.12.0/eventing-core.yamlSimple Function Example:\napiVersion: serving.knative.dev/v1 kind: Service metadata: name: agent-function spec: template: metadata: annotations: autoscaling.knative.dev/minScale: \u0026#34;0\u0026#34; # Scale to zero autoscaling.knative.dev/maxScale: \u0026#34;10\u0026#34; spec: containers: - image: my-agent:latest env: - name: ANTHROPIC_API_KEY valueFrom: secretKeyRef: name: api-keys key: anthropicWorkflow Example with Eventing:\n# Event source (e.g., Kafka, SQS, webhooks) apiVersion: sources.knative.dev/v1 kind: ApiServerSource metadata: name: agent-trigger spec: serviceAccountName: agent-sa mode: Resource resources: - apiVersion: v1 kind: Event sink: ref: apiVersion: serving.knative.dev/v1 kind: Service name: agent-function --- # Chain functions with sequences apiVersion: flows.knative.dev/v1 kind: Sequence metadata: name: agent-pipeline spec: steps: - ref: apiVersion: serving.knative.dev/v1 kind: Service name: gather-context - ref: apiVersion: serving.knative.dev/v1 kind: Service name: run-llm - ref: apiVersion: serving.knative.dev/v1 kind: Service name: format-response reply: ref: kind: Service apiVersion: serving.knative.dev/v1 name: result-handlerStrengths:\nMature - production-ready, widely adopted Portable - runs on any K8s (GKE, EKS, AKS, on-prem) Scale-to-zero - true pay-per-use Event-driven - comprehensive eventing framework HTTP focus - great for API endpoints Weaknesses:\nComplex setup (needs Istio or similar for routing) Learning curve for eventing model Cold start latency (seconds) Best for: HTTP APIs, webhook handlers, event-driven microservices\nAgent workflows: Good for stateless agents triggered by events\nOpenFaaS ‚≠ê Developer Friendly#\r# What it is Simple functions-as-a-service on K8sArchitecture:\nGateway: Routes requests to functions Watchdog: Wraps your code as HTTP service Prometheus: Built-in metrics License: MIT (CE), proprietary (Pro) Installation:\n# Using arkade (easiest) arkade install openfaas # Or Helm helm repo add openfaas https://openfaas.github.io/faas-netes/ helm install openfaas openfaas/openfaas \\ --namespace openfaas \\ --set functionNamespace=openfaas-fnFunction Example:\n# stack.yml version: 1.0 provider: name: openfaas gateway: http://gateway.openfaas:8080 functions: agent: lang: python3 handler: ./agent image: myregistry/agent:latest environment: ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY} limits: memory: 512Mi requests: memory: 256Mi annotations: com.openfaas.scale.min: \u0026#34;0\u0026#34; com.openfaas.scale.max: \u0026#34;5\u0026#34;Python Handler:\n# handler.py import anthropic def handle(req): \u0026#34;\u0026#34;\u0026#34;Handle function invocation\u0026#34;\u0026#34;\u0026#34; client = anthropic.Anthropic() response = client.messages.create( model=\u0026#34;claude-sonnet-4-20250514\u0026#34;, max_tokens=1024, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: req}] ) return response.content[0].textDeploy:\nfaas-cli build -f stack.yml faas-cli push -f stack.yml faas-cli deploy -f stack.ymlInvoke:\necho \u0026#34;Explain quantum computing\u0026#34; | faas-cli invoke agentWorkflow Chaining:\n# Use function composition faas-cli invoke gather-context | \\ faas-cli invoke process-llm | \\ faas-cli invoke format-outputStrengths:\nSimple - easiest to get started CLI-driven - great developer experience Template library - many languages supported Async support - queue-based invocation Metrics - Prometheus built-in Weaknesses:\nLess sophisticated than Knative Simpler eventing model Smaller ecosystem Best for: Quick function deployment, simple workflows, learning serverless\nAgent workflows: Great for simple agent APIs, less ideal for complex orchestration\nFission#\r# What it is Fast serverless functions on K8sArchitecture:\nExecutor: Manages function pods Router: HTTP routing Environments: Pre-warmed containers License: Apache 2.0 Installation:\nkubectl create -k \u0026#34;github.com/fission/fission/crds/v1?ref=v1.20.0\u0026#34; helm install fission fission-charts/fission-all \\ --namespace fissionFunction Example:\n# Create environment (Python) fission env create --name python --image fission/python-env # Create function fission function create \\ --name agent \\ --env python \\ --code agent.py \\ --minscale 0 \\ --maxscale 10Python Function:\n# agent.py from flask import request import anthropic def main(): body = request.get_data(as_text=True) client = anthropic.Anthropic() response = client.messages.create( model=\u0026#34;claude-sonnet-4-20250514\u0026#34;, max_tokens=1024, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: body}] ) return response.content[0].textWorkflow Support:\n# Fission Workflow CRD apiVersion: fission.io/v1 kind: Workflow metadata: name: agent-pipeline spec: apiVersion: v1 tasks: gather: functionRef: gather-context process: requires: [gather] functionRef: run-agent format: requires: [process] functionRef: format-resultStrengths:\nFast cold starts - pool of warm containers Workflow support - built-in workflow engine Triggers - HTTP, time, message queues Simplicity - easier than Knative Weaknesses:\nSmaller community than Knative/OpenFaaS Less active development recently Limited cloud integration Best for: Fast cold-start requirements, workflow-heavy use cases\nAgent workflows: Good - built-in workflow engine helps\nNuclio (Data-Focused)#\r# What it is High-performance serverless for real-time/data workloadsLicense: Apache 2.0 (iguazio-backed)\nInstallation:\nkubectl apply -f https://raw.githubusercontent.com/nuclio/nuclio/master/hack/k8s/resources/nuclio-rbac.yaml kubectl apply -f https://raw.githubusercontent.com/nuclio/nuclio/master/hack/k8s/resources/nuclio.yamlFunction Example:\n# function.yaml apiVersion: nuclio.io/v1 kind: NuclioFunction metadata: name: agent-function spec: runtime: python:3.9 handler: main:handler minReplicas: 0 maxReplicas: 10 triggers: http: kind: http attributes: port: 8080Python Handler:\n# main.py def handler(context, event): import anthropic client = anthropic.Anthropic() response = client.messages.create( model=\u0026#34;claude-sonnet-4-20250514\u0026#34;, max_tokens=1024, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: event.body}] ) return response.content[0].textStrengths:\nPerformance - designed for real-time Data triggers - Kafka, Kinesis, etc. GPU support - for ML workloads Built-in data sources - databases, streams Weaknesses:\nLess general-purpose than others Smaller community Data/ML focus might be overkill Best for: Real-time data processing, ML inference, streaming\nAgent workflows: Good if agents process data streams\nWorkflow Orchestration for Serverless#\rKnative Workflows#\rUses Sequences and Parallels:\n# Sequential workflow apiVersion: flows.knative.dev/v1 kind: Sequence metadata: name: agent-sequence spec: steps: - ref: apiVersion: serving.knative.dev/v1 kind: Service name: step1 - ref: apiVersion: serving.knative.dev/v1 kind: Service name: step2 reply: ref: apiVersion: serving.knative.dev/v1 kind: Service name: result-handler# Parallel workflow apiVersion: flows.knative.dev/v1 kind: Parallel metadata: name: agent-parallel spec: branches: - subscriber: ref: apiVersion: serving.knative.dev/v1 kind: Service name: branch1 - subscriber: ref: apiVersion: serving.knative.dev/v1 kind: Service name: branch2 reply: ref: apiVersion: serving.knative.dev/v1 kind: Service name: aggregator Argo Events + Knative#\rCombine Argo Events (event sources) with Knative (functions):\n# Event source apiVersion: argoproj.io/v1alpha1 kind: EventSource metadata: name: webhook spec: webhook: agent-trigger: port: \u0026#34;12000\u0026#34; endpoint: /agent method: POST --- # Sensor (workflow trigger) apiVersion: argoproj.io/v1alpha1 kind: Sensor metadata: name: agent-sensor spec: dependencies: - name: webhook-dep eventSourceName: webhook eventName: agent-trigger triggers: - template: name: knative-trigger knative: service: name: agent-function Temporal + Serverless Functions#\rTemporal orchestrates, functions execute:\n# Temporal workflow calling serverless functions @workflow.defn class AgentWorkflow: @workflow.run async def run(self, query: str): # Call Knative function via HTTP context = await workflow.execute_activity( call_knative_function, args=[\u0026#34;gather-context\u0026#34;, query] ) result = await workflow.execute_activity( call_knative_function, args=[\u0026#34;run-agent\u0026#34;, context] ) return result @activity.defn async def call_knative_function(func_name: str, payload: str): import httpx async with httpx.AsyncClient() as client: response = await client.post( f\u0026#34;http://{func_name}.default.svc.cluster.local\u0026#34;, json={\u0026#34;data\u0026#34;: payload} ) return response.json() Agent Workflow Patterns#\rPattern 1: Simple Request/Response Agent#\r# Single Knative function apiVersion: serving.knative.dev/v1 kind: Service metadata: name: chatbot spec: template: spec: containers: - image: my-chatbot:latest env: - name: MODEL value: claude-sonnet-4-20250514Use case: Chatbot API, simple Q\u0026amp;A\nPattern 2: Multi-Step Agent Pipeline#\r# Knative Sequence apiVersion: flows.knative.dev/v1 kind: Sequence metadata: name: research-agent spec: steps: - ref: kind: Service name: web-search # Search web - ref: kind: Service name: extract-info # Extract key info - ref: kind: Service name: synthesize # LLM synthesis - ref: kind: Service name: format-report # Format outputUse case: Research agent, multi-step processing\nPattern 3: Parallel Agent Tasks#\r# Knative Parallel apiVersion: flows.knative.dev/v1 kind: Parallel metadata: name: multi-source-agent spec: branches: - subscriber: ref: kind: Service name: search-web - subscriber: ref: kind: Service name: query-database - subscriber: ref: kind: Service name: call-api reply: ref: kind: Service name: aggregate-results # Combine all sourcesUse case: Agent gathering info from multiple sources simultaneously\nPattern 4: Event-Driven Agent#\r# Triggered by external events apiVersion: sources.knative.dev/v1 kind: KafkaSource metadata: name: support-tickets spec: bootstrapServers: - kafka:9092 topics: - support-tickets sink: ref: apiVersion: serving.knative.dev/v1 kind: Service name: ticket-agent # Agent processes each ticketUse case: Customer support agent triggered by new tickets\nCost \u0026amp; Performance Comparison#\rPlatform Cold Start Warm Latency Scale-to-Zero Learning Curve Knative 1-3s \u0026lt;50ms ‚úÖ Yes Medium-High OpenFaaS 1-2s \u0026lt;100ms ‚úÖ Yes Low Fission 100-500ms \u0026lt;50ms ‚úÖ Yes Medium Nuclio \u0026lt;100ms \u0026lt;20ms ‚úÖ Yes Medium When to Use Serverless Functions#\rGood Fit:#\r‚úÖ Bursty, unpredictable traffic ‚úÖ Event-driven workloads ‚úÖ Cost sensitivity (pay per use) ‚úÖ Stateless operations ‚úÖ API endpoints with variable load ‚úÖ Webhooks, message processing\nPoor Fit:#\r‚ùå Consistent high traffic (always-on cheaper) ‚ùå Long-running tasks (cold start overhead) ‚ùå Stateful workflows (needs external state) ‚ùå Low-latency requirements (\u0026lt;100ms) ‚ùå Complex orchestration (better tools exist)\nAgent-Specific Recommendations#\rFor AI Agents, Serverless Works When:#\r‚úÖ Simple agents (single LLM call, return response)\nUse: OpenFaaS or Knative Service Pattern: Direct HTTP invocation ‚úÖ Event-driven agents (triggered by events)\nUse: Knative Eventing Pattern: Kafka/webhook ‚Üí agent function ‚úÖ Parallel processing (multiple agents working simultaneously)\nUse: Knative Parallel Pattern: Fan-out to multiple agents, aggregate results ‚ùå Complex multi-step agents (ReAct loops, planning)\nProblem: Cold starts kill latency Better: Temporal + always-on workers ‚ùå Long-running agents (tasks taking minutes/hours)\nProblem: Timeout limits, cost inefficiency Better: Argo Workflows or Temporal Hybrid Approach (Recommended)#\rCombine serverless + traditional orchestration:\n# Temporal workflow orchestrating Knative functions Temporal Workflow (always-on, stateful) ‚Üì Calls Knative Function (scales to zero) ‚Üí Quick LLM operations ‚Üì Calls Knative Function ‚Üí Data processing ‚Üì Returns to TemporalBenefits:\nTemporal handles state/reliability Knative handles bursty compute Cost efficiency + reliability My Recommendation for You#\rGiven your DevOps/K8s background:\nStart with: Knative#\rIndustry standard, portable Integrates with ArgoCD (GitOps) Mature, production-ready Good documentation Quick Win: OpenFaaS#\rEasiest to learn serverless concepts Great for experimentation Simple deployment model For Agents Specifically:#\rSimple agent API ‚Üí Knative Service (scale-to-zero)\rMulti-step workflow ‚Üí Temporal + Knative (hybrid)\rEvent-driven ‚Üí Knative Eventing\rHigh-performance ‚Üí Nuclio (if data-heavy)Avoid for Agents:#\rPure serverless for complex ReAct loops (cold start kills UX) Serverless for long-running research tasks (use Argo/Temporal) Want me to walk through deploying a specific agent pattern on Knative? I can show you the full setup including eventing and observability.\n"},{"id":3,"href":"/musings/docs/","title":"Documentation","section":"Technology Documentation Hub","content":"Technology Documentation Hub#\rWelcome to our comprehensive technology documentation portal. This site contains curated information about various aspects of modern technology infrastructure, development practices, and operational procedures.\nNavigation#\rUse the sidebar to browse documentation by category, or use the search function to find specific topics.\nCategories#\rOur documentation is organized into the following main categories:\nAI \u0026amp; Machine Learning: Information about AI agents, tools, and frameworks Development Platforms: Documentation platforms, GitOps, and development workflows Infrastructure: Kubernetes, Helm, production readiness guides Workflows: Orchestration tools and automation patterns Browse the sections to find detailed information on each topic.\n"},{"id":4,"href":"/musings/docs/misc/flowwise-vs-langflow/","title":"Flowise vs Langflow: Detailed Comparison","section":"Miscellaneous","content":"Flowise vs Langflow: Detailed Comparison#\rBoth are open-source, self-hosted visual builders for LangChain workflows. They\u0026rsquo;re very similar in concept but have important differences.\nQuick Summary#\rFlowise: More polished UI, easier for beginners, Node.js-based Langflow: More customizable, Python-native, better for developers\nCore Architecture#\rFlowise#\rBackend: Node.js (TypeScript) Frontend: React LangChain: Uses LangChain.js (JavaScript/TypeScript version) Database: SQLite by default, supports PostgreSQL, MySQL API: RESTful + webhooks Langflow#\rBackend: Python (FastAPI) Frontend: React (TypeScript) LangChain: Uses LangChain Python (the original) Database: SQLite by default, can use PostgreSQL API: RESTful + Python SDK Key implication: If your team is Python-heavy, Langflow integrates better. If you prefer Node.js ecosystems, Flowise fits cleaner.\nUser Interface \u0026amp; Experience#\rFlowise#\rCleaner, more polished UI Canvas is easier to navigate Better visual feedback during execution More intuitive node configuration Pre-built templates are well-organized Learning curve: Lower - feels like a product Langflow#\rMore developer-oriented UI Canvas can feel cluttered with complex flows More configuration options exposed Component sidebar has more detail Learning curve: Steeper - feels like a developer tool Winner for UX: Flowise (unless you want maximum control)\nFeatures Comparison#\rFeature Flowise Langflow Drag-drop builder ‚úÖ Excellent ‚úÖ Good Pre-built agents ‚úÖ Many templates ‚úÖ Growing library Custom components ‚ö†Ô∏è Requires JS/TS ‚úÖ Native Python Streaming responses ‚úÖ Yes ‚úÖ Yes API generation ‚úÖ Auto per flow ‚úÖ Auto per flow Embeddings ‚úÖ Full support ‚úÖ Full support Vector stores ‚úÖ 10+ options ‚úÖ 10+ options Memory types ‚úÖ Multiple ‚úÖ Multiple Chat history ‚úÖ Built-in UI ‚úÖ Built-in UI Multi-user ‚úÖ User management ‚úÖ User management Versioning ‚ö†Ô∏è Basic ‚ö†Ô∏è Basic Export flows ‚úÖ JSON format ‚úÖ JSON format Agent Capabilities#\rFlowise#\rAgent types: ReAct, Conversational, OpenAI Functions Tool support: Good pre-built library Custom tools: Write in JavaScript/TypeScript Multi-agent: Possible but not primary focus State management: Standard LangChain patterns Langflow#\rAgent types: ReAct, Conversational, Custom (easier to extend) Tool support: Extensive (entire Python LangChain ecosystem) Custom tools: Write in Python (more natural for data science teams) Multi-agent: Better support via Python libraries State management: Full LangChain Python capabilities Winner for complex agents: Langflow (Python ecosystem advantage)\nCustomization \u0026amp; Extensibility#\rFlowise#\r// Custom tool example - requires Node.js knowledge class CustomTool extends Tool { name = \u0026#34;my_tool\u0026#34;; async _call(input: string): Promise\u0026lt;string\u0026gt; { // Your logic } } Requires packaging as npm module Less common for AI/ML teams Good if already in Node.js stack Langflow#\r# Custom component - native Python from langflow import CustomComponent class MyCustomTool(CustomComponent): def build(self): # Your logic using any Python library return tool Drop Python files in components folder Access to entire PyPI ecosystem Natural for data scientists/ML engineers Winner for customization: Langflow (Python is lingua franca of AI)\nDeployment \u0026amp; Operations#\rFlowise#\rInstallation:\n# Docker (easiest) docker run -p 3000:3000 flowiseai/flowise # npm npm install -g flowise npx flowise start # From source git clone https://github.com/FlowiseAI/Flowise cd Flowise \u0026amp;\u0026amp; npm install \u0026amp;\u0026amp; npm run build \u0026amp;\u0026amp; npm startResource usage: Generally lighter (Node.js efficiency)\nScaling: Horizontal scaling straightforward\nMonitoring: Standard Node.js tooling (PM2, New Relic, etc.)\nLangflow#\rInstallation:\n# Docker docker run -p 7860:7860 langflowai/langflow # pip pip install langflow langflow run # From source git clone https://github.com/logspace-ai/langflow cd langflow \u0026amp;\u0026amp; make install \u0026amp;\u0026amp; make runResource usage: Heavier (Python runtime + dependencies)\nScaling: Can scale but Python GIL considerations\nMonitoring: Standard Python tooling (Gunicorn, Prometheus)\nWinner for ops: Flowise (lighter, easier to containerize)\nIntegration Ecosystem#\rFlowise#\rLLM providers: OpenAI, Anthropic, HuggingFace, Cohere, Azure, AWS Bedrock, local models Vector DBs: Pinecone, Qdrant, Weaviate, Milvus, Chroma, Supabase, Redis Tools: ~50 pre-built (web search, calculators, APIs, databases) Limitations: Constrained to LangChain.js ecosystem\nLangflow#\rLLM providers: Same as Flowise plus easier custom additions Vector DBs: Same as Flowise Tools: 100+ because it\u0026rsquo;s full Python LangChain Advantage: Any Python library becomes a tool (pandas, requests, scikit-learn, etc.)\nWinner for integrations: Langflow (Python ecosystem breadth)\nObservability \u0026amp; Debugging#\rFlowise#\rExecution logs: Good visibility in UI Intermediate outputs: Can inspect each node Error handling: Clear error messages LangSmith integration: Yes, but separate setup Custom logging: Standard Node.js logging Langflow#\rExecution logs: Detailed Python stack traces Intermediate outputs: Full variable inspection Error handling: Python exception details LangSmith integration: Native (same ecosystem) Custom logging: Python logging module Winner: Tie (both adequate, different strengths)\nCommunity \u0026amp; Ecosystem#\rFlowise#\rGitHub stars: ~30k Active development: Very active Community: Strong Discord, good docs Templates: Growing marketplace Commercial support: Flowise Cloud (optional SaaS) Langflow#\rGitHub stars: ~35k Active development: Very active (DataStax backing) Community: Active Discord, improving docs Templates: Hub with shareable flows Commercial support: DataStax Langflow (enterprise version) Winner: Tie (both have strong communities)\nUse Case Recommendations#\rChoose Flowise if:#\rYou want the easiest onboarding experience Your team prefers TypeScript/JavaScript You need lightweight deployment You\u0026rsquo;re building simpler agent workflows You want a more \u0026ldquo;finished\u0026rdquo; product feel Your DevOps stack is Node.js-based Choose Langflow if:#\rYour team is Python-native You need extensive customization You want to leverage Python ML libraries directly You\u0026rsquo;re building complex data processing pipelines You need tighter LangChain ecosystem integration Your data scientists will be building flows Real-World Scenarios#\rScenario 1: RAG chatbot for documentation#\rEither works well - this is the sweet spot for both\nFlowise: Slightly easier setup Langflow: Better if you need custom document processing Scenario 2: Multi-agent research system#\rLangflow - Python ecosystem advantage\nAccess to specialized agents (AutoGen, CrewAI patterns) Easier to integrate research libraries Scenario 3: Customer service automation#\rFlowise - production-ready feel\nCleaner UI for non-technical stakeholders to view Lighter resource footprint for scale Scenario 4: Data enrichment pipeline#\rLangflow - Python data tools\nNative pandas, numpy integration Better for ETL + AI hybrid workflows My Recommendation for You#\rGiven your DevOps background:\nStart with Flowise for these reasons:\nFaster to value - you\u0026rsquo;ll have something working in 30 minutes Lighter ops burden - fits K8s deployments cleaner Good enough customization - unless you need heavy Python libraries Better for demos - when showing stakeholders Switch to Langflow if:\nYou find yourself fighting JavaScript for custom components Your team is primarily Python developers You need specific Python ML libraries in your agent workflows Quick Hands-On Test#\rTry both with this exercise:\nBuild a simple agent that:\nTakes a question about your company Searches internal docs (vector store) Generates answer with citations Logs the interaction Time yourself: Which one gets you to working prototype faster?\nFor 80% of use cases, they\u0026rsquo;re functionally equivalent. The decision comes down to:\nTeam skills (Python vs JavaScript) Customization needs (how much will you extend it?) UX preference (developer tool vs polished product) Want me to walk through setting up a specific agent workflow in one of them so you can see the actual development experience?\n"},{"id":5,"href":"/musings/docs/misc/one-model-multiple-diagrams/","title":"Generate all views","section":"Miscellaneous","content":"Perfect! Now I have comprehensive information to give you a complete answer with practical examples.\nComplete Solution: Central Model with Selective Views in PlantUML#\rGreat question! This is exactly how you should structure PlantUML for maintainable domain modeling. Here\u0026rsquo;s the complete approach:\nFile Structure üìÅ#\rproject/\r‚îú‚îÄ‚îÄ model/\r‚îÇ ‚îú‚îÄ‚îÄ classes.iuml # All class definitions\r‚îÇ ‚îî‚îÄ‚îÄ relationships.iuml # All relationships (optional)\r‚îú‚îÄ‚îÄ views/\r‚îÇ ‚îú‚îÄ‚îÄ view-all.puml # Diagram: A, B, C\r‚îÇ ‚îú‚îÄ‚îÄ view-ab.puml # Diagram: A, B only\r‚îÇ ‚îî‚îÄ‚îÄ view-bc.puml # Diagram: B, C only\r‚îî‚îÄ‚îÄ exports/\r‚îú‚îÄ‚îÄ svg/\r‚îî‚îÄ‚îÄ png/ Method 1: Include + Hide/Remove ‚≠ê Recommended#\rThis is the most powerful approach using PlantUML\u0026rsquo;s hide and remove commands.\nStep 1: Define All Classes in Central File#\rmodel/classes.iuml:\n@startuml\r\u0026#39; Central class definitions\rclass A {\r- idA: int\r- nameA: String\r+ methodA(): void\r}\rclass B {\r- idB: int\r- dataB: String\r+ methodB(): void\r+ processData(): String\r}\rclass C {\r- idC: int\r- valueC: double\r+ methodC(): void\r+ calculate(): double\r}\r\u0026#39; Define all relationships\rA --\u0026gt; B : uses\rB --\u0026gt; C : transforms\r@endumlStep 2: Create Selective Views#\rviews/view-all.puml (Shows A, B, C):\n@startuml\r!include ../model/classes.iuml\r\u0026#39; No hiding - shows everything\r@endumlviews/view-ab.puml (Shows only A, B):\n@startuml\r!include ../model/classes.iuml\r\u0026#39; Hide class C\rhide C\r@endumlviews/view-bc.puml (Shows only B, C):\n@startuml\r!include ../model/classes.iuml\r\u0026#39; Hide class A\rhide A\r@endumlThe hide command is useful when you define a large included file and want to hide some classes after inclusion. You can also use remove instead of hide - the difference is that remove completely removes the element from memory, while hide just makes it invisible.\nMethod 2: Using Tags for Complex Filtering üè∑Ô∏è#\rFor more sophisticated scenarios with many classes, use tags:\nmodel/classes-tagged.iuml:\n@startuml\rclass A $viewAB $viewAll {\r- idA: int\r- nameA: String\r+ methodA(): void\r}\rclass B $viewAB $viewBC $viewAll {\r- idB: int\r- dataB: String\r+ methodB(): void\r}\rclass C $viewBC $viewAll {\r- idC: int\r- valueC: double\r+ methodC(): void\r}\rA --\u0026gt; B : uses $viewAB $viewAll\rB --\u0026gt; C : transforms $viewBC $viewAll\r@endumlviews/view-ab-tagged.puml:\n@startuml\r!include ../model/classes-tagged.iuml\r\u0026#39; Show only elements tagged with $viewAB\rhide @unlinked\rremove $viewBC\rshow $viewAB\r@endumlYou can put $tags on elements, then remove, hide or restore components either individually or by tags.\nMethod 3: Using Named Blocks üì¶#\rInclude specific sections using !startsub and !endsub:\nmodel/classes-sections.iuml:\n@startuml\r!startsub CLASS_A\rclass A {\r- idA: int\r+ methodA(): void\r}\r!endsub\r!startsub CLASS_B\rclass B {\r- idB: int\r+ methodB(): void\r}\r!endsub\r!startsub CLASS_C\rclass C {\r- idC: int\r+ methodC(): void\r}\r!endsub\r!startsub REL_AB\rA --\u0026gt; B : uses\r!endsub\r!startsub REL_BC\rB --\u0026gt; C : transforms\r!endsub\r@endumlviews/view-ab-subsections.puml:\n@startuml\r!includesub ../model/classes-sections.iuml!CLASS_A\r!includesub ../model/classes-sections.iuml!CLASS_B\r!includesub ../model/classes-sections.iuml!REL_AB\r@endumlYou can use !startsub NAME and !endsub to indicate sections of text to include from other files using !includesub.\nMethod 4: Hybrid Approach with Packages üìö#\rFor large domain models, organize by package:\nmodel/domain-model.iuml:\n@startuml\rpackage \u0026#34;Core Domain\u0026#34; {\rclass A {\r- idA: int\r+ methodA(): void\r}\rclass B {\r- idB: int\r+ methodB(): void\r}\r}\rpackage \u0026#34;Extended Domain\u0026#34; {\rclass C {\r- idC: int\r+ methodC(): void\r}\r}\rA --\u0026gt; B\rB --\u0026gt; C\r@endumlviews/view-core-only.puml:\n@startuml\r!include ../model/domain-model.iuml\r\u0026#39; Hide entire package\rhide \u0026#34;Extended Domain\u0026#34;\r@enduml Advanced: Conditional Compilation with Macros üîß#\rFor maximum flexibility, use preprocessor variables:\nviews/view-ab-conditional.puml:\n@startuml\r!$SHOW_A = %true()\r!$SHOW_B = %true()\r!$SHOW_C = %false()\r!include ../model/classes-conditional.iuml\r@endumlmodel/classes-conditional.iuml:\n@startuml\r!if $SHOW_A\rclass A {\r- idA: int\r+ methodA(): void\r}\r!endif\r!if $SHOW_B\rclass B {\r- idB: int\r+ methodB(): void\r}\r!endif\r!if $SHOW_C\rclass C {\r- idC: int\r+ methodC(): void\r}\r!endif\r!if $SHOW_A \u0026amp;\u0026amp; $SHOW_B\rA --\u0026gt; B : uses\r!endif\r!if $SHOW_B \u0026amp;\u0026amp; $SHOW_C\rB --\u0026gt; C : transforms\r!endif\r@enduml Best Practice Recommendations üí°#\rFor Your Use Case (3 objects, 3 views):#\rI recommend Method 1 (Include + Hide) because:\n‚úÖ Simple and clear ‚úÖ Easy to maintain ‚úÖ Changes to class definitions automatically propagate ‚úÖ Visual - you see all classes defined, then hide unwanted ones Complete Working Example:#\rmodel/order-domain.iuml:\n@startuml\rclass Customer {\r- customerId: int\r- name: String\r- email: String\r+ placeOrder(): Order\r+ getOrders(): List\u0026lt;Order\u0026gt;\r}\rclass Order {\r- orderId: int\r- orderDate: Date\r- totalAmount: double\r+ addItem(item: OrderItem): void\r+ calculateTotal(): double\r}\rclass OrderItem {\r- itemId: int\r- quantity: int\r- price: double\r+ getSubtotal(): double\r}\rCustomer \u0026#34;1\u0026#34; --\u0026gt; \u0026#34;*\u0026#34; Order : places\rOrder \u0026#34;1\u0026#34; --\u0026gt; \u0026#34;*\u0026#34; OrderItem : contains\r@endumlviews/customer-order-view.puml (Customer and Order only):\n@startuml\rtitle Customer-Order View\r!include ../model/order-domain.iuml\rhide OrderItem\rnote right of Order\rThis view focuses on\rcustomer order management\rend note\r@endumlviews/order-items-view.puml (Order and OrderItem only):\n@startuml\rtitle Order Items View\r!include ../model/order-domain.iuml\rhide Customer\rnote right of OrderItem\rThis view focuses on\rorder line item details\rend note\r@enduml Generation Script Example üöÄ#\rgenerate-all-views.sh:\n#!/bin/bash # Generate all views for view in views/*.puml; do filename=$(basename \u0026#34;$view\u0026#34; .puml) java -jar plantuml.jar \u0026#34;$view\u0026#34; -tsvg -o \u0026#34;../exports/svg\u0026#34; java -jar plantuml.jar \u0026#34;$view\u0026#34; -tpng -o \u0026#34;../exports/png\u0026#34; echo \u0026#34;Generated: $filename\u0026#34; done echo \u0026#34;All diagrams generated!\u0026#34; Key Hide/Remove Commands üìã#\rhide ClassName \u0026#39; Hide specific class\rhide @unlinked \u0026#39; Hide classes with no relationships\rhide empty members \u0026#39; Hide empty attributes/methods\rhide circle \u0026#39; Hide class circle notation\rhide ClassName attributes \u0026#39; Hide only attributes of ClassName\rhide ClassName methods \u0026#39; Hide only methods of ClassName\rhide \u0026lt;\u0026lt;stereotype\u0026gt;\u0026gt; \u0026#39; Hide all classes with stereotype\rremove ClassName \u0026#39; Completely remove class\rshow ClassName \u0026#39; Re-show hidden class Validation Checklist ‚úÖ#\rBefore committing your structure:\nAll classes defined once in central file(s) Each view file uses !include for central definitions Views use hide/remove to filter classes File paths are correct (use ../ for relative paths) Test generation: java -jar plantuml.jar views/*.puml Verify exports render correctly Would you like me to:\nShow how to handle more complex scenarios (inheritance, interfaces, nested packages)? Provide a CI/CD pipeline that auto-generates all views? Demonstrate how to add metadata (notes, colors, styles) that also uses the central model? Show how to generate a \u0026ldquo;model catalog\u0026rdquo; that documents all available classes? This approach gives you exactly what you need: one source of truth, multiple selective views, easy maintenance!\nExcellent thinking! Yes, absolutely - you can hide all, then selectively show. This is much better for maintainability. Let me show you the best approaches:\nSolution 1: Hide All + Show Specific ‚≠ê Best for Your ScenarioPerfect! Now I can give you the complete solution. Here are the best approaches:#\rMethod 1: Use hide @unlinked + Define Relationships ‚≠ê Cleanest#\rThis is the smartest approach: PlantUML will automatically hide any class that doesn\u0026rsquo;t have a relationship shown in the diagram.\nCentral Model File#\rmodel/classes.iuml:\n@startuml\rclass A {\r- idA: int\r- nameA: String\r+ methodA(): void\r}\rclass B {\r- idB: int\r- dataB: String\r+ methodB(): void\r}\rclass C {\r- idC: int\r- valueC: double\r+ methodC(): void\r}\rclass D {\r- idD: int\r+ methodD(): void\r}\rclass E {\r- idE: int\r+ methodE(): void\r}\r\u0026#39; Define ALL relationships\rA --\u0026gt; B : uses\rB --\u0026gt; C : transforms\rC --\u0026gt; D : processes\rD --\u0026gt; E : notifies\r@endumlView Files - Just Show What You Want!#\rviews/view-all.puml (Shows A, B, C):\n@startuml\rtitle All Classes View\r!include ../model/classes.iuml\r\u0026#39; Show only the relationships we care about\rA --\u0026gt; B\rB --\u0026gt; C\r\u0026#39; Hide anything not connected\rhide @unlinked\r@endumlResult: Only A, B, and C appear. D and E are automatically hidden! üéâ\nviews/view-ab.puml (Shows only A, B):\n@startuml\rtitle A-B View\r!include ../model/classes.iuml\r\u0026#39; Show only this relationship\rA --\u0026gt; B\rhide @unlinked\r@endumlviews/view-bc.puml (Shows only B, C):\n@startuml\rtitle B-C View\r!include ../model/classes.iuml\r\u0026#39; Show only this relationship\rB --\u0026gt; C\rhide @unlinked\r@endumlhide @unlinked hides classes that aren\u0026rsquo;t linked to anything in the current diagram.\nMethod 2: Using Stereotypes as Tags üè∑Ô∏è Most Explicit#\rThis gives you complete control - mark which classes belong in which views.\nCentral Model with Stereotypes#\rmodel/classes-tagged.iuml:\n@startuml\rclass A \u0026lt;\u0026lt;ViewAll\u0026gt;\u0026gt; \u0026lt;\u0026lt;ViewAB\u0026gt;\u0026gt; {\r- idA: int\r+ methodA(): void\r}\rclass B \u0026lt;\u0026lt;ViewAll\u0026gt;\u0026gt; \u0026lt;\u0026lt;ViewAB\u0026gt;\u0026gt; \u0026lt;\u0026lt;ViewBC\u0026gt;\u0026gt; {\r- idB: int\r+ methodB(): void\r}\rclass C \u0026lt;\u0026lt;ViewAll\u0026gt;\u0026gt; \u0026lt;\u0026lt;ViewBC\u0026gt;\u0026gt; {\r- idC: int\r+ methodC(): void\r}\rclass D {\r- idD: int\r+ methodD(): void\r}\rclass E {\r- idE: int\r+ methodE(): void\r}\rA --\u0026gt; B\rB --\u0026gt; C\rC --\u0026gt; D\rD --\u0026gt; E\r@endumlView Files - Hide All, Show Tagged#\rviews/view-ab-tagged.puml:\n@startuml\rtitle A-B View (Tagged)\r!include ../model/classes-tagged.iuml\r\u0026#39; Hide everything first\rhide class\r\u0026#39; Show only what we want\rshow \u0026lt;\u0026lt;ViewAB\u0026gt;\u0026gt;\r@endumlviews/view-bc-tagged.puml:\n@startuml\rtitle B-C View (Tagged)\r!include ../model/classes-tagged.iuml\r\u0026#39; Hide everything first\rhide class\r\u0026#39; Show only what we want\rshow \u0026lt;\u0026lt;ViewBC\u0026gt;\u0026gt;\r@enduml Method 3: Using Packages for Grouping üì¶ Best for Complex Models#\rGroup related classes in packages, then show/hide packages.\nCentral Model with Packages#\rmodel/domain-model.iuml:\n@startuml\rpackage \u0026#34;Core\u0026#34; {\rclass A {\r- idA: int\r+ methodA(): void\r}\rclass B {\r- idB: int\r+ methodB(): void\r}\r}\rpackage \u0026#34;Processing\u0026#34; {\rclass C {\r- idC: int\r+ methodC(): void\r}\r}\rpackage \u0026#34;Notification\u0026#34; {\rclass D {\r- idD: int\r+ methodD(): void\r}\rclass E {\r- idE: int\r+ methodE(): void\r}\r}\rA --\u0026gt; B\rB --\u0026gt; C\rC --\u0026gt; D\rD --\u0026gt; E\r@endumlView Files - Show Specific Packages#\rviews/view-core.puml:\n@startuml\rtitle Core View\r!include ../model/domain-model.iuml\r\u0026#39; Hide all packages first\rhide package\r\u0026#39; Show only what we want\rshow \u0026#34;Core\u0026#34;\r@endumlviews/view-core-processing.puml:\n@startuml\rtitle Core + Processing View\r!include ../model/domain-model.iuml\rhide package\rshow \u0026#34;Core\u0026#34;\rshow \u0026#34;Processing\u0026#34;\r@enduml Method 4: Combination Approach üéØ Most Flexible#\rCombine hide @unlinked with selective relationship definitions.\nmodel/classes-all.iuml:\n@startuml\r\u0026#39; Define all classes once\rclass A {\r- idA: int\r+ methodA(): void\r}\rclass B {\r- idB: int\r+ methodB(): void\r}\rclass C {\r- idC: int\r+ methodC(): void\r}\rclass D {\r- idD: int\r+ methodD(): void\r}\rclass E {\r- idE: int\r+ methodE(): void\r}\r@endumlmodel/relationships.iuml:\n@startuml\r\u0026#39; Define all relationships\rA --\u0026gt; B : uses\rB --\u0026gt; C : transforms\rC --\u0026gt; D : processes\rD --\u0026gt; E : notifies\r@endumlviews/view-ab-combo.puml:\n@startuml\rtitle A-B View\r!include ../model/classes-all.iuml\r\u0026#39; Include only the relationships we want\rA --\u0026gt; B : uses\r\u0026#39; Hide unlinked classes\rhide @unlinked\r@endumlBenefits:\n‚úÖ Add new class to classes-all.iuml ‚Üí doesn\u0026rsquo;t appear in any view unless you link it ‚úÖ No need to update existing view files when adding new classes ‚úÖ Relationships are explicit in each view ‚úÖ Can reuse relationship definitions when needed My Recommendation for Your Use Case üí°#\rUse Method 1 (hide @unlinked with selective relationships) because:\n‚úÖ Zero maintenance overhead - add classes to model, they won\u0026rsquo;t show up anywhere unless explicitly linked ‚úÖ Self-documenting - each view clearly shows which relationships it cares about ‚úÖ Clean separation - model has all definitions, views control visibility ‚úÖ No tagging required - simpler syntax Complete Working Example#\rProject Structure:#\rdomain-model/\r‚îú‚îÄ‚îÄ model/\r‚îÇ ‚îî‚îÄ‚îÄ order-system.iuml # All class definitions\r‚îú‚îÄ‚îÄ views/\r‚îÇ ‚îú‚îÄ‚îÄ customer-orders.puml # Customer + Order only\r‚îÇ ‚îú‚îÄ‚îÄ order-items.puml # Order + OrderItem only\r‚îÇ ‚îî‚îÄ‚îÄ complete.puml # Everything\r‚îî‚îÄ‚îÄ generate.shmodel/order-system.iuml:#\r@startuml\rclass Customer {\r- customerId: int\r- name: String\r- email: String\r+ placeOrder(): Order\r}\rclass Order {\r- orderId: int\r- orderDate: Date\r- total: double\r+ calculateTotal(): double\r}\rclass OrderItem {\r- itemId: int\r- quantity: int\r- price: double\r+ getSubtotal(): double\r}\rclass Product {\r- productId: int\r- name: String\r- price: double\r+ getDetails(): String\r}\rclass Inventory {\r- stockLevel: int\r+ checkAvailability(): boolean\r}\r\u0026#39; Define all relationships\rCustomer \u0026#34;1\u0026#34; --\u0026gt; \u0026#34;*\u0026#34; Order : places\rOrder \u0026#34;1\u0026#34; --\u0026gt; \u0026#34;*\u0026#34; OrderItem : contains\rOrderItem \u0026#34;*\u0026#34; --\u0026gt; \u0026#34;1\u0026#34; Product : references\rProduct \u0026#34;1\u0026#34; --\u0026gt; \u0026#34;1\u0026#34; Inventory : tracked_by\r@endumlviews/customer-orders.puml:#\r@startuml\rtitle Customer-Order Management View\r!include ../model/order-system.iuml\r\u0026#39; Redefine only relationships we care about\rCustomer \u0026#34;1\u0026#34; --\u0026gt; \u0026#34;*\u0026#34; Order : places\r\u0026#39; Hide everything not connected\rhide @unlinked\rnote right of Customer\rThis view focuses on\rcustomer order management\r- Product, Inventory, OrderItem\rare automatically hidden\rend note\r@endumlviews/order-items.puml:#\r@startuml\rtitle Order Line Items View\r!include ../model/order-system.iuml\r\u0026#39; Show these relationships\rOrder \u0026#34;1\u0026#34; --\u0026gt; \u0026#34;*\u0026#34; OrderItem : contains\rOrderItem \u0026#34;*\u0026#34; --\u0026gt; \u0026#34;1\u0026#34; Product : references\rhide @unlinked\r@endumlviews/complete.puml:#\r@startuml\rtitle Complete Order System\r!include ../model/order-system.iuml\r\u0026#39; Include all relationships\rCustomer \u0026#34;1\u0026#34; --\u0026gt; \u0026#34;*\u0026#34; Order : places\rOrder \u0026#34;1\u0026#34; --\u0026gt; \u0026#34;*\u0026#34; OrderItem : contains\rOrderItem \u0026#34;*\u0026#34; --\u0026gt; \u0026#34;1\u0026#34; Product : references\rProduct \u0026#34;1\u0026#34; --\u0026gt; \u0026#34;1\u0026#34; Inventory : tracked_by\r\u0026#39; Everything is linked, so nothing hidden\rhide @unlinked\r@enduml Generation Script#\rgenerate.sh:\n#!/bin/bash echo \u0026#34;Generating all views...\u0026#34; for view in views/*.puml; do filename=$(basename \u0026#34;$view\u0026#34; .puml) echo \u0026#34; Processing: $filename\u0026#34; java -jar plantuml.jar \u0026#34;$view\u0026#34; -tsvg -o \u0026#34;../exports/svg\u0026#34; -quiet java -jar plantuml.jar \u0026#34;$view\u0026#34; -tpng -o \u0026#34;../exports/png\u0026#34; -quiet done echo \u0026#34;‚úÖ All diagrams generated!\u0026#34; Key Commands Reference#\r\u0026#39; Hide everything unlinked\rhide @unlinked\r\u0026#39; Hide all classes, then show specific ones\rhide class\rshow ClassA\rshow ClassB\r\u0026#39; Hide all packages, then show specific ones\rhide package\rshow \u0026#34;PackageName\u0026#34;\r\u0026#39; Hide by stereotype\rhide \u0026lt;\u0026lt;stereotype\u0026gt;\u0026gt;\r\u0026#39; Show by stereotype (after hiding all)\rhide class\rshow \u0026lt;\u0026lt;ViewTag\u0026gt;\u0026gt;\r\u0026#39; Remove vs Hide\rremove ClassA \u0026#39; Completely removes from memory\rhide ClassA \u0026#39; Just makes invisible, can be shown later Testing Your Setup ‚úÖ#\rAdd a new class to order-system.iuml:\nclass PaymentMethod {\r- paymentId: int\r+ processPayment(): boolean\r} Regenerate all views ‚Üí PaymentMethod doesn\u0026rsquo;t appear in any existing view! ‚ú®\nWhen you want it, add to specific view:\nOrder --\u0026gt; PaymentMethod : paid_with This approach scales beautifully - you can have 100 classes in your model, but each view only shows the 3-5 relevant ones, and adding new classes never breaks existing views!\nWould you like me to show:\nHow to handle inheritance and interfaces with this approach? How to add conditional compilation for even more control? How to generate a \u0026ldquo;catalog\u0026rdquo; document showing all available classes? Unfortunately, PlantUML doesn\u0026rsquo;t have a built-in feature to automatically show \u0026ldquo;all relationships between visible classes only.\u0026rdquo; However, I have excellent workarounds that give you exactly what you want. Let me show you the best approaches:\nSolution 1: Remove Hidden Classes from Model ‚≠ê Cleanest Approach#\rInstead of hiding classes, use remove to completely eliminate them from memory. This automatically removes all their relationships too!\nmodel/order-system.iuml:\n@startuml\rclass Customer {\r- customerId: int\r- name: String\r}\rclass Order {\r- orderId: int\r- total: double\r}\rclass OrderItem {\r- itemId: int\r- quantity: int\r}\rclass Product {\r- productId: int\r- name: String\r}\rclass Inventory {\r- stockLevel: int\r}\r\u0026#39; ALL relationships defined\rCustomer \u0026#34;1\u0026#34; --\u0026gt; \u0026#34;*\u0026#34; Order : places\rOrder \u0026#34;1\u0026#34; --\u0026gt; \u0026#34;*\u0026#34; OrderItem : contains\rOrderItem \u0026#34;*\u0026#34; --\u0026gt; \u0026#34;1\u0026#34; Product : references\rProduct \u0026#34;1\u0026#34; --\u0026gt; \u0026#34;1\u0026#34; Inventory : tracked_by\r@endumlviews/customer-orders.puml:\n@startuml\rtitle Customer-Order View\r!include ../model/order-system.iuml\r\u0026#39; Remove unwanted classes - their relationships disappear too!\rremove OrderItem\rremove Product\rremove Inventory\r@endumlResult: Only shows Customer, Order, and the relationship between them! üéâ\nSolution 2: Separate Model and Relationships Files üì¶ Most Flexible#\rSplit your class definitions from relationships, then selectively include what you need.\nFile Structure:#\rmodel/\r‚îú‚îÄ‚îÄ classes.iuml # All class definitions\r‚îú‚îÄ‚îÄ rel-customer-order.iuml\r‚îú‚îÄ‚îÄ rel-order-item.iuml\r‚îú‚îÄ‚îÄ rel-item-product.iuml\r‚îî‚îÄ‚îÄ rel-product-inventory.iumlmodel/classes.iuml:\n@startuml\rclass Customer {\r- customerId: int\r- name: String\r}\rclass Order {\r- orderId: int\r- total: double\r}\rclass OrderItem {\r- itemId: int\r- quantity: int\r}\rclass Product {\r- productId: int\r- name: String\r}\rclass Inventory {\r- stockLevel: int\r}\r@endumlmodel/rel-customer-order.iuml:\n@startuml\rCustomer \u0026#34;1\u0026#34; --\u0026gt; \u0026#34;*\u0026#34; Order : places\r@endumlmodel/rel-order-item.iuml:\n@startuml\rOrder \u0026#34;1\u0026#34; --\u0026gt; \u0026#34;*\u0026#34; OrderItem : contains\r@endumlmodel/rel-item-product.iuml:\n@startuml\rOrderItem \u0026#34;*\u0026#34; --\u0026gt; \u0026#34;1\u0026#34; Product : references\r@endumlviews/customer-orders.puml:\n@startuml\rtitle Customer-Order View\r!include ../model/classes.iuml\r!include ../model/rel-customer-order.iuml\r\u0026#39; Hide everything unlinked\rhide @unlinked\r@endumlBenefits:\n‚úÖ Relationships are reusable ‚úÖ Easy to see what\u0026rsquo;s included ‚úÖ No need to manually list classes to show ‚úÖ Auto-hides unconnected classes Solution 3: Using Procedures for Smart Includes üîß Most Powerful#\rCreate a macro system where you define \u0026ldquo;views\u0026rdquo; and automatically include relevant relationships.\nmodel/domain-complete.iuml:\n@startuml\r\u0026#39; Define all classes\rclass Customer {\r- customerId: int\r- name: String\r}\rclass Order {\r- orderId: int\r- total: double\r}\rclass OrderItem {\r- itemId: int\r- quantity: int\r}\rclass Product {\r- productId: int\r- name: String\r}\rclass Inventory {\r- stockLevel: int\r}\r\u0026#39; Procedure to include relationships by view\r!procedure $show_relationships($view)\r!if ($view == \u0026#34;customer-order\u0026#34;)\rCustomer \u0026#34;1\u0026#34; --\u0026gt; \u0026#34;*\u0026#34; Order : places\r!endif\r!if ($view == \u0026#34;order-items\u0026#34;)\rOrder \u0026#34;1\u0026#34; --\u0026gt; \u0026#34;*\u0026#34; OrderItem : contains\rOrderItem \u0026#34;*\u0026#34; --\u0026gt; \u0026#34;1\u0026#34; Product : references\r!endif\r!if ($view == \u0026#34;inventory\u0026#34;)\rProduct \u0026#34;1\u0026#34; --\u0026gt; \u0026#34;1\u0026#34; Inventory : tracked_by\r!endif\r!if ($view == \u0026#34;complete\u0026#34;)\rCustomer \u0026#34;1\u0026#34; --\u0026gt; \u0026#34;*\u0026#34; Order : places\rOrder \u0026#34;1\u0026#34; --\u0026gt; \u0026#34;*\u0026#34; OrderItem : contains\rOrderItem \u0026#34;*\u0026#34; --\u0026gt; \u0026#34;1\u0026#34; Product : references\rProduct \u0026#34;1\u0026#34; --\u0026gt; \u0026#34;1\u0026#34; Inventory : tracked_by\r!endif\r!endprocedure\r@endumlviews/customer-orders-macro.puml:\n@startuml\rtitle Customer-Order View\r!include ../model/domain-complete.iuml\r\u0026#39; Show only customer-order relationships\r$show_relationships(\u0026#34;customer-order\u0026#34;)\rhide @unlinked\r@enduml Solution 4: Tag-Based Relationship Filtering üè∑Ô∏è Most Scalable#\rTag both classes AND relationships, then filter by tag.\nmodel/tagged-model.iuml:\n@startuml\rclass Customer \u0026lt;\u0026lt;view_customer_order\u0026gt;\u0026gt; \u0026lt;\u0026lt;view_complete\u0026gt;\u0026gt; {\r- customerId: int\r- name: String\r}\rclass Order \u0026lt;\u0026lt;view_customer_order\u0026gt;\u0026gt; \u0026lt;\u0026lt;view_order_items\u0026gt;\u0026gt; \u0026lt;\u0026lt;view_complete\u0026gt;\u0026gt; {\r- orderId: int\r- total: double\r}\rclass OrderItem \u0026lt;\u0026lt;view_order_items\u0026gt;\u0026gt; \u0026lt;\u0026lt;view_complete\u0026gt;\u0026gt; {\r- itemId: int\r- quantity: int\r}\rclass Product \u0026lt;\u0026lt;view_order_items\u0026gt;\u0026gt; \u0026lt;\u0026lt;view_inventory\u0026gt;\u0026gt; \u0026lt;\u0026lt;view_complete\u0026gt;\u0026gt; {\r- productId: int\r- name: String\r}\rclass Inventory \u0026lt;\u0026lt;view_inventory\u0026gt;\u0026gt; \u0026lt;\u0026lt;view_complete\u0026gt;\u0026gt; {\r- stockLevel: int\r}\r\u0026#39; Tag relationships with same tags\rCustomer \u0026#34;1\u0026#34; --\u0026gt; \u0026#34;*\u0026#34; Order : places $view_customer_order $view_complete\rOrder \u0026#34;1\u0026#34; --\u0026gt; \u0026#34;*\u0026#34; OrderItem : contains $view_order_items $view_complete\rOrderItem \u0026#34;*\u0026#34; --\u0026gt; \u0026#34;1\u0026#34; Product : references $view_order_items $view_complete\rProduct \u0026#34;1\u0026#34; --\u0026gt; \u0026#34;1\u0026#34; Inventory : tracked_by $view_inventory $view_complete\r@endumlviews/customer-orders-tagged.puml:\n@startuml\rtitle Customer-Order View\r!include ../model/tagged-model.iuml\r\u0026#39; Hide everything\rhide class\r\u0026#39; Show only tagged elements\rshow \u0026lt;\u0026lt;view_customer_order\u0026gt;\u0026gt;\r@enduml Solution 5: Hybrid Approach üéØ My Recommendation#\rCombine the best of remove with relationship files for maximum maintainability.\nStructure:#\rmodel/\r‚îú‚îÄ‚îÄ all-classes.iuml # Every class\r‚îî‚îÄ‚îÄ all-relationships.iuml # Every relationship\rviews/\r‚îú‚îÄ‚îÄ customer-orders.puml\r‚îú‚îÄ‚îÄ order-items.puml\r‚îî‚îÄ‚îÄ complete.pumlmodel/all-classes.iuml:\n@startuml\rclass Customer {\r- customerId: int\r- name: String\r+ placeOrder(): Order\r}\rclass Order {\r- orderId: int\r- orderDate: Date\r- total: double\r+ calculateTotal(): double\r}\rclass OrderItem {\r- itemId: int\r- quantity: int\r- price: double\r+ getSubtotal(): double\r}\rclass Product {\r- productId: int\r- name: String\r- price: double\r+ getDetails(): String\r}\rclass Inventory {\r- stockLevel: int\r+ checkAvailability(): boolean\r+ reduceStock(amount: int): void\r}\r@endumlmodel/all-relationships.iuml:\n@startuml\rCustomer \u0026#34;1\u0026#34; --\u0026gt; \u0026#34;*\u0026#34; Order : places\rOrder \u0026#34;1\u0026#34; --\u0026gt; \u0026#34;*\u0026#34; OrderItem : contains\rOrderItem \u0026#34;*\u0026#34; --\u0026gt; \u0026#34;1\u0026#34; Product : references\rProduct \u0026#34;1\u0026#34; --\u0026gt; \u0026#34;1\u0026#34; Inventory : tracked_by\r@endumlviews/customer-orders.puml:\n@startuml\rtitle Customer-Order View\r!include ../model/all-classes.iuml\r!include ../model/all-relationships.iuml\r\u0026#39; Remove unwanted classes\r\u0026#39; Their relationships automatically disappear!\rremove OrderItem\rremove Product\rremove Inventory\r@endumlviews/order-items.puml:\n@startuml\rtitle Order Items View\r!include ../model/all-classes.iuml\r!include ../model/all-relationships.iuml\r\u0026#39; Remove what we don\u0026#39;t need\rremove Customer\rremove Inventory\r@endumlviews/complete.puml:\n@startuml\rtitle Complete Order System\r!include ../model/all-classes.iuml\r!include ../model/all-relationships.iuml\r\u0026#39; Nothing removed - show everything!\r@enduml Complete Working Example#\rLet me show you a real-world example with all files:\nmodel/all-classes.iuml:\n@startuml\rclass Customer {\r- customerId: int\r- name: String\r- email: String\r+ register(): void\r+ login(): boolean\r+ placeOrder(): Order\r}\rclass Order {\r- orderId: int\r- orderDate: Date\r- status: String\r- totalAmount: double\r+ addItem(item: OrderItem): void\r+ calculateTotal(): double\r+ submit(): void\r}\rclass OrderItem {\r- itemId: int\r- quantity: int\r- unitPrice: double\r+ getSubtotal(): double\r}\rclass Product {\r- productId: int\r- name: String\r- description: String\r- basePrice: double\r+ getDetails(): String\r+ updatePrice(price: double): void\r}\rclass Inventory {\r- inventoryId: int\r- stockLevel: int\r- reorderPoint: int\r+ checkAvailability(): boolean\r+ reduceStock(amount: int): void\r+ restock(amount: int): void\r}\rclass Payment {\r- paymentId: int\r- amount: double\r- paymentMethod: String\r+ process(): boolean\r}\rclass Shipment {\r- shipmentId: int\r- trackingNumber: String\r- status: String\r+ ship(): void\r+ trackStatus(): String\r}\r@endumlmodel/all-relationships.iuml:\n@startuml\rCustomer \u0026#34;1\u0026#34; --\u0026gt; \u0026#34;*\u0026#34; Order : places\rOrder \u0026#34;1\u0026#34; --\u0026gt; \u0026#34;*\u0026#34; OrderItem : contains\rOrder \u0026#34;1\u0026#34; --\u0026gt; \u0026#34;0..1\u0026#34; Payment : paid_with\rOrder \u0026#34;1\u0026#34; --\u0026gt; \u0026#34;0..1\u0026#34; Shipment : fulfilled_by\rOrderItem \u0026#34;*\u0026#34; --\u0026gt; \u0026#34;1\u0026#34; Product : references\rProduct \u0026#34;1\u0026#34; --\u0026gt; \u0026#34;1\u0026#34; Inventory : tracked_by\r@endumlviews/customer-management.puml:\n@startuml\rtitle Customer Management View\r!include ../model/all-classes.iuml\r!include ../model/all-relationships.iuml\rremove OrderItem\rremove Product\rremove Inventory\rremove Payment\rremove Shipment\rnote right of Customer\rFocus: Customer orders\rShows: Customer ‚Üí Order relationship\rend note\r@endumlviews/order-fulfillment.puml:\n@startuml\rtitle Order Fulfillment View\r!include ../model/all-classes.iuml\r!include ../model/all-relationships.iuml\rremove Customer\rremove OrderItem\rremove Product\rremove Inventory\rnote right of Order\rFocus: Order processing\rShows: Payment \u0026amp; Shipment\rend note\r@endumlviews/inventory-management.puml:\n@startuml\rtitle Inventory Management View\r!include ../model/all-classes.iuml\r!include ../model/all-relationships.iuml\rremove Customer\rremove Order\rremove Payment\rremove Shipment\rnote right of Product\rFocus: Stock management\rShows: Products \u0026amp; Inventory\rend note\r@enduml Why This Works So Well ‚ú®#\rAdd a new class (e.g., Category) to all-classes.iuml Add its relationships to all-relationships.iuml No existing views change - they automatically exclude it! When you want it in a view, just don\u0026rsquo;t remove it Key Commands Summary#\r\u0026#39; Remove classes (removes their relationships too)\rremove ClassName\r\u0026#39; Hide classes (keeps relationships)\rhide ClassName\r\u0026#39; Hide unlinked classes\rhide @unlinked\r\u0026#39; Show specific classes after hiding all\rhide class\rshow ClassA\rshow ClassB Best Practice Recommendation üí°#\rFor your use case:\nUse Solution 5 (Hybrid Approach with remove) Maintain two files: all-classes.iuml and all-relationships.iuml Each view includes both, then uses remove to exclude unwanted classes Relationships to removed classes automatically disappear! This gives you:\n‚úÖ Single source of truth for classes ‚úÖ Single source of truth for relationships ‚úÖ Automatic relationship filtering (this is what you wanted!) ‚úÖ Zero maintenance when adding new classes ‚úÖ Clear, explicit view definitions Would you like me to show:\nHow to handle inheritance hierarchies with this approach? How to create a script that validates all views still compile? How to generate a \u0026ldquo;relationship matrix\u0026rdquo; showing which views include which classes? "},{"id":6,"href":"/musings/docs/misc/verification-architecture/","title":"Is This a Reasonable Architecture?","section":"Miscellaneous","content":"prompt#\rNew topic. Say I have a domain model of my system. The concrete data is spread across many systems and data stores. Is it a reasonable architecture to declare rules about what states the domain objects should be in, then go verify the various systems that have the concrete objects to ensure the business rules are being followed?\nThat\u0026rsquo;s an excellent architectural question! You\u0026rsquo;re describing a form of data validation architecture or invariant checking system. Let me analyze this approach and provide some perspective.Excellent question! This is a sophisticated architectural pattern. Let me analyze it thoroughly:\nIs This a Reasonable Architecture?#\rShort answer: Yes, but with important caveats. This is a valid pattern called \u0026ldquo;Invariant Verification\u0026rdquo; or \u0026ldquo;Consistency Checking\u0026rdquo; but it\u0026rsquo;s better suited for specific scenarios than as a general-purpose architecture.\nWhat You\u0026rsquo;re Describing#\rYou\u0026rsquo;re proposing an architecture with:\nCanonical Domain Model - Your idealized view of how objects should be structured Distributed Reality - Data actually lives in multiple systems/databases Verification Layer - Checks that the distributed state conforms to domain rules This is essentially a form of data quality monitoring or integrity validation at the domain level.\nWhen This Architecture Makes Sense ‚úÖ#\r1. Legacy System Integration#\rWhen you have:\nMultiple legacy systems you can\u0026rsquo;t easily change Data scattered across incompatible databases Need to enforce new business rules on old data Example:\nDomain Rule: \u0026#34;An order must have a valid customer and all items in stock\u0026#34;\rReality:\r- Orders in Oracle\r- Customers in SAP\r- Inventory in MongoDB\r- Payment in external serviceYour verification system periodically checks these systems to ensure the invariant holds.\n2. Data Quality Auditing#\rDomain entities should always be valid entities with certain invariants that should always be true, but in distributed systems with eventual consistency, you need:\nContinuous monitoring of data quality Alerting when invariants are violated Reporting on compliance levels 3. Regulatory Compliance#\rWhen you need to:\nProve compliance with regulations Generate audit trails Demonstrate data integrity 4. Eventually Consistent Systems#\rIn systems using eventual consistency where:\nUpdates propagate asynchronously Temporary violations are acceptable You need to verify eventual convergence to valid state Significant Challenges ‚ö†Ô∏è#\r1. The \u0026ldquo;Checking After the Fact\u0026rdquo; Problem#\rIssue: By the time you verify, damage may already be done.\nTimeline:\r10:00 AM - Invalid order created (violates inventory rule)\r10:05 AM - Order shipped\r10:10 AM - Verification runs, finds violation\rBUT: Already shipped! Can\u0026#39;t undo.Better Alternative: Invariant enforcement should happen before that state has even been reached, like a self-protection reflex performed by the thing being mutated.\n**2. The \u0026ldquo;Who\u0026rsquo;s Responsible?\u0026rdquo; Problem#\rIssue: When verification finds a violation, what happens?\nGenerate a ticket for manual review? Attempt automatic remediation? Rollback transactions (but data is in multiple systems)? Just log and hope someone notices? 3. Performance \u0026amp; Scale Issues#\rVerification requires:\nQuerying multiple systems Joining data across boundaries Running complex checks This can be very expensive at scale 4. Race Conditions#\rThread 1: Verifies Order 123 - VALID\rThread 2: Updates Customer (Order 123\u0026#39;s customer) - Makes it invalid\rThread 1: Verification completes - Reports VALID (but it\u0026#39;s now invalid!)5. Consistency Window#\rHow often do you verify?\nToo often: Performance impact, resource consumption Too infrequent: Long window where violations exist undetected Better Architectural Patterns üéØ#\rPattern 1: Proactive Enforcement (Recommended)#\rInstead of checking after, enforce before:\nApplication Layer ‚Üí Domain Service ‚Üí Validate Invariants ‚Üí Commit\r‚Üì\rREJECT if invalidInvariants enforcement is the responsibility of the domain entities, especially the aggregate root, and an entity object should not be able to exist without being valid.\nImplementation:\nclass OrderService { public Order createOrder(CreateOrderCommand cmd) { // BEFORE creating the order, verify invariants Customer customer = customerRepo.findById(cmd.customerId); if (!customer.isActive()) { throw new InvalidOrderException(\u0026#34;Customer not active\u0026#34;); } for (Item item : cmd.items) { if (!inventory.isAvailable(item)) { throw new InvalidOrderException(\u0026#34;Item not in stock\u0026#34;); } } // NOW create the order - guaranteed valid Order order = new Order(customer, cmd.items); return orderRepo.save(order); } }Pattern 2: Saga Pattern for Distributed Transactions#\rA saga is a message driven process manager that reacts to events published by aggregates and sends commands to other aggregates to coordinate business processes.\nExample:\n1. Order Service ‚Üí Order Created Event\r2. Saga receives event ‚Üí Verify Customer\r3. If invalid ‚Üí Send Cancel Order Command\r4. If valid ‚Üí Send Reserve Inventory Command\r5. Continue coordination...Pattern 3: Domain Events + Eventual Consistency#\rIn a domain-driven system, domain events are an excellent way of achieving eventual consistency. Any system that needs to update itself when something happens can subscribe to domain events.\nOrder Created (in valid state)\r‚Üì\rEvent Published\r‚Üì\rInventory Service subscribes ‚Üí Reserves stock\rPayment Service subscribes ‚Üí Processes payment\rShipping Service subscribes ‚Üí Prepares shipmentPattern 4: Hybrid Approach (What You Might Actually Want)#\rCombine proactive enforcement with reactive verification:\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\r‚îÇ Real-time Enforcement (Primary) ‚îÇ\r‚îÇ - Validate on write ‚îÇ\r‚îÇ - Reject invalid operations ‚îÇ\r‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\r‚Üì\r‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\r‚îÇ Periodic Verification (Safety Net) ‚îÇ\r‚îÇ - Catch edge cases ‚îÇ\r‚îÇ - Handle race conditions ‚îÇ\r‚îÇ - Detect external corruption ‚îÇ\r‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îòThis gives you:\n‚úÖ Prevention - Most violations never happen ‚úÖ Detection - Catch things that slip through ‚úÖ Audit trail - Prove compliance over time If You Must Use Verification Architecture#\rHere\u0026rsquo;s how to do it well:\n1. Define Clear SLAs#\rverification_rules: order_consistency: rule: \u0026#34;All orders must have valid customer and inventory\u0026#34; severity: CRITICAL check_frequency: 5 minutes alert_threshold: 1 violation remediation: AUTO_CANCEL2. Categorize Invariants#\rHard Invariants (Enforce proactively):\nMust never be violated Prevent operations that would violate Example: \u0026ldquo;Cannot have negative inventory\u0026rdquo; Soft Invariants (Can verify reactively):\nTemporary violations acceptable Verify periodically Example: \u0026ldquo;Customer addresses should be validated within 24 hours\u0026rdquo; 3. Build Automated Remediation#\rclass InvariantChecker: def check_order_invariants(self, order_id): violations = [] order = order_system.get(order_id) customer = customer_system.get(order.customer_id) if not customer.is_active: violations.append( Violation( rule=\u0026#34;active_customer\u0026#34;, remediation=CancelOrder(order_id) ) ) for item in order.items: if not inventory_system.is_available(item.sku): violations.append( Violation( rule=\u0026#34;inventory_available\u0026#34;, remediation=RemoveItem(order_id, item.sku) ) ) return violations4. Implement Compensation Logic#\rWhen violations are found:\n1. Log the violation\r2. Create a compensation transaction\r3. Execute remediation (if safe)\r4. Alert if manual intervention needed\r5. Update audit trail Real-World Example#\rScenario: E-commerce Platform\nDomain Model: Order\r- Must have valid Customer\r- Must have in-stock Products\r- Must have valid ShippingAddress\r- Must have successful Payment\rReality:\r- Orders: PostgreSQL\r- Customers: Auth0\r- Products: Elasticsearch\r- Inventory: Redis\r- Addresses: Google Address API\r- Payments: Stripe\rArchitecture:Layer 1: Proactive Enforcement\n// At order creation time OrderService.createOrder() { // Validate BEFORE creating validateCustomer(); validateInventory(); validateAddress(); // Only create if all valid return createValidOrder(); }Layer 2: Reactive Verification (Your pattern)\n// Scheduled job (every 15 minutes) InvariantVerifier.verifyOrders() { for (Order order : getAllRecentOrders()) { List\u0026lt;Violation\u0026gt; violations = check(order); if (!violations.isEmpty()) { handleViolations(order, violations); } } } My Recommendation üí°#\rUse verification as a safety net, not the primary mechanism:\nPrimary Strategy: Enforce invariants at write time (proactive) Secondary Strategy: Periodic verification (reactive) as backup Tertiary Strategy: Manual audits and reports Why?\nPrevention is better than detection Detection is better than ignorance But detection alone leaves windows of vulnerability Good Use Cases for Your Pattern:\n‚úÖ Data quality monitoring ‚úÖ Compliance auditing ‚úÖ Detecting external data corruption ‚úÖ Catching race condition edge cases ‚úÖ Legacy system integration where you can\u0026rsquo;t enforce up-front Poor Use Cases:\n‚ùå Primary mechanism for maintaining consistency ‚ùå Real-time transaction validation ‚ùå Systems where violations have immediate bad consequences Would you like me to:\nDesign a specific verification architecture for your use case? Show how to implement saga pattern for distributed invariant enforcement? Create patterns for automated remediation when violations are detected? Discuss how to determine which invariants should be enforced proactively vs. verified reactively? "},{"id":7,"href":"/musings/docs/misc/kiro-ide-comparison-tools/","title":"Kiro IDE: Comprehensive Analysis, Comparisons \u0026 Optimal Workflows","section":"Miscellaneous","content":"Kiro IDE: Comprehensive Analysis, Comparisons \u0026amp; Optimal Workflows#\rResearch compiled December 2025\nExecutive Summary#\rKiro is AWS\u0026rsquo;s new AI-powered IDE (launched July 2025, public preview) that introduces spec-driven development as an alternative to traditional \u0026ldquo;vibe coding.\u0026rdquo; Unlike competitors that focus on speed and autocomplete, Kiro emphasizes structure, planning, and production-ready code through a three-phase workflow: Requirements ‚Üí Design ‚Üí Implementation.\nKey Finding: The consensus across reviews is that Kiro trades raw speed for maintainability, documentation quality, and long-term project coherence. It\u0026rsquo;s best suited for complex projects, enterprise teams, and developers who value structure over rapid iteration.\nMajor Trends \u0026amp; Community Consensus#\r1. Spec-Driven Development as a Paradigm Shift#\rWhat reviewers are saying:\nMoving from \u0026ldquo;vibe coding\u0026rdquo; (prompt until it works) to structured planning represents a fundamental shift in AI-assisted development Kiro forces developers to think through requirements before writing code This approach mirrors traditional enterprise software engineering practices but with AI acceleration Quote from AWS re:Post review:\n\u0026ldquo;Start with the Spec - Never jump straight into coding. Use Kiro\u0026rsquo;s spec workflow to clarify requirements first.\u0026rdquo;\n2. The \u0026ldquo;Production-Ready\u0026rdquo; vs \u0026ldquo;Speed\u0026rdquo; Trade-off#\rConsensus: Kiro sacrifices initial velocity for long-term maintainability\nCommon observations:\nCursor/Windsurf: Faster for rapid prototyping and MVP development Kiro: Better for production systems that need documentation, tests, and team collaboration The task queuing system feels slow for agile/fast-paced environments Best for projects where \u0026ldquo;throwaway prototypes\u0026rdquo; are unacceptable From Medium review by Rajesh Sood:\n\u0026ldquo;Kiro ‚Üí Best for AWS-heavy teams needing structure + tests\u0026rdquo;\n3. Unique Agent Hooks System#\rWhat makes it special:\nEvent-driven automation that triggers on file saves, edits, or commits Can auto-update tests, documentation, security scans without manual intervention Acts like \u0026ldquo;an experienced developer watching over your shoulder\u0026rdquo; Popular use cases from community:\nAuto-commit to git after task completion Update documentation when code changes Run security scans before commits Keep tests synchronized with source code Kiro vs. Alternative Tools: Detailed Comparisons#\rKiro vs Claude Code#\rAspect Kiro Claude Code Interface Full IDE (VS Code fork) Terminal/CLI-based Workflow Spec-driven (requirements ‚Üí design ‚Üí tasks) Conversational, flexible Structure Enforced 3-phase workflow Developer-controlled Best For Complex projects, teams Quick tasks, CLI enthusiasts Documentation Auto-generated and synced Manual with .claude.md files Speed Slower (thorough) Faster (flexible) Cost $19-39/month (after preview) Included with API usage Key Insight from ashexplained.com: One developer successfully recreated Kiro\u0026rsquo;s workflow inside Claude Code using slash commands:\n\u0026ldquo;You get Kiro\u0026rsquo;s methodology without Kiro\u0026rsquo;s limitations. You keep your freedom. You keep your tools.\u0026rdquo;\nBlog post: Kiro IDE\u0026rsquo;s Workflow in Claude Code Slash Commands\nHybrid Approach (from Joe Njenga\u0026rsquo;s Medium article): Running Claude Code inside Kiro\u0026rsquo;s terminal creates an interesting combination:\nKiro handles project structure and specs Claude Code handles quick iterations and specific tasks \u0026ldquo;Sweet spot\u0026rdquo; for balancing structure with flexibility Kiro vs Cursor#\rAspect Kiro Cursor Philosophy Planning-first Code-first Autocomplete Basic Multi-line, unlimited (Pro) Models Claude Sonnet 3.7/4.0 only GPT, Claude, Gemini, Grok Context Spec files + steering docs Codebase indexing Diff View Side-by-side traditional Inline additions/deletions Approval Supervised mode (review each change) Auto-apply in agent mode Maturity Preview (July 2025) Established (2023) Price $19-39/mo (after preview) $20/mo + $20 credits From DEV Community review:\n\u0026ldquo;Unlike Cline or Cursor, which show additions and deletions in a single screen, Kiro uses a side-by-side diff view that I find more intuitive.\u0026rdquo;\nDataCamp\u0026rsquo;s tic-tac-toe test results:\nKiro: 200 lines of code, \u0026ldquo;perfectly captured requirements\u0026rdquo; Cursor: 250+ lines Kiro showed superior \u0026ldquo;spatial reasoning\u0026rdquo; in game design Best choice scenarios:\nCursor: Fast-moving teams, prototypes, MVPs, model flexibility Kiro: Enterprise environments, regulated industries, long-term maintenance Kiro vs Windsurf#\rAspect Kiro Windsurf Key Feature Spec-driven development Cascade agent (multi-step) Autocomplete Basic Supercomplete (unlimited free) Workflow Requirements ‚Üí Design ‚Üí Tasks Flow-based, non-disruptive Team Features Shared specs, hooks Memories, collaborative AWS Integration Native Standard Price $19/mo (1,000 tasks) $15/mo (500 credits) Local-first No Yes From premiersol.live comparison:\n\u0026ldquo;Kiro\u0026rsquo;s spec-driven, agentic approach is a clear winner, especially for teams managing technical debt, long-term maintenance, and high onboarding rates.\u0026rdquo;\n\u0026ldquo;Windsurf\u0026rsquo;s Cascade and Supercomplete offer streamlined, intuitive multi-file edits and \u0026lsquo;in-flow\u0026rsquo; coding for developers who want AI to reinforce their coding process instead of replacing it.\u0026rdquo;\nThe New Stack (December 2025):\n\u0026ldquo;AWS today launched Kiro, its answer to Windsurf and Cursor, with a focus on writing specs over prompts.\u0026rdquo;\nVentureBeat (timing note): Windsurf was acquired by Cognition just as Kiro launched - fortuitous timing for developers looking for alternatives.\nOptimal Kiro Workflows: Community Best Practices#\rCore Workflow Recommendations#\r1. Always Start with Specs (Don\u0026rsquo;t Jump to Code)#\rFrom AWS re:Post guide by Community Builder:\nNever jump straight into coding. Use Kiro\u0026#39;s spec workflow to clarify requirements first. You can also use the free-form chat mode whenever needed for general assistance and vibe coding.Three-phase workflow:\nRequirements: Generate user stories in EARS format (Event-driven Acceptance Requirements Specification) Design: Create technical design docs with data flow, API endpoints, schemas Tasks: Break down into sequenced, dependency-aware implementation tasks 2. Strategic Use of Autopilot vs Supervised Mode#\rAutopilot Mode (autonomous):\nUse for: Foundational components, boilerplate code, standard patterns Agent works independently without approval Best for: Data models, Lambda functions, infrastructure code Supervised Mode (collaborative):\nUse for: Critical components, complex logic, security-sensitive code Review and approve each change Best for: Policy engines, authentication, payment processing From AWS re:Post:\n\u0026ldquo;For my AI Compliance Auditor project, Autopilot mode was perfect for implementing the foundational components. I used Supervised Mode for critical components like the policy engine.\u0026rdquo;\n3. Configure Steering Files Early#\rWhat are steering files?\nMarkdown files in .kiro/steering/ that guide AI behavior Define coding standards, architectural patterns, team practices Act as project-specific knowledge for the AI Best practices from Kiro docs:\nKeep files focused: One domain per file (API standards, testing, deployment) Include context: Explain why decisions were made, not just what Provide examples: Code snippets, before/after comparisons Security first: Never include API keys or sensitive data Common steering files:\napi-standards.md - REST conventions, error formats, versioning testing-standards.md - Unit test patterns, coverage expectations deployment-workflow.md - Build procedures, rollback strategies 4. Leverage Agent Hooks for Automation#\rStart simple, build complexity: From Kiro blog on agent hooks:\n\u0026ldquo;Begin with basic file-to-file relationships like updating tests when you change the source code. You\u0026rsquo;ll see the value right away and can build up to more complex workflows.\u0026rdquo;\nPopular hook configurations:\n# Test Synchronization Hook when: source code changes action: Update corresponding unit tests # Documentation Helper when: new feature added action: Update README and API docs # Git Auto-commit when: Kiro completes task action: Commit changes with descriptive message # Security Scanner when: file saved action: Check for secrets, vulnerabilities # Code Quality Check when: file edited action: Analyze for code smells, suggest improvementsFrom AWS Community Builder:\n\u0026ldquo;Agent Hooks automate workflows by monitoring file changes and triggering actions. I configured several hooks that dramatically improved my workflow just by asking Kiro to create them using natural language prompts!\u0026rdquo;\n5. Use MCP Servers Extensively#\rModel Context Protocol integration:\nConnect external tools and data sources Context7 and AWS Labs MCP servers provide AWS-specific help Use for every AWS-related task From best practices guide:\n\u0026ldquo;Leverage MCP Servers Extensively - The Context7 and AWS Labs MCP servers provide incredible value.\u0026rdquo;\n6. Keep Tasks Granular#\rDon\u0026rsquo;t create massive tasks:\nBreak complex features into small, manageable tasks in tasks.md Execute one task at a time for better results Easier to review, rollback, and debug From Kiro docs:\n\u0026ldquo;We do not recommend executing all tasks at once as we recommend a task-wise execution to get better results.\u0026rdquo;\n7. Iterate on Requirements#\rSpecs are living documents:\nDon\u0026rsquo;t be afraid to refine requirements as you learn Kiro syncs specs with evolving codebase Manual updates to specs refresh tasks automatically From AWS re:Post:\n\u0026ldquo;Iterate on Requirements - Don\u0026rsquo;t be afraid to refine your requirements as you learn more about the problem space.\u0026rdquo;\n8. Mix Vibe and Spec Modes Strategically#\rVibe Mode (chat-based):\nQuick questions and clarifications Exploratory coding Small bug fixes Spec Mode (structured):\nNew features Complex refactoring Team projects Production code From DEV Community:\n\u0026ldquo;If you\u0026rsquo;re doing full-stack development, I strongly recommend spec mode.\u0026rdquo;\nAdvanced Workflow Techniques#\rKiro Best Practices Boilerplate#\rA GitHub repository provides pre-configured steering docs and hooks:\nRepository: awsdataarchitect/kiro-best-practices\nQuick setup:\n# Add to existing project cd your-existing-project mkdir -p .kiro \u0026amp;\u0026amp; curl -L https://github.com/awsdataarchitect/kiro-best-practices/archive/main.tar.gz | tar -xz --strip-components=2 -C .kiro kiro-best-practices-main/.kiroWhat it includes:\nPre-configured steering documents for best practices Automated quality checks (testing, linting, security) Git workflow automation Code review helpers From AWS re:Post:\n\u0026ldquo;The Kiro Best Practices Boilerplate solves this by creating an AI-driven development environment that automatically enforces best practices, runs quality checks, and streamlines workflows through intelligent automation.\u0026rdquo;\nProperty-Based Testing (PBT)#\rNew feature in recent updates:\nGenerates hundreds of test cases with random inputs Detects edge cases traditional unit tests miss Verifies spec requirements with evidence How it works: Unlike unit tests that check specific cases, PBT runs the same test hundreds of times with different inputs to find edge cases.\nCheckpointing System#\rRoll back AI mistakes:\nRevert to previous conversation states Retrace agent\u0026rsquo;s steps when ideas go sideways Review decision history Common Workflows \u0026amp; Use Cases#\r1. Building Production SaaS (Solo Developer)#\rFrom AWS re:Post case study:\n\u0026ldquo;I can deploy complete startup ideas - from concept to monetization - in less than one hour.\u0026rdquo;\nWorkflow:\nDefine business requirements in spec Let Kiro generate architecture for AWS services Use hooks for auto-testing and documentation Supervised mode for payment processing (Stripe) Deploy to AWS with infrastructure as code Result: \u0026ldquo;AI-Powered Solo Enterprise era\u0026rdquo; - one person can build enterprise-scale applications\n2. Legacy Code Refactoring#\rFrom Lothar Schulz comparison:\n\u0026ldquo;When faced with a non-functional React application generated through Kiro\u0026rsquo;s spec-driven approach, GitHub Copilot proved its value through rapid iteration.\u0026rdquo;\nHybrid approach:\nUse Kiro for initial refactoring spec and design Switch to Cursor/Copilot for rapid iteration on broken code Return to Kiro for final documentation sync 3. Cloud Infrastructure Development#\rAWS-native workflows:\nTerraform modules with built-in best practices Lambda functions with automatic testing Security scanning integrated into hooks Compliance checks automated From AWS re:Post:\n\u0026ldquo;In my role designing AWS Cloud and AI solutions with Terraform and Python, spec-driven development with Kiro has brought code relevancy and quality to a whole new level.\u0026rdquo;\n4. Cross-Platform Development#\rCase study from DEV Community: Developer rebuilt CrossPlatformDiskTest (CPDT) in Rust using Kiro:\nOriginal: .NET with 500k Android downloads Kiro version: Rust cross-platform with full test coverage Agent maintained coherence across extended autonomous work periods Some CI/CD issues but overall successful Lesson: Kiro can maintain long-running autonomous tasks better than chat-based tools\nTweaks \u0026amp; Customizations to Default Workflow#\r1. Custom Steering for Project-Specific Patterns#\rBeyond defaults: Create custom steering files for:\nFramework-specific patterns (React hooks, Vue composition) Company coding standards Domain-specific logic (fintech, healthcare compliance) Performance optimization rules Example custom steering:\n# React Component Standards ## Naming Conventions - Components: PascalCase (UserProfile.tsx) - Hooks: camelCase with \u0026#39;use\u0026#39; prefix (useUserData.ts) - Utils: camelCase (formatDate.ts) ## Architecture Patterns - Always use functional components - Prefer composition over inheritance - One component per file - Co-locate tests with components ## Performance Rules - Use React.memo for expensive components - Implement useMemo for complex calculations - Prefer useCallback for event handlers passed to children2. Global vs Project-Specific Configuration#\rGlobal steering (~/.kiro/steering/):\nApply across all projects Personal coding preferences Reusable patterns Project-specific (.kiro/steering/):\nTeam standards Framework requirements Project architecture Support for AGENTS.md standard: Recent update adds support for defining guidelines in AGENTS.md files\n3. Hook Chaining for Complex Workflows#\rSequential automation:\nFile Save ‚Üí Run Tests ‚Üí Update Docs ‚Üí Security Scan ‚Üí Git CommitConditional hooks:\nOnly run expensive operations on specific file patterns Skip tests for documentation-only changes Trigger different workflows for different branches 4. Context Optimization#\rUse #filename references: Instead of pasting entire files, reference them:\nCreate a spec for adding user authentication.\rReference #auth-requirements.md and #security-policy.mdBenefit: Reduces token usage, keeps context focused\n5. MVP vs Comprehensive Mode#\rRecent feature (changelog): When creating specs, choose:\n\u0026ldquo;Keep optional tasks (faster MVP)\u0026rdquo; - Marks tests/docs as optional (‚ú±) \u0026ldquo;Make all tasks required\u0026rdquo; - Comprehensive coverage from start Strategy: Start with MVP mode, add optional tasks later for production\n6. Background Process Management#\rNew feature: Dev servers, build watchers run in background\nnpm run dev starts automatically Agent returns control immediately Process continues tracking without blocking terminal Challenges \u0026amp; Limitations (Honest Assessment)#\rKnown Issues from Community#\r1. Speed Trade-off#\rFrom Geeky Gadgets review:\n\u0026ldquo;Kiro\u0026rsquo;s meticulous task queuing system may feel slow for developers in fast-paced, agile environments.\u0026rdquo;\nWhen it matters: Rapid prototyping, hackathons, quick MVPs\n2. Agent Stubbornness#\rFrom ludditus.com:\n\u0026ldquo;Kiro requires a different kind of project management. You\u0026rsquo;re not just writing code anymore; you\u0026rsquo;re steering an AI that can get overwhelmed by complexity, sometimes prefers workarounds over root cause analysis.\u0026rdquo;\nSolution: Use supervised mode for critical sections, be explicit in requirements\n3. Limited Model Selection#\rOnly Claude Sonnet 3.7 and 4.0 No GPT, Gemini, or other models No thinking mode in vibe sessions Recent addition: Claude Opus 4.5 (experimental) Comparison: Cursor offers GPT, Claude, Gemini, Grok\n4. Import Limitations#\rFrom DataCamp tutorial:\n\u0026ldquo;If you\u0026rsquo;ve moved to other VS Code-based editors like Cursor, you might hit some friction‚ÄîKiro doesn\u0026rsquo;t support importing from these newer tools.\u0026rdquo;\nWorkaround: Import from original VS Code, then configure manually\n5. Keybinding Conflicts#\rCommon issue: Kiro overrides certain VS Code shortcuts for AI features\nRequires manual reconfiguration Can break muscle memory from other editors 6. Database Wipeout Bug#\rFrom DEV Community (Maxim Saplin):\n\u0026ldquo;I can state in every single location a critical instruction (never add RefreshDatabase to tests) and still guarantee that every new test wipes my local development database.\u0026rdquo;\nStatus: Temporary reliability issues being addressed\n7. CI/CD Complexity#\rMultiple reviewers noted CI/CD setup can break and require manual fixes\n8. Billing Issues (resolved)#\rFrom DEV Community comments:\n\u0026ldquo;They had a bug last week and everybody was basically paying double\u0026hellip; they are supposed to be giving refunds starting today\u0026rdquo;\nStatus: Fixed as of late 2025\nWho Should Use Kiro? (Decision Framework)#\r‚úÖ Ideal Use Cases#\rEnterprise Teams\nNeed consistent documentation Multiple developers on codebase Long-term maintenance concerns Regulatory compliance requirements AWS-Heavy Workflows\nNative AWS integration MCP servers for AWS services Infrastructure as code projects Complex, Long-Lived Projects\nWhere \u0026ldquo;throwaway prototypes\u0026rdquo; are unacceptable Technical debt is a concern Team onboarding is frequent Structured Learners\nBeginners who want to understand architecture Developers transitioning to new stacks Teams adopting best practices Solo Entrepreneurs Building SaaS\nNeed production-ready code quickly Want automated testing/docs Building for scale from day one ‚ùå Not Ideal For#\rRapid Prototyping / Hackathons\nSpec workflow adds overhead Speed is more important than structure Quick Bug Fixes\nOverhead of spec creation not justified Better to use Cursor or Claude Code Teams Requiring Model Flexibility\nLimited to Claude models Can\u0026rsquo;t A/B test different models Offline Development\nRequires internet connection Sign-in required (Google, GitHub, AWS) Non-AWS Ecosystems (debatable)\nWorks with any stack, but AWS integration is strongest selling point Pricing \u0026amp; Cost Optimization#\rCurrent Pricing (Post-Preview)#\rFree Tier:\n50 agent tasks/month During preview: \u0026ldquo;reasonable limits at no cost\u0026rdquo; Kiro Pro - $19/month:\n1,000 interactions per month All features Kiro Pro+ - $39/month:\n3,000 interactions per month All features Enterprise:\nCustom pricing Team onboarding Enterprise billing RBAC, audit trails Cost Optimization Strategies#\r1. Hybrid Approach (from Cursor IDE blog):#\r\u0026ldquo;The hybrid approach works because Kiro excels at project initialization and architecture, while API access handles routine coding tasks at lower cost.\u0026rdquo;\nStrategy:\nUse Kiro for: Specs, architecture, initial setup Use Claude API directly for: Routine tasks, simple edits Savings: ~75% on coding costs 2. Use Vibe Mode for Simple Tasks#\rVibe conversations don\u0026rsquo;t consume as many interactions Save spec mode for complex features 3. Optimize Hook Triggers#\rDon\u0026rsquo;t run expensive operations on every file save Use specific file patterns to trigger hooks Batch operations when possible 4. Free During Preview#\rTake advantage: Currently free with generous limits Timeline: Pro plans activate post-preview (timeline TBD)\nThe Future of Kiro (Roadmap Signals)#\rRecent Updates (Changelog Analysis)#\rClaude Opus 4.5 Support (experimental)\nMost intelligent model for complex tasks Available in IDE and CLI Enterprise Billing\nLarge team onboarding Admin controls Checkpointing\nRevert to previous conversation states Undo agent decisions Property-Based Testing\nGenerate test evidence for specs Edge case detection MCP Remote Servers\nConnect to cloud services via HTTP Broader integration options Multi-Root Workspaces\nHandle monorepos better Multiple projects in one IDE Community Predictions#\rFrom Park Labs (Korean blog): Timeline predictions for 2025-2026:\nQ3 2025: Kiro formal release Q4 2025: Enterprise Kiro expansion, AI model integration acceleration Q1 2026: Hybrid tools emerge, standardization discussions Q2 2026: AI coding tool consolidation, next-gen development paradigm Trend: Hybrid approaches combining multiple tools will dominate\nRecommended Resources#\rEssential Reading#\rOfficial Kiro Documentation\nhttps://kiro.dev/docs/ Comprehensive guides for getting started AWS re:Post: \u0026ldquo;Kiro Agentic AI IDE: Beyond a Coding Assistant\u0026rdquo;\nhttps://repost.aws/articles/AROjWKtr5RTjy6T2HbFJD_Mw/ Real-world case study building AI Compliance Auditor Best practices from AWS Community Builder \u0026ldquo;Kiro IDE\u0026rsquo;s Workflow in Claude Code Slash Commands\u0026rdquo;\nhttps://ashexplained.com/kiro-ide-workflow-claude-code-slash-commands/ Recreating Kiro methodology in Claude Code Open source implementation on GitHub DataCamp Tutorial: \u0026ldquo;Kiro AI: A Guide With Practical Examples\u0026rdquo;\nhttps://www.datacamp.com/tutorial/kiro-ai Hands-on comparison with Cursor Practical coding tests (tic-tac-toe benchmark) \u0026ldquo;Accelerating AI Development Workflows: The Kiro Best Practices Boilerplate\u0026rdquo;\nhttps://repost.aws/articles/ARXfJeAJ14Sh65Odc0rw6wOg/ Pre-configured steering docs and hooks GitHub repository with templates Comparison Reviews#\r\u0026ldquo;Battle of the AI Coding Agents\u0026rdquo; - Lothar Schulz\nhttps://www.lotharschulz.info/2025/09/30/battle-of-the-ai-coding-agents-github-copilot-vs-claude-code-vs-cursor-vs-windsurf-vs-kiro-vs-gemini-cli/ Comprehensive comparison across 6 tools Exercism Rust challenges as benchmarks \u0026ldquo;Kiro vs Cursor: Which AI IDE Is Best for Devs in 2025?\u0026rdquo; - ScaleVise\nhttps://scalevise.com/resources/kiro-vs-cursor-ai-ide/ Enterprise perspective 15-min IDE Fit Review offer \u0026ldquo;AI-Powered IDEs: Cursor vs Windsurf vs Kiro\u0026rdquo; - Rajesh Sood (Medium)\nhttps://medium.com/@soodrajesh/ai-powered-ides-cursor-vs-windsurf-vs-kiro-f7f7795f0c9c Pricing comparison Real-world examples The New Stack: \u0026ldquo;Kiro Is AWS\u0026rsquo;s Specs-Centric Answer to Windsurf and Cursor\u0026rdquo;\nhttps://thenewstack.io/kiro-is-awss-specs-centric-answer-to-windsurf-and-cursor/ Industry analysis EARS notation explanation Hands-On Experiences#\r\u0026ldquo;Why I Ditched Cursor for Kiro\u0026rdquo; - DEV Community\nhttps://dev.to/fallon_jimmy/why-i-ditched-cursor-for-kiro-the-ultimate-ai-ide-for-beginners-ja9 Beginner perspective Diff view comparisons \u0026ldquo;The magic of Amazon\u0026rsquo;s Kiro: my 1st vibe-coded PyQt6 app!\u0026rdquo; - Ludditus\nhttps://ludditus.com/2025/07/25/the-magic-of-amazons-kiro/ Honest review including \u0026ldquo;The Bad\u0026rdquo; and \u0026ldquo;The Ugly\u0026rdquo; PyQt6 development experience \u0026ldquo;AI Dev: Testing Kiro\u0026rdquo; - Maxim Saplin (DEV Community)\nhttps://dev.to/maximsaplin/ai-dev-testing-kiro-3b5j Rebuilding CPDT in Rust Long-running task maintenance \u0026ldquo;I Tried Running Claude Code Inside Kiro\u0026rdquo; - Joe Njenga (Medium)\nhttps://medium.com/@joe.njenga/i-tried-running-claude-code-inside-kiro-amazons-ide-and-got-a-sweet-surprise-d8fddcb93d22 Hybrid workflow experiment Finding the \u0026ldquo;sweet spot\u0026rdquo; Official Kiro Resources#\rKiro Blog: \u0026ldquo;Introducing Kiro\u0026rdquo;\nhttps://kiro.dev/blog/introducing-kiro/ Official announcement Product philosophy Kiro Blog: \u0026ldquo;Automate your development workflow with Kiro\u0026rsquo;s AI agent hooks\u0026rdquo;\nhttps://kiro.dev/blog/automate-your-development-workflow-with-agent-hooks/ Deep dive on hooks Configuration examples Kiro Changelog\nhttps://kiro.dev/changelog/ Latest features and updates Product evolution tracking Video Content (Limited availability as of Dec 2025)#\rNote: YouTube content specifically about Kiro is limited as the tool only launched in July 2025. Most video tutorials are from:\nAWS Summit New York (July 15, 2025) - launch announcement Individual developer experiments on personal channels \u0026ldquo;Syntax\u0026rdquo; YouTube channel review (mentioned in Geeky Gadgets article) Recommendation: Check Kiro\u0026rsquo;s official YouTube presence and AWS\u0026rsquo;s developer channels for official tutorials.\nKey Takeaways \u0026amp; Final Recommendations#\rThe Kiro Philosophy#\rStructure Over Speed\nTrading rapid iteration for long-term maintainability Planning before coding prevents \u0026ldquo;AI slop\u0026rdquo; Documentation as First-Class Citizen\nSpecs stay synced with code No more documentation drift Team Collaboration Built-In\nShared specs as single source of truth Hooks enforce consistency across team Production-Ready from Start\nAutomated testing, security scanning Compliance checks integrated Optimal Team Strategy#\rHybrid Multi-Tool Approach:\nKiro: Architecture, specs, production features Cursor: Rapid iteration, bug fixes, experimentation Claude Code: Terminal tasks, quick scripts, CI/CD From Park Labs Korean blog:\n\u0026ldquo;Start with Cursor for speed, introduce Kiro\u0026rsquo;s structure as project grows, handle specific tasks with Claude Code.\u0026rdquo;\nWhen to Choose Kiro#\rChoose Kiro if you answer \u0026ldquo;yes\u0026rdquo; to:\nIs this a long-term project (6+ months)? Will multiple developers work on this? Do you need comprehensive documentation? Is production quality critical from the start? Are you in a regulated industry? Do you use AWS extensively? Choose alternatives if:\nBuilding a prototype or MVP (‚Üí Cursor) Need maximum model flexibility (‚Üí Cursor) Want fastest autocomplete (‚Üí Windsurf) Prefer terminal workflows (‚Üí Claude Code) Working offline (‚Üí None of these work) The \u0026ldquo;AI-Powered Solo Enterprise\u0026rdquo; Vision#\rMost ambitious claim from AWS re:Post:\n\u0026ldquo;We\u0026rsquo;re witnessing the dawn of the AI-Powered Solo Enterprise era, where brilliant founders or solopreneurs with game changing ideas can build billion dollar companies with zero employees.\u0026rdquo;\nIs it realistic?\nTechnology: Yes, Kiro provides the structure Business reality: Solo technical development ‚â† solo business Best use: Technical co-founders building MVP without hiring developers Final Verdict#\rKiro represents a philosophical shift in AI-assisted coding:\nFrom \u0026ldquo;move fast and break things\u0026rdquo; to \u0026ldquo;move deliberately and build right\u0026rdquo; From \u0026ldquo;AI as autocomplete\u0026rdquo; to \u0026ldquo;AI as software architect\u0026rdquo; From \u0026ldquo;prototype and rewrite\u0026rdquo; to \u0026ldquo;prototype becomes production\u0026rdquo; Best described as: \u0026ldquo;An IDE that thinks like a senior engineer teaching you to fish, not giving you fish.\u0026rdquo;\nRecommendation for most developers: Try Kiro during free preview to learn spec-driven development principles, even if you ultimately use other tools. The methodology is valuable regardless of the tool.\nAppendix: Quick Reference#\rEssential Kiro Commands#\r# Installation kiro . # Open current directory kiro /path/to/project # Open specific project # In IDE Cmd/Ctrl + L # Open chat Cmd/Ctrl + Shift + P # Command paletteSpec Workflow Cheatsheet#\rOpen chat, describe feature Ask: \u0026ldquo;Create a spec\u0026rdquo; Review requirements ‚Üí Approve Review design ‚Üí Approve Review tasks ‚Üí Execute one by one Use supervised mode for critical sections Hook Template#\r{ \u0026#34;enabled\u0026#34;: true, \u0026#34;when\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;fileEdited\u0026#34;, \u0026#34;patterns\u0026#34;: [\u0026#34;src/**/*.ts\u0026#34;] }, \u0026#34;action\u0026#34;: \u0026#34;npm test -- --silent\u0026#34; }Steering File Template#\r# [Domain Name] Standards ## Context [Why these standards exist] ## Rules - Rule 1 with rationale - Rule 2 with rationale ## Examples [Before/after code examples] ## Anti-patterns [What to avoid]Quick Comparison Matrix#\rNeed Best Tool Fastest autocomplete Windsurf Most models Cursor Best structure Kiro Terminal workflow Claude Code Enterprise features Kiro Rapid prototyping Cursor Best documentation Kiro Lowest cost Claude Code (API) Last updated: December 2025 Sources: 30+ blog posts, documentation, community reviews Research compiled from July-December 2025 content\n"},{"id":8,"href":"/musings/docs/misc/langchain-architecture/","title":"Langchain Architecture","section":"Miscellaneous","content":"LangChain and LangGraph: Architectural Overview#\rLangChain: The Foundation Library#\rThink of LangChain as a comprehensive toolkit for building LLM-powered applications. It\u0026rsquo;s essentially a collection of composable abstractions and integrations that handle common LLM application patterns.\nCore Architecture:\nAbstraction Layers: Provides standardized interfaces for LLMs, vector stores, document loaders, retrievers, and tools Composition Pattern: Everything is designed to be chained together (hence the name) - you build pipelines by connecting components Integration Hub: Pre-built connectors for 100+ LLMs, databases, APIs, and services LCEL (LangChain Expression Language): A declarative syntax for composing chains using the pipe operator - think Unix pipes for LLM workflows What LangChain gives you:\nDocument processing pipeline (loaders, splitters, transformers) Vector store abstractions (Chroma, Pinecone, FAISS, etc.) Prompt templates and management Output parsers and structured data extraction Tool/function calling abstractions Memory systems for maintaining conversation state RAG (Retrieval Augmented Generation) patterns out of the box Mental Model: LangChain is like a well-designed SDK for LLM applications - lots of pre-built components that you compose together. It handles the \u0026ldquo;plumbing\u0026rdquo; so you can focus on application logic.\nLangGraph: The Orchestration Layer#\rLangGraph is a newer library (built on top of LangChain) that addresses a specific limitation: stateful, cyclic workflows with complex control flow.\nThe Problem LangGraph Solves: LangChain\u0026rsquo;s chain-based model is great for linear or tree-like workflows, but breaks down when you need:\nCycles (agent tries something, fails, tries again differently) Complex state management across multiple steps Dynamic routing based on intermediate results Human-in-the-loop approval gates Parallel execution with synchronization points Core Architecture:\nGraph-Based State Machines: You define workflows as directed graphs where nodes are functions and edges are transitions Persistent State: Built-in state management with checkpointing - can pause/resume anywhere Conditional Edges: Dynamic routing based on state/results Cycles and Loops: First-class support for iterative workflows Streaming: Can stream intermediate results as the graph executes Key Concepts:\nStateGraph: The main abstraction - you define a schema for your state and add nodes that transform it Nodes: Python functions that take current state and return state updates Edges: Define flow between nodes (can be conditional) Checkpointers: Persistence layer for state (memory, SQLite, Redis, etc.) Reducers: Control how state updates merge (append, overwrite, custom logic) Mental Model: LangGraph is like a workflow engine specifically designed for LLM applications. Think Apache Airflow or Temporal, but optimized for agentic AI patterns where you need cycles, retries, and complex state management.\nHow They Relate#\rLangChain (Foundation)\r‚îú‚îÄ‚îÄ Components: LLMs, Tools, Retrievers, Memory\r‚îú‚îÄ‚îÄ Patterns: Chains, Prompts, Output Parsers\r‚îî‚îÄ‚îÄ Integrations: 100+ services\rLangGraph (Orchestration)\r‚îú‚îÄ‚îÄ Built on LangChain components\r‚îú‚îÄ‚îÄ Adds: State machines, cycles, checkpointing\r‚îî‚îÄ‚îÄ Use case: Complex multi-step agentic workflowsRelationship:\nLangGraph uses LangChain components (LLMs, tools, retrievers) as building blocks You can use LangChain without LangGraph for simple linear workflows LangGraph is necessary when you need stateful, cyclic, or complex control flow They share the same ecosystem (LangSmith for observability, same integrations) When to Use What#\rUse LangChain alone when:\nSimple question-answering or RAG Linear processing pipelines Document summarization/transformation Basic chatbots with simple memory Use LangGraph (which includes LangChain) when:\nMulti-agent systems with complex interactions Workflows that need to retry/branch based on results Long-running processes that need checkpointing Human-in-the-loop workflows Your use case: Multi-task data extraction with error recovery and state management For Your Conference Summaries Project#\rYour 4-task architecture maps to LangGraph like this:\nTask 1 (Conference Discovery): LangGraph node that searches, validates URLs, updates shared state Task 2 (Raw Data Extraction): LangGraph node with retry logic, rate limiting, checkpoint-driven resumability Task 3 (AI Processing): LangGraph node that processes in batches, with conditional routing based on confidence scores Task 4 (Issue Resolution): LangGraph node with human-in-the-loop gates, automatic retry after fixes\nThe shared data store becomes the StateGraph\u0026rsquo;s state, persisted via checkpointers. Each task becomes a node (or subgraph) with edges defining when to transition between tasks.\nLearning Roadmap#\rPhase 1: LangChain Fundamentals\nUnderstand LCEL (LangChain Expression Language) syntax Learn the core abstractions (LLMs, prompts, output parsers) Explore document loaders and text splitters Understand tool/function calling Phase 2: LangGraph Basics\nStateGraph concept and state management Adding nodes and edges Conditional routing Basic checkpointing Phase 3: Advanced LangGraph\nSubgraphs for task decomposition Custom reducers for complex state updates Parallel execution patterns Error handling and retry strategies Phase 4: Production Patterns\nLangSmith integration for observability Persistence strategies (SQLite vs Redis) Streaming and progress reporting Testing and debugging workflows Key Resources:\nLangChain docs focus on component usage LangGraph docs focus on workflow patterns LangSmith for understanding execution traces The \u0026ldquo;ReAct agent\u0026rdquo; pattern is the canonical example that shows why LangGraph exists Does this give you the architectural understanding you need to start digging deeper?\n"},{"id":9,"href":"/musings/docs/misc/llm-tracing-metrics-comparison/","title":"Llm Tracing Metrics Comparison","section":"Miscellaneous","content":"LLM-Focused Observability Platforms#\rYou\u0026rsquo;re absolutely right - generic APM tools miss the critical LLM-specific challenges: prompt debugging, chain-of-thought analysis, token costs, model behavior, and agentic decision-making visibility.\nHere are the platforms built specifically for LLM/agent observability:\nOpen Source Options#\r1. Phoenix (Arize AI)#\rBest for: Comprehensive LLM observability with strong evaluation focus\nLLM-Specific Features:\nTrace visualization: See entire agent execution flow with LLM calls highlighted Prompt/response tracking: Every LLM interaction logged with inputs/outputs Embedding analysis: Visualize embedding spaces, detect drift Evaluation metrics: Built-in evals for hallucination, toxicity, relevance Dataset management: Track test sets and evaluation runs Retrieval analysis: Debug RAG systems (what was retrieved vs what was used) Cost tracking: Token usage and cost per trace LangChain integration: Native instrumentation What makes it better than LangSmith:\nFully open source (self-hostable) Stronger evaluation framework Better for production monitoring at scale ML-focused (drift detection, embedding visualization) Cons:\nLess polished UI than LangSmith Smaller ecosystem No built-in prompt playground Deployment: Docker, Python package, or cloud (coming)\nUse case for you: Primary LLM observability platform, especially for quality monitoring and debugging agent decisions\n2. OpenLLMetry (Traceloop)#\rBest for: OpenTelemetry-native LLM tracing\nLLM-Specific Features:\nBuilt on OpenTelemetry: Use standard OTel tooling Automatic instrumentation: LangChain, LlamaIndex, OpenAI SDK, etc. Prompt tracking: Captures full prompt templates and variables Association tracking: Links prompts to outputs across iterations Multi-framework: Not just LangChain Cost calculation: Automatic token cost tracking What makes it unique:\nPure OTel implementation (true vendor neutrality) Can send to any OTel backend Minimal code changes Framework-agnostic Cons:\nRequires separate backend for visualization (Grafana, Jaeger, etc.) No evaluation framework built-in Less LLM-specific analysis than Phoenix Deployment: Python SDK + any OTel backend\nUse case for you: If you want maximum vendor neutrality and already have OTel infrastructure\n3. LangWatch#\rBest for: Monitoring and analytics for production LLM apps\nLLM-Specific Features:\nUser analytics: Track user sessions, conversation flows Quality metrics: Response quality, hallucination detection Cost tracking: Per-user, per-conversation cost analysis A/B testing: Compare different prompts/models Real-time alerts: On quality degradation, cost spikes Conversation replay: Debug user sessions What makes it different:\nProduct/user-focused (not just dev debugging) Analytics dashboard for business metrics Good for production monitoring Cons:\nLess focused on development/debugging More product analytics than agent introspection Smaller community Deployment: Open source + cloud offering\nUse case for you: If you\u0026rsquo;re building user-facing features and need product analytics\n4. Lunary#\rBest for: Simple, focused LLM observability\nLLM-Specific Features:\nTrace viewing: Clean UI for LLM call sequences Prompt management: Version control for prompts User feedback: Collect thumbs up/down on responses Analytics: Usage patterns, costs, latency Datasets: Manage test cases LangChain integration: Native support What makes it appealing:\nVery simple to get started Clean, focused UI Good for small teams Cons:\nLess sophisticated than Phoenix Fewer evaluation features Smaller feature set overall Deployment: Docker or cloud\nUse case for you: If you want something simpler than Phoenix but more LLM-focused than generic tools\nCommercial Options#\r1. Weights \u0026amp; Biases (W\u0026amp;B) - Weave#\rBest for: ML teams already using W\u0026amp;B\nLLM-Specific Features:\nTrace visualization: Detailed execution graphs Prompt tracking: Version control and comparison Model evaluation: A/B testing, automated evals Dataset versioning: Track training/eval datasets Cost tracking: Token usage across experiments Integration: Works with LangChain, LlamaIndex, custom code Collaboration: Team-based prompt development What makes it strong:\nExcellent if you\u0026rsquo;re already in W\u0026amp;B ecosystem Strong evaluation and experimentation features Great for research ‚Üí production transition Powerful versioning and comparison tools Cons:\nExpensive at scale Overkill if you don\u0026rsquo;t use other W\u0026amp;B features More experiment-focused than production monitoring Pricing: Free tier, then usage-based\nUse case for you: If you need strong experimentation and evaluation capabilities\n2. Helicone#\rBest for: Simple, cost-effective LLM observability\nLLM-Specific Features:\nProxy-based: Routes LLM calls through their proxy Automatic logging: Zero code changes (just change endpoint) Cost tracking: Detailed token/cost analytics Caching: Built-in prompt caching to reduce costs Rate limiting: Prevent runaway costs User segmentation: Track costs by user/feature Prompt analytics: Popular prompts, success rates What makes it unique:\nEasiest integration (proxy-based) Strong cost optimization features No SDK changes required Cons:\nProxy adds latency Less detailed tracing than LangSmith Primarily for OpenAI/Anthropic APIs Limited agent workflow visualization Pricing: Free tier, then per-request\nUse case for you: If cost control is primary concern and you use standard APIs\n3. LangFuse#\rBest for: Open-source-first commercial LLM observability\nLLM-Specific Features:\nTrace visualization: Detailed LLM chain execution Prompt management: Versioning, deployment, rollback Evaluation: Score traces, run evals, compare models User feedback: Collect and analyze user ratings Cost analytics: Token usage, cost breakdowns Dataset management: Test sets for evaluation Sessions: Group related traces (conversations, tasks) LangChain integration: Native support What makes it compelling:\nOpen source core (self-hostable) Commercial cloud offering Very active development Feature parity with LangSmith Better pricing model Cons:\nNewer than LangSmith Smaller community Some features still maturing Pricing: Open source free, cloud has generous free tier\nUse case for you: Strong alternative to LangSmith with better economics and self-hosting option\n4. HumanLoop#\rBest for: Prompt engineering and iteration\nLLM-Specific Features:\nPrompt IDE: Visual prompt development environment A/B testing: Compare prompts systematically Evaluation: Automated testing of prompt variations Versioning: Track prompt changes over time Monitoring: Production prompt performance User feedback: Collect ratings on outputs Model routing: Switch between models dynamically What makes it unique:\nStrongest prompt engineering workflow Collaboration features for non-technical users Good for teams with PM/designers involved Cons:\nMore prompt-focused, less trace-focused Less detailed agent execution debugging Expensive for high volume Pricing: Usage-based\nUse case for you: If prompt quality and iteration speed are critical\n5. Braintrust#\rBest for: Evaluation and testing focus\nLLM-Specific Features:\nEvaluation framework: Comprehensive testing for LLM apps Dataset management: Version control for test sets Scoring: Automated and human scoring Experiments: Compare models, prompts, parameters Monitoring: Production quality tracking Trace logging: Detailed execution capture Cost tracking: Token usage analytics What makes it strong:\nBest-in-class evaluation tooling Strong CI/CD integration Good for quality-focused teams Academic rigor in evaluation Cons:\nMore evaluation than debugging Less real-time monitoring features Newer platform Pricing: Free tier, then usage-based\nUse case for you: If evaluation and quality assurance are primary concerns\n6. Confident AI (Deepeval)#\rBest for: Testing and evaluation framework\nLLM-Specific Features:\nEvaluation metrics: Pre-built (hallucination, bias, toxicity) Test framework: Unit testing for LLM apps Benchmarking: Compare model performance Monitoring: Production quality tracking Dataset versioning: Test set management CI/CD integration: Automated testing in pipelines What makes it different:\nTesting-first approach Strong metric library Good for quality assurance workflows Cons:\nLess observability, more testing Smaller feature set for debugging Less mature than alternatives Pricing: Open source + cloud\nUse case for you: Complement to observability for QA automation\nFeature Comparison Matrix#\rPlatform Trace Viz Prompt Mgmt Evaluation Cost Track Agent Debug Self-Host Price Phoenix ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê ‚úÖ Free OpenLLMetry ‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê ‚≠ê ‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê ‚úÖ Free LangWatch ‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê ‚úÖ Free/Paid Lunary ‚≠ê‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê ‚úÖ Free/Paid W\u0026amp;B Weave ‚≠ê‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê‚≠ê ‚ùå $$ Helicone ‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê ‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê ‚ùå $ LangFuse ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê‚≠ê ‚úÖ Free/Paid HumanLoop ‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê ‚ùå $$ Braintrust ‚≠ê‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê‚≠ê ‚ùå $ LangSmith ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê ‚ùå $$ Specific Agent Debugging Features#\rWhat You Need for Agentic Workflows:#\rDecision point visibility: See why agent chose Tool A vs Tool B State tracking: View state evolution across LangGraph nodes Retry/loop analysis: Understand why agent is stuck in loops Tool call inspection: See exact parameters passed to tools Prompt reconstruction: View final prompts with all variables filled Error context: Full stack trace with LLM conversation context Cost per task: Track spend by logical unit of work Quality metrics: Evaluate agent output quality automatically Best Platforms for Each:#\rAgent decision debugging: Phoenix, LangFuse, LangSmith State tracking: Phoenix (embedding viz), LangFuse (sessions) Retry analysis: Phoenix, LangSmith (span relationships) Tool inspection: All platforms (basic), Phoenix (advanced) Prompt debugging: LangFuse, HumanLoop, LangSmith Error diagnosis: Phoenix (correlates errors with context) Cost attribution: Helicone, LangFuse, LangSmith Quality evaluation: Phoenix, Braintrust, W\u0026amp;B Weave\nRecommended Stack for Your Project#\rOption 1: Maximum Control (Open Source)#\rPrimary: Phoenix for observability and evaluation Secondary: OpenLLMetry for OTel compatibility Evaluation: Braintrust (open source) or Confident AI Cost: $0 + infrastructure Why: Full control, no vendor lock-in, strong evaluation\nOption 2: Best Balance (Hybrid)#\rPrimary: LangFuse (self-hosted or cloud) Evaluation: Braintrust Cost tracking: Built into LangFuse Cost: $0-$ depending on volume Why: LangSmith-like features, better economics, self-host option\nOption 3: Fast Start (Commercial)#\rPrimary: LangFuse Cloud or W\u0026amp;B Weave Prompt mgmt: HumanLoop (if needed) Cost: $$ Why: Fastest to production value, good support\nMy Specific Recommendation for You#\rBased on your 4-task architecture with complex agent workflows:\nStart with: Phoenix (open source)\nHandles your multi-task traces well Strong evaluation for QA tasks Embedding analysis for similarity detection Free to run at scale LangChain native integration Add: LangFuse (self-hosted)\nBetter prompt management Sessions for grouping conference extractions User feedback for manual QA Cost tracking by conference/task Can migrate to cloud later if needed Consider later: Braintrust\nAutomate your QA agent evaluation Build test datasets for extraction accuracy CI/CD integration for quality gates This gives you:\n‚úÖ LLM-specific debugging (Phoenix traces) ‚úÖ Agent decision visibility (Phoenix + LangFuse) ‚úÖ Prompt versioning (LangFuse) ‚úÖ Quality evaluation (Phoenix evals) ‚úÖ Cost control (LangFuse) ‚úÖ Self-hosted (no vendor lock-in) ‚úÖ Can add commercial tools later All integrated via OpenTelemetry for true portability.\n"},{"id":10,"href":"/musings/docs/misc/n8n-overview/","title":"n8n Overview","section":"Miscellaneous","content":"n8n Overview#\rn8n is a workflow automation platform that lets you connect different apps and services together to automate repetitive tasks. Think of it as a visual programming tool where you create workflows by connecting nodes in a flowchart-like interface.\nWhat Problems Does n8n Solve?#\rIntegration between services: Moving data between apps that don\u0026rsquo;t natively talk to each other (Slack ‚Üí Google Sheets, webhook ‚Üí email, database ‚Üí CRM, etc.)\nAutomating repetitive tasks: Things like processing form submissions, syncing data across platforms, or triggering notifications based on events\nBuilding custom automations: Creating sophisticated workflows with conditional logic, data transformation, and multi-step processes without writing much code\nKey Features#\rSelf-hostable: You can run it on your own infrastructure (important for data privacy/compliance) Visual workflow builder: Drag-and-drop interface for creating automation flows 400+ integrations: Pre-built connectors for popular services Code when needed: JavaScript support for custom transformations Trigger-based or scheduled: Workflows can run on webhooks, schedules, or manual execution Alternative Approaches#\rCloud-only SaaS alternatives#\rZapier: Most popular, easiest to use, but expensive at scale and cloud-only Make (formerly Integromat): More complex workflows than Zapier, visual interface, cloud-hosted Tray.io: Enterprise-focused, powerful but pricier Developer-first alternatives#\rTemporal: Write workflows as code (Go/Java/Python), excellent for complex state management and reliability Prefect/Airflow: Python-based workflow orchestration, popular in data engineering Inngest: Event-driven workflow engine with code-first approach Windmill: Similar to n8n but more code-centric, also self-hostable Event-driven architectures#\rApache Kafka + stream processing: For high-volume, real-time data pipelines AWS Step Functions or Google Cloud Workflows: Cloud-native state machines Message queues (RabbitMQ, Redis) + custom workers: Build your own orchestration Low-code platforms#\rRetool Workflows: Tied to Retool\u0026rsquo;s internal tool builder Pipedream: Developer-friendly with lots of free tier, cloud-hosted IFTTT: Consumer-focused, very simple but limited DIY approaches#\rCron jobs + scripts: Simple, full control, but no orchestration/visibility CI/CD pipelines (GitHub Actions, GitLab CI): Can repurpose for automation workflows Custom microservices: Complete flexibility but high maintenance overhead When n8n Makes Sense#\rGiven your DevOps background, n8n fits well when you want:\nVisual workflows that non-developers can understand/modify Self-hosted solution (keeps data in your infrastructure) Mix of pre-built integrations with ability to write custom code Something between \u0026ldquo;no-code\u0026rdquo; (too limiting) and \u0026ldquo;pure code\u0026rdquo; (too much overhead) The sweet spot is internal automation workflows where you need quick integration between 3-10 services without building custom microservices for each connection.\nWhat kind of automation problem are you looking at? That might help narrow down whether n8n or one of the alternatives would be a better fit.\nn8n is a general workflow automation tool - AI agents are just one recent use case, not its primary purpose.\nOriginal Purpose (Pre-AI Hype)#\rn8n was built for traditional workflow automation:\nMoving data between business apps Processing webhooks and API calls Scheduled data syncing Form submission handling Notification triggers Think: \u0026ldquo;When a new row appears in Google Sheets, create a Jira ticket and send a Slack message\u0026rdquo;\nAI Integration (Recent Addition)#\rn8n added AI capabilities as LLMs became popular, but it\u0026rsquo;s layered on top:\nLLM nodes: Call OpenAI, Anthropic, local models Vector store integrations: Pinecone, Qdrant for RAG Agent nodes: Chain multiple LLM calls with tool use AI agent templates: Pre-built workflows for common patterns The AI features make it useful for building AI agents, but that\u0026rsquo;s ~10-20% of use cases.\nAI Agent-Specific Alternatives#\rIf you\u0026rsquo;re specifically building AI agents, these are more purpose-built:\nAgent frameworks:\nLangChain/LangGraph: Python/JS frameworks for complex agent workflows AutoGen: Microsoft\u0026rsquo;s multi-agent framework CrewAI: Orchestrate teams of AI agents Semantic Kernel: Microsoft\u0026rsquo;s agent SDK Agent platforms:\nRelevance AI: Build and deploy AI agents/workflows Stack AI: Visual AI agent builder Flowise: Open-source LangChain UI (similar to n8n but AI-first) LangFlow: Another visual LangChain builder AI-native workflow tools:\nDust.tt: AI assistants with workflow capabilities Superagent: Deploy AI agents with APIs Key Difference#\rn8n: \u0026ldquo;I have a workflow that happens to use AI in steps 3 and 7\u0026rdquo; Agent frameworks: \u0026ldquo;I\u0026rsquo;m building an AI system that autonomously handles tasks\u0026rdquo;\nn8n is great for deterministic workflows with AI components (RAG pipelines, content generation flows, data enrichment with LLMs).\nIt\u0026rsquo;s less ideal for autonomous agents that need complex reasoning, memory, and self-directed tool use - those benefit from frameworks purpose-built for agent patterns.\nGiven your DevOps background and interest in practical tooling, n8n would be good for things like:\nAutomated incident response with AI analysis Documentation generation from logs/metrics Intelligent alerting with LLM filtering GitOps workflows with AI-powered PR analysis Are you looking at general automation or specifically trying to build AI agents?\n"},{"id":11,"href":"/musings/docs/misc/kubernetes-based/","title":"Open Source Workflow Orchestration on Kubernetes","section":"Miscellaneous","content":"Open Source Workflow Orchestration on Kubernetes#\rYou\u0026rsquo;re in luck - there\u0026rsquo;s a rich ecosystem of workflow tools built for K8s. Let me break down the landscape based on different workflow needs.\nCategories of Workflow Tools#\rGeneral workflow orchestration (Temporal, Argo Workflows, Prefect) Data pipeline focused (Airflow, Dagster, Prefect) CI/CD workflows (Tekton, Argo Workflows) Serverless/function workflows (Knative, Fission Workflows) Lightweight script orchestration (Windmill, Kestra) Production-Grade Orchestrators#\rTemporal ‚≠ê Most Robust#\r# What it is Durable execution engine - workflows survive failures/restartsArchitecture:\nTemporal Server: Go-based, stateful (requires DB) Workers: Your code (Python/Go/Java/TypeScript/.NET) Persistence: PostgreSQL, MySQL, or Cassandra License: MIT K8s Deployment:\nhelm repo add temporal https://temporal.io/charts helm install temporal temporal/temporal \\ --set server.replicaCount=3 \\ --set cassandra.enabled=false \\ --set postgresql.enabled=trueStrengths:\nGuaranteed execution - workflow state persists through crashes Long-running workflows - can run for days/months Timeouts \u0026amp; retries - built-in, not your problem Versioning - deploy new code without breaking running workflows Visibility - excellent UI for debugging Weaknesses:\nComplex infrastructure (needs DB, multiple services) Learning curve for \u0026ldquo;durable execution\u0026rdquo; concepts Resource overhead Best for: Mission-critical workflows where you can\u0026rsquo;t afford data loss or stuck states\nAgent workflows: Perfect for long-running agents that need guaranteed completion\nArgo Workflows ‚≠ê K8s Native#\r# What it is Container-native workflow engine - each step is a podArchitecture:\nWorkflow Controller: Watches K8s CRDs Steps: Each step = container/pod Persistence: K8s etcd or external DB License: Apache 2.0 K8s Deployment:\nkubectl create namespace argo kubectl apply -n argo -f \\ https://raw.githubusercontent.com/argoproj/argo-workflows/stable/manifests/quick-start-postgres.yamlStrengths:\nK8s native - workflows as YAML CRDs DAG support - complex dependencies Artifact passing - between steps via S3/Minio Templates - reusable workflow components GitOps friendly - everything is declarative Weaknesses:\nEach step creates a pod (overhead) Not ideal for high-frequency tasks Steeper YAML learning curve Best for: Data pipelines, ML training, batch jobs, CI/CD\nAgent workflows: Good for containerized agent tasks, less ideal for interactive agents\nPrefect ‚≠ê Python Developer Friendly#\r# What it is Modern data workflow orchestration with Python-first APIArchitecture:\nPrefect Server: FastAPI (Python) Agents: Pull work from queues Persistence: PostgreSQL License: Apache 2.0 K8s Deployment:\nhelm repo add prefect https://prefecthq.github.io/prefect-helm helm install prefect-server prefect/prefect-server # Or simpler kubectl apply -f prefect-server-deployment.yamlStrengths:\nPython native - write workflows in Python, not YAML Dynamic workflows - code-defined, not static DAGs Great DX - excellent documentation, modern UI Hybrid execution - work pools for different environments Observability - built-in monitoring Weaknesses:\nRelatively newer (less battle-tested than Airflow) Smaller ecosystem than Airflow Python-only (not polyglot) Best for: Data engineering teams, ML pipelines, Python shops\nAgent workflows: Excellent - native Python makes LLM integration natural\nApache Airflow - Industry Standard#\r# What it is Veteran workflow scheduler - DAG-based, widely adoptedArchitecture:\nScheduler: Triggers tasks Webserver: UI Workers: Execute tasks (Celery/K8s executor) Metadata DB: PostgreSQL License: Apache 2.0 K8s Deployment:\nhelm repo add apache-airflow https://airflow.apache.org helm install airflow apache-airflow/airflow \\ --set executor=KubernetesExecutorStrengths:\nMature ecosystem - thousands of operators Battle-tested - used at Airbnb, Netflix, etc. KubernetesExecutor - each task as K8s pod Rich UI - comprehensive DAG visualization Community - huge, lots of examples Weaknesses:\nStatic DAGs - must be defined upfront Complexity - many moving parts Legacy feel - improving but shows age Python 2 roots - some legacy cruft Best for: Data pipelines, ETL, scheduled jobs\nAgent workflows: Workable but not ideal - static DAGs clash with dynamic agents\nLightweight/Developer-Focused Options#\rWindmill ‚≠ê DevOps Friendly#\r# What it is Developer platform for workflows, scripts, and appsArchitecture:\nServer: Rust (fast, low resource) Workers: Deno/Python/Bash/Go Database: PostgreSQL License: AGPLv3 K8s Deployment:\nhelm repo add windmill https://windmill-labs.github.io/windmill-helm-charts helm install windmill windmill/windmillStrengths:\nLow overhead - Rust is efficient Multi-language - Python, TypeScript, Go, Bash Git sync - workflows from Git repos Auto-generated UI - for scripts Fast execution - designed for speed Self-service - non-devs can build flows Weaknesses:\nSmaller community than others Newer product (less proven at scale) AGPLv3 license (copyleft) Best for: Internal tools, DevOps automation, rapid prototyping\nAgent workflows: Good fit - supports Python LLM libraries natively\nKestra#\r# What it is Event-driven workflow orchestrationArchitecture:\nCore: Java/Micronaut Workers: Multi-language via plugins Database: PostgreSQL/MySQL License: Apache 2.0 K8s Deployment:\nhelm repo add kestra https://helm.kestra.io helm install kestra kestra/kestraStrengths:\nEvent-driven - trigger on events, not just schedules Plugin ecosystem - 300+ plugins YAML DSL - declarative workflows Real-time - streaming support Weaknesses:\nJava ecosystem (less common in AI/data space) Smaller community Less mature than alternatives Best for: Event-driven workflows, real-time processing\nAgent workflows: Decent - event-driven fits agent callbacks\nSpecialized Options#\rDagster#\r# What it is Data orchestration platform with strong typingLicense: Apache 2.0\nStrengths:\nSoftware-defined assets - declare what you produce Type system - catch errors before runtime Data lineage - track data through pipelines Testing - first-class testing support Best for: Data platforms, analytics engineering\nAgent workflows: Overkill unless you\u0026rsquo;re already using Dagster\nTekton#\r# What it is K8s-native CI/CD frameworkLicense: Apache 2.0\nStrengths:\nPure K8s CRDs (Tasks, Pipelines) GitOps native Cloud-native buildpacks Best for: CI/CD, not general workflows\nAgent workflows: Wrong tool for the job\nComparison Matrix#\rTool Complexity Resource Use Agent Fit Best Use Case Temporal High Medium-High ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Mission-critical agents Argo Workflows Medium Medium ‚≠ê‚≠ê‚≠ê Containerized pipelines Prefect Low-Medium Low-Medium ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Python AI workflows Airflow High High ‚≠ê‚≠ê Scheduled data pipelines Windmill Low Low ‚≠ê‚≠ê‚≠ê‚≠ê DevOps automation Kestra Medium Medium ‚≠ê‚≠ê‚≠ê Event-driven flows Dagster Medium Medium ‚≠ê‚≠ê Data platforms Agent-Specific Considerations#\rFor AI Agent Workflows, You Need:#\rDynamic execution - agents change plans mid-flight Long-running support - some agent tasks take hours Easy Python integration - most LLM SDKs are Python Retry/error handling - LLM calls fail unpredictably State persistence - agent context across steps Observability - trace agent reasoning chains Best Matches for Agents:#\r1. Temporal (if you need bulletproof reliability)\n# Temporal workflow @workflow.defn class AgentWorkflow: @workflow.run async def run(self, goal: str) -\u0026gt; str: # This persists through failures context = await workflow.execute_activity( gather_context, goal ) # This retries automatically result = await workflow.execute_activity( run_agent, context ) return result2. Prefect (if you want Python simplicity)\n# Prefect flow from prefect import flow, task @task(retries=3) def call_llm(prompt: str) -\u0026gt; str: return anthropic.messages.create(...) @flow def agent_flow(goal: str): context = gather_context(goal) result = call_llm(context) return process_result(result)3. Windmill (if you want lightweight)\n# Windmill script - auto-becomes workflow step def main(user_query: str): # Direct Python, runs in isolated worker from anthropic import Anthropic client = Anthropic() return client.messages.create(...)Architecture Patterns on K8s#\rPattern 1: Agent as Job#\r# Simple one-off agent execution apiVersion: batch/v1 kind: Job metadata: name: agent-task spec: template: spec: containers: - name: agent image: my-agent:latest env: - name: ANTHROPIC_API_KEY valueFrom: secretKeyRef: name: api-keys key: anthropicWhen: Simple, stateless agents\nPattern 2: Agent with Temporal#\r# Temporal handles orchestration, K8s runs workers apiVersion: apps/v1 kind: Deployment metadata: name: agent-workers spec: replicas: 3 template: spec: containers: - name: worker image: my-temporal-worker:latest env: - name: TEMPORAL_SERVER: temporal-frontend:7233When: Long-running, stateful agent workflows\nPattern 3: Agent with Prefect#\r# Prefect work pool pulls from K8s apiVersion: apps/v1 kind: Deployment metadata: name: prefect-worker spec: template: spec: containers: - name: worker image: prefecthq/prefect:2-python3.11 command: [\u0026#34;prefect\u0026#34;, \u0026#34;worker\u0026#34;, \u0026#34;start\u0026#34;, \u0026#34;--pool\u0026#34;, \u0026#34;k8s-pool\u0026#34;]When: Python-heavy agent workflows\nPattern 4: Stateless Functions#\r# Knative serving for agent API apiVersion: serving.knative.dev/v1 kind: Service metadata: name: agent-api spec: template: spec: containers: - image: my-agent-api:latestWhen: Agent as API endpoint (scale-to-zero)\nMy Recommendation for Your Use Case#\rBased on your DevOps background and GitOps experience (ArgoCD):\nStart Here: Argo Workflows + Python#\r# Why: # 1. You already know Argo ecosystem # 2. K8s native (fits your mental model) # 3. GitOps friendly (store workflows in Git) # 4. Can integrate with ArgoCD # Example agent workflow apiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: agent-research- spec: entrypoint: agent-pipeline templates: - name: agent-pipeline dag: tasks: - name: gather-context template: python-task - name: run-agent dependencies: [gather-context] template: llm-task - name: synthesize dependencies: [run-agent] template: python-task - name: python-task container: image: python:3.11 command: [python] source: | # Your agent code hereGraduate To: Temporal (when you need reliability)#\rOnce you hit limitations with Argo (state management, long-running workflows), migrate to Temporal.\nAlternative: Prefect (if Python-first appeals)#\rIf your team prefers writing Python over YAML, Prefect might feel more natural.\nQuick Decision Tree#\rDo you need guaranteed execution through failures?\r‚îú‚îÄ Yes ‚Üí Temporal\r‚îî‚îÄ No ‚Üì\rIs your team primarily Python developers?\r‚îú‚îÄ Yes ‚Üí Prefect\r‚îî‚îÄ No ‚Üì\rDo you want K8s-native, GitOps-friendly?\r‚îú‚îÄ Yes ‚Üí Argo Workflows\r‚îî‚îÄ No ‚Üì\rNeed lightweight, fast iteration?\r‚îú‚îÄ Yes ‚Üí Windmill\r‚îî‚îÄ Still here? ‚Üí Probably Airflow for data, Argo for everything elseWant me to walk through setting up an actual agent workflow in Argo or Prefect on K8s? I can show you the full deployment including observability.\n"},{"id":12,"href":"/musings/docs/misc/tools-overview-and-getting-started/","title":"Self-Hosted Open Source Agent Tools","section":"Miscellaneous","content":"Self-Hosted Open Source Agent Tools#\rHere\u0026rsquo;s the landscape of tools you can fully control and run on your own infrastructure.\nAgent Frameworks (Code-First)#\rLangChain + LangGraph (Python/TypeScript)#\rLicense: MIT Self-host: Everything runs in your code Observability: LangSmith is cloud (paid), but alternatives exist Deployment: Your choice - Docker, K8s, serverless Best for: Maximum flexibility, production-grade agents Tracing alternatives: OpenLIT (OSS), Phoenix (Arize OSS version) # Simple ReAct agent example from langchain.agents import create_react_agent from langchain.tools import ToolAutoGen (Python)#\rLicense: CC-BY-4.0 / MIT Self-host: Fully local Visualization: AutoGen Studio (web UI, self-hosted) Best for: Multi-agent conversations, research workflows Deployment: Run anywhere Python runs CrewAI (Python)#\rLicense: MIT Self-host: Yes Focus: Role-based multi-agent teams Best for: Task delegation across specialized agents Deployment: Standard Python deployment Semantic Kernel (C#, Python, Java)#\rLicense: MIT Self-host: Yes From: Microsoft, but fully open Best for: Enterprise .NET environments Integration: Works well with Azure but not required Visual Workflow Builders#\rFlowise ‚≠ê Recommended for visual + self-hosted#\rLicense: Apache 2.0 Tech: Node.js, React Self-host: Docker, K8s, or npm install Features: Drag-and-drop LangChain workflows Built-in vector DB integrations API endpoints for each flow Agent templates Credential management Debugging: Flow execution logs, intermediate outputs Deployment: docker run -p 3000:3000 flowiseai/flowise Best for: Teams wanting visual UI without cloud dependency Langflow#\rLicense: MIT Tech: Python (FastAPI) + React Self-host: Docker or pip install Features: Similar to Flowise, more Python-centric Deployment: Docker Compose or K8s Best for: Python shops, LangChain power users n8n (mentioned earlier)#\rLicense: Sustainable Use License (source-available, self-host friendly) Self-host: Docker, K8s, npm AI features: LLM nodes, vector stores, agents Best for: General workflows with some AI, not pure agents Note: Enterprise features require license Windmill#\rLicense: AGPLv3 Tech: Rust + TypeScript Self-host: Docker, K8s Features: Run Python/TypeScript/Bash scripts as workflows Can build agent patterns manually Strong DevOps focus Best for: DevOps teams, script orchestration with AI Observability \u0026amp; Debugging (Self-Hosted)#\rOpenLIT ‚≠ê#\rLicense: Apache 2.0 Features: OpenTelemetry-native LLM observability Trace LangChain, OpenAI, Anthropic calls Metrics, traces, cost tracking Integrates with existing observability stack Self-host: Docker Compose Best for: Production monitoring with your own Prometheus/Grafana Phoenix (Arize AI)#\rLicense: Elastic License 2.0 Features: LLM traces and evaluations LangChain integration Embeddings visualization Self-host: Docker or pip install UI: Web-based dashboard Langfuse#\rLicense: MIT (core), AGPL (enterprise features) Features: LLM observability and analytics Prompt management Evaluation datasets Self-host: Docker Compose Best for: Teams needing prompt versioning + observability Vector Databases (for Agent Memory)#\rQdrant#\rLicense: Apache 2.0 Self-host: Docker, K8s, binary Best for: Production vector search, fast Weaviate#\rLicense: BSD-3 Self-host: Docker, K8s Features: Hybrid search, multi-tenancy Milvus#\rLicense: Apache 2.0 Self-host: Docker Compose, K8s (complex) Best for: Large-scale deployments ChromaDB#\rLicense: Apache 2.0 Self-host: Pip install or Docker Best for: Simplicity, local development Orchestration Platforms (Build Your Own Agent Patterns)#\rTemporal#\rLicense: MIT Self-host: Docker Compose, K8s (Helm charts) Agent use: Build durable agent workflows with guaranteed execution Best for: Mission-critical agents that can\u0026rsquo;t lose state Complexity: Higher DevOps overhead, but rock-solid Airflow#\rLicense: Apache 2.0 Self-host: K8s (common), Docker Agent use: Schedule and orchestrate LLM calls Best for: Data pipeline teams extending to AI agents Prefect#\rLicense: Apache 2.0 Self-host: Server runs anywhere Agent use: Python workflows with LLM steps Best for: Python-native teams Complete Self-Hosted Stacks#\rOption 1: Visual Agent Builder#\rFlowise (UI + agent runtime)\r+ Qdrant (vector memory)\r+ OpenLIT (observability)\r+ PostgreSQL (flow storage)Deploy: Single Docker Compose file\nOption 2: Code-First Production#\rLangGraph (agent framework)\r+ Temporal (orchestration)\r+ Phoenix (debugging/tracing)\r+ Weaviate (vector DB)\r+ Redis (caching)Deploy: Kubernetes cluster\nOption 3: DevOps-Native#\rWindmill (workflow engine)\r+ Custom Python agents\r+ ChromaDB (local vector store)\r+ Prometheus + Grafana (metrics)Deploy: Fits existing DevOps infrastructure\nOption 4: Lightweight#\rLangChain (framework)\r+ FastAPI (expose as API)\r+ SQLite + ChromaDB (persistence)\r+ Structured loggingDeploy: Single Docker container\nRecommended Starting Point#\rGiven your DevOps expertise:\nWeek 1: Local experimentation\n# Install Flowise locally docker run -d -p 3000:3000 flowiseai/flowise # Or LangGraph for code-first pip install langgraph langchain-anthropicWeek 2: Add observability\n# Add OpenLIT docker run -d -p 3000:3000 ghcr.io/openlit/openlitWeek 3: Production pattern\nEvaluate if you need Temporal-level reliability Or if simpler FastAPI + K8s deployment suffices Key Architecture Decisions#\rStateless vs Stateful:\nStateless: Agent runs on-demand, no persistent memory Stateful: Temporal/similar for long-running, resumable agents Synchronous vs Async:\nSync: Request ‚Üí agent runs ‚Üí response (simple) Async: Queue-based, agent runs in background (scalable) Single-tenant vs Multi-tenant:\nSingle: One agent instance per use case Multi: Shared infrastructure with isolation Model hosting:\nUse Anthropic/OpenAI APIs (easier, costs per use) Self-host LLMs (Ollama, vLLM) for full control Quick Setup Example#\rHere\u0026rsquo;s a self-hosted stack you could have running in 30 minutes:\n# docker-compose.yml version: \u0026#39;3\u0026#39; services: flowise: image: flowiseai/flowise ports: - \u0026#34;3000:3000\u0026#34; volumes: - flowise_data:/root/.flowise environment: - DATABASE_PATH=/root/.flowise qdrant: image: qdrant/qdrant ports: - \u0026#34;6333:6333\u0026#34; volumes: - qdrant_data:/qdrant/storage volumes: flowise_data: qdrant_data:Run: docker-compose up -d\nThis gives you:\nVisual agent builder (Flowise) Vector database for memory (Qdrant) All running locally What\u0026rsquo;s your infrastructure like? Kubernetes cluster, VMs, or local Docker? That would help me suggest the best fit for your environment.\n"},{"id":13,"href":"/musings/docs/ai-ml/","title":"AI \u0026 Machine Learning","section":"Documentation","content":"AI \u0026amp; Machine Learning#\rBrowse all documents in the AI \u0026amp; Machine Learning category.\n"},{"id":14,"href":"/musings/docs/misc/backstage-techdocs-pros-cons/","title":"Backstage Techdocs Pros Cons","section":"Miscellaneous","content":"Based on my research, here\u0026rsquo;s a comprehensive breakdown:\nTechDocs: Pros and Cons#\rPros:#\rDocs-as-code approach - Documentation lives alongside your code in Git, making it easy to keep in sync Version control - Full Git history and review workflow for docs Integrated experience - Documentation appears directly in the Backstage catalog alongside service information Free and open source - No licensing costs Ownership clarity - Easy to track who owns what documentation MkDocs ecosystem - Access to a large library of MkDocs plugins and themes Cons:#\rNo instant editing - Requires Git commit/PR workflow; can\u0026rsquo;t edit directly in the browser Slow first-time generation - Initial markdown-to-HTML conversion can be slow Build complexity - Production setup requires CI/CD configuration Not great for collaboration - Less intuitive than wiki-style tools for non-technical contributors Manual updates required - Docs can still go stale; relies on discipline Limited search/discovery - Compared to tools like Confluence or Notion with AI-powered search Wiki-Like Options for Backstage#\rUnfortunately, there isn\u0026rsquo;t a true instant-editing wiki plugin for Backstage that I found. However, you have a few options:\n1. Confluence Plugin Integration#\rSeveral Confluence plugins exist:\n@backstage-community/plugin-confluence - Integrates Confluence search into Backstage @backstage/plugin-scaffolder-backend-module-confluence-to-markdown - Converts Confluence pages to markdown for TechDocs These are read-only integrations, not instant editing within Backstage 2. External Wiki Integration Pattern#\rYou could link to external wikis from Backstage entities:\nKeep using Confluence, Notion, or GitBook separately Add links in your catalog-info.yaml files Users click through to edit in the native tool Other Documentation Options Integrated with Backstage#\r1. API Documentation Plugin#\r@backstage/plugin-api-docs - Built-in support for OpenAPI, AsyncAPI, GraphQL specs Renders API documentation directly from spec files 2. Azure DevOps Wiki Plugin#\rIndex wiki articles from Azure DevOps into Backstage search Read-only integration 3. Hybrid Approach#\rMany teams use a combination:\nTechDocs for technical/API documentation that needs to be versioned with code Confluence/Notion for collaborative knowledge bases, runbooks, and frequently-changing content README files for quick service overviews If You Need Instant Editing\u0026hellip;#\rYour best options outside Backstage integration are:\nConfluence - Enterprise wiki with WYSIWYG editing, robust permissions Notion - Modern, flexible, great for cross-functional teams GitBook - Developer-focused with good UX, though pricey BookStack - Open-source wiki alternative The reality is that TechDocs\u0026rsquo; strength (docs-as-code) is also its weakness when you need instant, wiki-style editing. Most teams find success using TechDocs for what it\u0026rsquo;s good at (versioned technical docs) and complementing it with a traditional wiki for everything else.\n"},{"id":15,"href":"/musings/docs/misc/cross-diagram-reuse-guide/","title":"Cross-Diagram Domain Model Reuse in PlantUML","section":"Miscellaneous","content":"Cross-Diagram Domain Model Reuse in PlantUML#\rOverview#\rThis document demonstrates how to reuse your centralized domain model across multiple UML diagram types in PlantUML.\nWhat We\u0026rsquo;ve Created#\r1. Central Domain Model#\rLocation: model/all-classes.iuml and model/all-relationships.iuml Contains: 7 domain classes (Customer, Order, OrderItem, Product, Inventory, Payment, Shipment) Purpose: Single source of truth for all domain objects 2. Class Diagrams (4 different views)#\rviews/customer-management.puml - Customer and Order views/order-fulfillment.puml - Order, Payment, and Shipment views/inventory-management.puml - OrderItem, Product, and Inventory views/complete-system.puml - All classes and relationships 3. Sequence Diagram#\rsequences/place-order-sequence.puml Shows the flow of placing an order References domain objects: Customer, Order, OrderItem, Product, Inventory Demonstrates method calls from the class model 4. Deployment Diagram#\rdeployment/system-deployment.puml Shows physical deployment architecture Notes which domain objects are managed by which services Maps domain model to infrastructure 5. State Diagram#\rstates/order-state.puml Shows state transitions for the Order class References methods from Customer, Order, Payment, Inventory, Shipment classes Demonstrates lifecycle of a domain object 6. Component Diagram#\rcomponents/system-components.puml Shows logical components of the system Documents which components manage which domain classes Links architecture to domain model How Model Reuse Works#\rApproach 1: Direct Reference (Documentation)#\rThe simplest approach is to reference your domain model in comments:\n@startuml\rtitle My Sequence Diagram\r\u0026#39; This uses objects from our domain model:\r\u0026#39; Customer, Order, OrderItem, Product, Inventory\rparticipant \u0026#34;Customer\u0026#34; as cust\rparticipant \u0026#34;Order\u0026#34; as order\r...\r@endumlPros:\nSimple and clear Works with all diagram types No technical limitations Cons:\nDoesn\u0026rsquo;t enforce consistency Manual synchronization required Approach 2: Shared Procedures (Advanced)#\rUse PlantUML procedures to extract reusable information:\n\u0026#39; In model/domain-model-unified.iuml\r!function $order_methods()\r!return \u0026#34;addItem(), calculateTotal(), submit()\u0026#34;\r!endfunction\r\u0026#39; In your sequence diagram\r!include ../model/domain-model-unified.iuml\r\u0026#39; Now you can reference $order_methods() programmaticallyPros:\nEnforces consistency Single source of truth for method names Can be used programmatically Cons:\nMore complex setup Requires PlantUML preprocessing knowledge Approach 3: Hybrid Documentation#\rMaintain a \u0026ldquo;model catalog\u0026rdquo; that documents your domain:\nmodel/README.md:\n# Domain Model Catalog ## Classes - **Customer**: manages user accounts - **Order**: represents a purchase - **OrderItem**: line item in an order - **Product**: item available for purchase - **Inventory**: stock management - **Payment**: payment processing - **Shipment**: delivery tracking ## Key Methods - Customer.placeOrder() -\u0026gt; Order - Order.addItem(OrderItem) -\u0026gt; void - Order.calculateTotal() -\u0026gt; double - Inventory.checkAvailability() -\u0026gt; boolean - Payment.process() -\u0026gt; booleanThen reference this in all diagram types.\nBest Practices for Cross-Diagram Consistency#\r1. Naming Conventions#\rUse consistent names across all diagram types:\nClass names: PascalCase (e.g., Customer, OrderItem) Methods: camelCase (e.g., placeOrder(), calculateTotal()) Participants in sequences: Match class names 2. Documentation Standards#\rAdd notes in each diagram explaining the domain model connection:\nnote right of [Order Service]\rManages:\r- Customer\r- Order\r- OrderItem\rend note3. File Organization#\rproject/\r‚îú‚îÄ‚îÄ model/\r‚îÇ ‚îú‚îÄ‚îÄ all-classes.iuml # Central class definitions\r‚îÇ ‚îú‚îÄ‚îÄ all-relationships.iuml # Central relationships\r‚îÇ ‚îî‚îÄ‚îÄ domain-model-unified.iuml # Procedures for reuse\r‚îú‚îÄ‚îÄ views/ # Class diagram views\r‚îú‚îÄ‚îÄ sequences/ # Sequence diagrams\r‚îú‚îÄ‚îÄ deployment/ # Deployment diagrams\r‚îú‚îÄ‚îÄ states/ # State diagrams\r‚îú‚îÄ‚îÄ components/ # Component diagrams\r‚îî‚îÄ‚îÄ README.md # Model catalog4. Traceability#\rIn each non-class diagram, explicitly document which domain classes are involved:\n\u0026#39; Domain objects involved: Customer, Order, Payment\r\u0026#39; Related class diagram: views/customer-management.pumlExample: End-to-End Model Usage#\rStep 1: Define Domain Model#\rmodel/all-classes.iuml - Define Customer and Order classes\nStep 2: Create Class View#\rviews/customer-management.puml - Show Customer ‚Üí Order relationship\nStep 3: Create Sequence Diagram#\rsequences/place-order-sequence.puml - Show Customer.placeOrder() calling Order methods\nStep 4: Create State Diagram#\rstates/order-state.puml - Show Order lifecycle with state transitions\nStep 5: Create Deployment Diagram#\rdeployment/system-deployment.puml - Show which service hosts Customer and Order logic\nStep 6: Create Component Diagram#\rcomponents/system-components.puml - Show CustomerManagement component managing Customer class\nAll six diagrams reference the same Customer and Order domain objects, maintaining consistency!\nBenefits of This Approach#\r‚úÖ Single Source of Truth - Domain model defined once in model/ ‚úÖ Consistency - Same names and concepts across all diagram types ‚úÖ Traceability - Easy to see which diagrams involve which classes ‚úÖ Maintainability - Change a class name once, update all diagrams ‚úÖ Documentation - Each diagram type shows different perspective ‚úÖ Completeness - Combined, diagrams give full system picture\nLimitations to Be Aware Of#\r‚ö†Ô∏è PlantUML doesn\u0026rsquo;t validate cross-references - If you rename Order to PurchaseOrder in the class model, sequence diagrams won\u0026rsquo;t automatically update\n‚ö†Ô∏è No automatic synchronization - Method signatures can drift between class and sequence diagrams\n‚ö†Ô∏è Manual consistency required - You must actively maintain alignment\nSolutions to Limitations#\rSolution 1: Code Generation#\rGenerate PlantUML from actual source code:\nClass diagrams from Java/C#/Python classes Sequence diagrams from execution traces This ensures diagrams match implementation Solution 2: Review Process#\rWhen updating domain model, grep for class names across all diagram types Use version control to track changes Review diagrams when classes change Solution 3: Documentation Links#\rAdd explicit links between diagrams:\nnote\rSee class diagram: [[../views/customer-management.puml]]\rImplementation: [[../../src/Customer.java]]\rend noteSummary#\rYes, you can reuse your domain model across all PlantUML diagram types!\nThe key is:\nDefine your model once in a central location Reference it consistently in all diagram types Document the connections between diagrams Maintain alignment through process and tooling This gives you a comprehensive, multi-view documentation system where:\nClass diagrams show structure Sequence diagrams show behavior State diagrams show lifecycle Component diagrams show architecture Deployment diagrams show infrastructure All referencing the same underlying domain model!\n"},{"id":16,"href":"/musings/docs/misc/prompt/","title":"Crossplane EKS Management Cluster Implementation - Project Brief","section":"Miscellaneous","content":"Crossplane EKS Management Cluster Implementation - Project Brief#\rAI Agent Persona#\rYou are an experienced engineer with deep knowledge of Kubernetes, Crossplane, and AWS infrastructure and architectures.\nProject Objective#\rCreate a new Kubernetes management cluster using AWS EKS with Crossplane. This management cluster will be configured to provision and manage workload clusters later. The primary goal is educational - to understand and compare different implementation approaches for using Crossplane to build clusters including all AWS infrastructure (VPC, RBAC, EKS, node groups, etc.).\nContext and Scope#\rBootstrap Environment#\rAn ephemeral Kind cluster with Crossplane and ArgoCD is already provisioned (outside project scope) This project focuses solely on the Crossplane manifests to be applied The mechanism for applying manifests (ArgoCD, kubectl, etc.) is outside project scope Management Cluster#\rWill be provisioned via Crossplane manifests fed to the bootstrap cluster Focus is on the Managed Resource Definitions (MRDs) and architecture AWS credentials will be configured via Crossplane ProviderConfiguration Do NOT configure IRSA or other cluster-based AWS credential setups Self-management via ArgoCD will be added later (outside this task\u0026rsquo;s scope) Workload Clusters#\rBare-bones EKS clusters (minimal configuration) Same AWS account and region as management cluster Provisioned manually via kubectl apply on manifest files Manifests will be parameterized for cluster-specific values (e.g., cluster name) Templating mechanism (Jinja2) should be treated as an implementation detail, not a focus area The core focus is understanding Crossplane usage for AWS infrastructure provisioning Approaches to Explore#\r1. Direct Managed Resources#\rUsing only managed resources provided by the crossplane provider-aws (or provider-upjet-aws, whichever is more straightforward)\n2. Composite Resources#\rBuilding abstractions using Crossplane XRDs (Composite Resource Definitions) composed of MRDs (Managed Resource Definitions)\n3. Alternative Crossplane-Native Approaches#\rExplore different publicly available Crossplane providers Must be Crossplane-native solutions (no external tools like standalone Terraform) Upjet-based providers are acceptable (they use Terraform providers in the background without requiring explicit Terraform commands) Show a sample of different approaches to demonstrate variety Note: Comparing official provider-aws vs provider-upjet-aws is NOT architecturally interesting enough for this option Key Requirements#\rThe management cluster must be set up to add and manage workload clusters later Each approach should include manifests for creating workload clusters Content should be structured for later packaging as: Architectural Decision Record (ADR) Blog post Output Structure#\rPrimary Content (Main Body)#\rFocus on trade-offs and architectural decisions Convey information through prose and conceptual explanations Decision-making criteria between approaches Architectural patterns and their implications Core focus: How to use Crossplane to provision complete AWS infrastructure (VPC, RBAC, EKS, node groups, etc.) Reference Implementation (Appendix)#\rComplete manifests for all approaches Parameterization handled via templating (implementation detail) Implementation serves as a validation/testing mechanism Not the primary means of expressing ideas Working Process#\rAsk clarifying questions before making assumptions Create a high-level plan organized in stages Provide a brief summary of approaches with pros/cons Work in reviewable parts/stages Keep responses concise, focusing on concepts initially Minimize code examples in early dialogue phases Provide implementation details only when that stage is reached Constraints#\rDo not implement solutions prematurely Avoid excessive detail upfront Do not anticipate unstated needs Keep token usage efficient Only do what is explicitly requested Avoid discussing ArgoCD or self-management patterns Treat templating mechanism as an implementation detail, not a focus area "},{"id":17,"href":"/musings/docs/misc/kargo-intro/","title":"Example Stage configuration","section":"Miscellaneous","content":"I\u0026rsquo;d be happy to explain how Kargo works with ArgoCD to manage deployments, but I want to make sure I provide you with the most accurate and current information since this involves specific tooling that may have evolved.\nLet me search for the latest information about Kargo and its integration with ArgoCD.Great! Now I have comprehensive information about Kargo and ArgoCD. Let me explain how they work together.\nHow Kargo Works with ArgoCD to Manage Deployments#\rThe Complementary Relationship#\rArgoCD handles syncing Kubernetes clusters to desired states in Git repositories but doesn\u0026rsquo;t have a concept of \u0026ldquo;promotion\u0026rdquo; between environments. This is where Kargo comes in - Kargo orchestrates progressive rollouts across the application lifecycle while integrating seamlessly with ArgoCD.\nThink of it this way:\nArgoCD = The deployment engine (handles the \u0026ldquo;how\u0026rdquo; of deploying) Kargo = The promotion orchestrator (handles the \u0026ldquo;when\u0026rdquo; and \u0026ldquo;what\u0026rdquo; gets promoted between environments) Key Concepts#\r1. Stages Kargo introduces the concept of Stages, which represent your environments (dev, staging, production). Each Stage is a Kubernetes Custom Resource that defines:\nWhat artifacts to track (container images, Helm charts, Git commits) How to promote changes (update Git repos, trigger ArgoCD syncs) Which ArgoCD Applications to manage 2. Warehouses Warehouses monitor sources like Docker registries for new images following specific version constraints, such as semver patterns.\n3. Freight Freight represents a collection of artifacts (images, charts, configs) that move through your promotion pipeline - like cargo being shipped through stages.\nHow the Integration Works#\rStep 1: ArgoCD Applications Deploy, Kargo Promotes#\r# Example Stage configuration apiVersion: kargo.akuity.io/v1alpha1 kind: Stage metadata: name: dev namespace: kargo-demo spec: subscriptions: warehouse: docker-registry # Watches for new images promotionMechanisms: gitRepoUpdates: - repoURL: ${GITOPS_REPO_URL} writeBranch: main kustomize: images: - image: nginx path: overlays/dev argoCDAppUpdates: - appName: nginx-dev # Links to ArgoCD Application appNamespace: argocdStep 2: The Promotion Flow#\rArtifact Detection: Kargo\u0026rsquo;s Warehouse detects a new container image in your registry Freight Creation: Kargo creates a \u0026ldquo;Freight\u0026rdquo; object representing this new version Git Updates: When you approve a promotion, Kargo updates your GitOps repo (modifies Kustomize overlays, Helm values, etc.) ArgoCD Trigger: Kargo uses the argocd-update promotion step to modify an ArgoCD Application\u0026rsquo;s operation field, forcing it to sync to the recently updated state Health Monitoring: Kargo registers health checks to continuously monitor the health of Applications updated by argocd-update steps Practical Workflow Example#\rScenario: Promoting a new nginx image from dev ‚Üí uat ‚Üí prod\nNew nginx:1.25.0 pushed to registry\r‚Üì\rKargo Warehouse detects it\r‚Üì\rFreight created with nginx:1.25.0\r‚Üì\rDeveloper clicks \u0026#34;Promote to Dev\u0026#34;\r‚Üì\rKargo updates overlays/dev/kustomization.yaml in Git\r‚Üì\rKargo triggers ArgoCD Application \u0026#34;nginx-dev\u0026#34; sync\r‚Üì\rArgoCD deploys nginx:1.25.0 to dev cluster\r‚Üì\rKargo monitors health\r‚Üì\rIf healthy, Freight becomes available for UAT promotion\r‚Üì\rRepeat process for UAT ‚Üí Prod Key Benefits of the Integration#\rSeparation of Concerns: ArgoCD focuses on deployment mechanics; Kargo handles promotion logic GitOps Compliance: All changes flow through Git - no direct cluster modifications Audit Trail: Every promotion is tracked and versioned Progressive Rollouts: Kargo enables progressive deployment strategies without custom CI scripts Self-Service: Developers can promote their own changes with built-in guardrails Reasoning \u0026amp; Best Practices#\rWhy this architecture makes sense:\nSingle Responsibility: ArgoCD does what it\u0026rsquo;s best at (syncing Git to clusters), while Kargo adds the missing orchestration layer No CI/CD Lock-in: You don\u0026rsquo;t need complex CI pipelines or custom automation scripts - Kargo manages the promotion workflow declaratively State-Driven: Unlike traditional CI-triggered deployments, this approach is pull-based and declarative, making it more reliable and auditable Common deployment pattern:\nInstall both Kargo and ArgoCD in the same cluster (management cluster) ArgoCD manages multiple target clusters Kargo orchestrates what gets deployed where and when Each environment (dev/staging/prod) has its own ArgoCD Application and Kargo Stage Questions for You#\rTo provide more specific guidance, I\u0026rsquo;d like to understand:\nAre you planning to use this for a multi-cluster setup or single cluster with namespaces? What type of artifacts are you primarily promoting (container images, Helm charts, raw manifests)? Do you already have ArgoCD running, or are you setting up both from scratch? This will help me tailor the explanation to your specific use case and provide more concrete implementation examples.\n"},{"id":18,"href":"/musings/docs/misc/export-conversion/","title":"Export Conversion","section":"Miscellaneous","content":"Excellent question! This is a critical consideration for enterprise adoption. Let me give you a comprehensive answer about PlantUML\u0026rsquo;s export and embedding capabilities.\nPlantUML Export \u0026amp; Embedding Options#\rDirect Native Integrations ‚úÖ#\rConfluence#\rMultiple PlantUML plugins exist for Confluence that allow embedding UML diagrams directly in wiki pages using PlantUML syntax. You can:\nEmbed PlantUML code directly in Confluence pages using macros Diagrams render automatically when viewing the page Multiple vendors offer plugins (some free, some paid ~$0.50/user/month) Important note: Some plugins support referencing PlantUML code from BitBucket/GitHub repositories using !include with direct URLs, keeping diagrams synchronized with your codebase.\nJira#\rPlantUML plugins are also available for Jira, though integration is less mature than Confluence.\nDraw.io (Diagrams.net)#\rCRITICAL UPDATE: PlantUML support in draw.io is being phased out - it will be removed at the end of 2025 in the online version and in 2028 in Confluence/Jira Cloud apps. This is due to security review challenges.\nExport Format Support üé®#\rPlantUML natively exports to multiple formats perfect for documentation:\nFormat Quality Use Case SVG Vector (scalable) Best for wikis, web docs, presentations PNG Raster/bitmap Simple embedding, but large files PDF Vector (with setup) Formal documents, printing EPS Vector LaTeX documents ASCII art Text Plain text documentation Recommendation: SVG is the recommended export format as it retains vector graphics with no loss of detail and remains highly editable.\nConversion Tools üîÑ#\rPlantUML ‚Üí Mermaid#\rSeveral tools exist for conversion:\npu2mm: Open-source tool that converts simple PlantUML sequence diagrams to Mermaid Browser extensions: Chrome extensions available for sequence and class diagrams Manual conversion: Syntax is similar enough for manual rewrites Why Mermaid matters: Mermaid is widely supported in:\nGitHub/GitLab (native rendering in README.md) Confluence (via plugins) Notion Many wiki systems PlantUML ‚Üí Draw.io#\rThird-party tools like plantuml2drawio and plantuml_to_drawio are available on GitHub for conversion:\nplantuml2drawio: Converts PlantUML to Draw.io format element by element, currently supporting activity diagrams Catalyst: JavaScript library specifically for converting C4 diagrams from PlantUML to draw.io format Process:\nExport PlantUML as SVG Import SVG into draw.io (can edit further) OR use conversion tools to create native draw.io XML Best Practices for Enterprise Documentation üíº#\rStrategy 1: Native Image Embedding (Recommended)#\r1. Generate SVG from PlantUML\r2. Commit SVG to version control alongside .puml source\r3. Embed SVG images in Confluence/Jira/wikis\r4. CI/CD pipeline regenerates SVGs on .puml changesPros:\nWorks everywhere (universal image support) Version controlled Automated updates possible Strategy 2: Direct Plugin Integration#\r1. Install PlantUML plugin in Confluence\r2. Embed PlantUML code directly in pages\r3. Diagrams render on page loadPros:\nSingle source of truth (the code) Easier to update No build process needed Cons:\nPlugin dependency May not work in all contexts (email exports, PDFs) Strategy 3: Hybrid Approach#\r1. Store master models as PlantUML in Git\r2. Generate both SVG and Mermaid versions\r3. Use Mermaid for GitHub/modern wikis\r4. Use SVG for Confluence and formal docs Real-World Workflow Example üî®#\rFor maximum compatibility across tools:\nSource Control (Git) ‚îú‚îÄ‚îÄ models/ ‚îÇ ‚îú‚îÄ‚îÄ domain-model.puml # Master PlantUML source ‚îÇ ‚îú‚îÄ‚îÄ view-orders.puml # View: Order subsystem ‚îÇ ‚îî‚îÄ‚îÄ view-inventory.puml # View: Inventory subsystem ‚îú‚îÄ‚îÄ exports/ ‚îÇ ‚îú‚îÄ‚îÄ svg/ # For Confluence, docs ‚îÇ ‚îú‚îÄ‚îÄ mermaid/ # For GitHub, modern wikis ‚îÇ ‚îî‚îÄ‚îÄ png/ # Fallback compatibility ‚îî‚îÄ‚îÄ .github/workflows/ ‚îî‚îÄ‚îÄ generate-diagrams.yml # Auto-regenerate on commitCI/CD pipeline:\nDetects changes to .puml files Generates SVG, PNG, and Mermaid versions Commits exports back to repo Updates documentation automatically Format Comparison for Your Use Case#\rTool/Platform Native PlantUML SVG Import PNG Import Mermaid Best Approach Confluence ‚úÖ Via plugin ‚úÖ ‚úÖ ‚úÖ Via plugin Plugin or SVG Jira ‚ö†Ô∏è Limited ‚úÖ ‚úÖ ‚ö†Ô∏è Limited SVG attachment GitHub/GitLab ‚ùå ‚úÖ ‚úÖ ‚úÖ Native Mermaid preferred SharePoint ‚ùå ‚úÖ ‚úÖ ‚ùå SVG/PNG embed Draw.io ‚ö†Ô∏è Until 2025 ‚úÖ ‚úÖ ‚úÖ Import SVG MediaWiki ‚úÖ Via extension ‚úÖ ‚úÖ ‚úÖ Via extension Extension or SVG Notion ‚ùå ‚úÖ ‚úÖ ‚úÖ Via embed Mermaid or image My Recommendation üéØ#\rFor an enterprise environment with Confluence, Jira, and various wikis:\nPrimary format: Keep PlantUML as your source of truth Build pipeline: Auto-generate SVG for every diagram Confluence integration: Use PlantUML plugin for living documentation Static docs: Embed SVG images GitHub/modern platforms: Maintain parallel Mermaid versions for key diagrams Formal documents: Export to PDF via SVG This gives you:\n‚úÖ Text-based, version-controlled source ‚úÖ Wide compatibility via SVG ‚úÖ Living documentation in Confluence ‚úÖ Modern platform support via Mermaid ‚úÖ No vendor lock-in Would you like me to:\nShow example CI/CD pipeline configurations for auto-generating exports? Dive deeper into specific Confluence/Jira plugin options? Explore the Mermaid conversion workflow in more detail? Compare this approach with Structurizr\u0026rsquo;s embedding capabilities? "},{"id":19,"href":"/musings/docs/misc/kargo-polyrepo/","title":"Fast-moving image Warehouse","section":"Miscellaneous","content":"Prompt#\rIf I have helm charts in one repository, dockerfiles in another, and application code spread across multiple repositories how does kargo keep them all in sync? How does this differ from tagging all artifacts with a common release version?\nResponse#\rExcellent question! This gets at the heart of how Kargo handles the complex reality of polyrepo architectures versus traditional release versioning. Let me search for specific information about Kargo\u0026rsquo;s artifact tracking capabilities.Perfect! Now I have the detailed information needed. Let me provide a comprehensive answer.\nHow Kargo Keeps Multi-Repo Artifacts in Sync#\rThe Core Mechanism: Warehouses Create \u0026ldquo;Freight\u0026rdquo; Bundles#\rA single Warehouse subscribes to repositories of all relevant artifacts, and each time it discovers a new revision of any or all of them, the Warehouse produces a new Freight resource referencing one revision of each. This is fundamentally different from traditional versioning.\nHere\u0026rsquo;s the key insight: Kargo treats a \u0026ldquo;release\u0026rdquo; as a bundle of specific artifact versions (a Freight object), not as a single version number applied to everything.\nExample: Your Multi-Repo Scenario#\rLet\u0026rsquo;s say you have:\nApp Code Repo A: Contains microservice-a ‚Üí produces image myapp/service-a:v1.2.3 App Code Repo B: Contains microservice-b ‚Üí produces image myapp/service-b:v2.1.0 Helm Charts Repo: Contains charts for both services Dockerfiles Repo: Contains build definitions Warehouse Configuration:\napiVersion: kargo.akuity.io/v1alpha1 kind: Warehouse metadata: name: my-app-warehouse namespace: my-project spec: subscriptions: # Subscribe to container images (built from your code repos) - image: repoURL: myapp/service-a semverConstraint: ^1.0.0 - image: repoURL: myapp/service-b semverConstraint: ^2.0.0 # Subscribe to Helm chart repository - chart: repoURL: https://charts.example.com name: my-app-chart semverConstraint: ^0.5.0 # Subscribe to configuration Git repo - git: repoURL: https://github.com/myorg/app-config.git branch: mainWhat Happens: When something new is discovered in any repository to which a warehouse subscribes, the warehouse produces a new piece of freight.\nResulting Freight might look like:\napiVersion: kargo.akuity.io/v1alpha1 kind: Freight metadata: name: freight-abc123 spec: warehouse: my-app-warehouse commits: - repoURL: https://github.com/myorg/app-config.git id: 7f4e8c2a9b1d3e5f6a8c9d0e1f2a3b4c5d6e7f8a images: - repoURL: myapp/service-a tag: v1.2.3 digest: sha256:abcdef... - repoURL: myapp/service-b tag: v2.1.0 digest: sha256:123456... charts: - repoURL: https://charts.example.com name: my-app-chart version: 0.5.2This single Freight object now represents a testable, promotable unit containing:\nSpecific image digests (not just tags) Specific Git commit SHAs Specific Helm chart versions Being referenced by a single Freight resource, these artifacts will be promoted from stage to stage together as a unit.\nHow This Differs from Common Release Versioning#\rLet me break down the key differences:\nTraditional Approach: Unified Version Tags#\rWhat you described:\nTag everything with v1.5.0 service-a image: v1.5.0 service-b image: v1.5.0 Helm chart: v1.5.0 Git tag: v1.5.0 Characteristics: ‚úÖ Pros:\nSimple mental model Easy to communicate (\u0026ldquo;deploy v1.5.0 to prod\u0026rdquo;) Clear version history ‚ùå Cons:\nTight coupling: All components must be versioned together, even if only one changed Build overhead: Every release requires rebuilding/re-tagging all artifacts Deployment inflexibility: Can\u0026rsquo;t promote just a config change or just one service update Dependency on CI/CD: Requires sophisticated CI orchestration to tag everything consistently Hotfix complexity: Patching one component requires a new unified version across all Kargo Approach: Artifact Bundles (Freight)#\rWhat Kargo does:\nCreates immutable bundles of specific artifact versions service-a: v1.2.3 (sha256:abc\u0026hellip;) service-b: v2.1.0 (sha256:def\u0026hellip;) chart: v0.5.2 config: commit 7f4e8c2 Characteristics: ‚úÖ Pros:\nIndependent versioning: Each artifact maintains its own semantic version Selective promotion: Can promote config changes without rebuilding images State-driven: No need for CI to coordinate tags - Kargo discovers what exists Immutable references: Uses image digests, not just tags (tags can be overwritten) Flexible cadences: You can promote image revisions rapidly many times per day, while configuration changes are promoted more slowly with greater deliberation No rebuild required: Creating a new Freight doesn\u0026rsquo;t require any new builds ‚ùå Cons:\nComplexity: Harder to communicate (\u0026ldquo;promote Freight abc123\u0026rdquo;) Learning curve: Requires understanding Kargo\u0026rsquo;s model Potential incompatibilities: Risk of bundling incompatible versions The Compatibility Problem \u0026amp; Solutions#\rThe Race Condition Issue#\rWhen there are multiple subscriptions, you may be subjected to race conditions when discovering an artifact in one subscription that is incompatible with the latest artifact in another subscription.\nExample Problem:\nCI pushes frontend:v2.0.0 Kargo discovers it immediately Kargo creates Freight with frontend:v2.0.0 + backend:v1.5.0 (old version) This combo is incompatible! Later, backend:v2.0.0 gets pushed, but Freight already exists Kargo\u0026rsquo;s Solutions:#\rSolution 1: Freight Creation Criteria (Newer Feature)#\rWarehouses can now use expression-based criteria to control when automatic Freight creation occurs, solving the long-standing problem of Warehouses creating Freight with incompatible artifact combinations.\napiVersion: kargo.akuity.io/v1alpha1 kind: Warehouse metadata: name: my-warehouse spec: freightCreationPolicy: Automatic freightCreationCriteria: # Only create Freight when frontend and backend tags match expression: | imageFrom(\u0026#39;myapp/frontend\u0026#39;).Tag == imageFrom(\u0026#39;myapp/backend\u0026#39;).Tag subscriptions: - image: repoURL: myapp/frontend - image: repoURL: myapp/backendReasoning: This ensures Freight is only created when both frontend and backend have matching version tags, preventing incompatible combinations.\nSolution 2: Multiple Warehouses (Different Cadences)#\rCreate separate Warehouse resources, with one subscribing to the container image repository and the other subscribing to a Git repository containing configuration, so each Stage effectively has two parallel pipelines.\n# Fast-moving image Warehouse apiVersion: kargo.akuity.io/v1alpha1 kind: Warehouse metadata: name: images-warehouse spec: subscriptions: - image: repoURL: myapp/service-a - image: repoURL: myapp/service-b --- # Slow-moving config Warehouse apiVersion: kargo.akuity.io/v1alpha1 kind: Warehouse metadata: name: config-warehouse spec: subscriptions: - git: repoURL: https://github.com/myorg/app-config.gitReasoning: This allows you to promote images frequently while promoting configuration changes separately at a slower, more deliberate pace.\nSolution 3: Gatekeeper Stage#\rCreate an initial stage that only allows compatible Freight to proceed:\napiVersion: kargo.akuity.io/v1alpha1 kind: Stage metadata: name: compatibility-gate spec: subscriptions: warehouse: my-warehouse verification: analysisTemplates: - name: compatibility-checkReasoning: Test compatibility before allowing Freight to proceed to real environments.\nWhen to Use Each Approach#\rUse Traditional Unified Versioning When:#\rYou have a true monolith or tightly coupled services All components genuinely need to change together Your team is small and coordination is easy Simplicity is more important than flexibility Marketing/external communication needs simple version numbers Use Kargo\u0026rsquo;s Freight-Based Approach When:#\rYou have microservices or polyrepo architecture Different components change at different rates You want to decouple deployments from builds You need to promote config changes without rebuilding You want progressive delivery (canary, blue/green) with state tracking You need strong audit trails of what\u0026rsquo;s deployed where Hybrid Approach (Best of Both Worlds)#\rYou can actually combine both approaches:\napiVersion: kargo.akuity.io/v1alpha1 kind: Warehouse metadata: name: release-warehouse spec: freightCreationPolicy: Automatic freightCreationCriteria: # Only create Freight when all artifacts have the same semver tag expression: | imageFrom(\u0026#39;myapp/frontend\u0026#39;).Tag == imageFrom(\u0026#39;myapp/backend\u0026#39;).Tag \u0026amp;\u0026amp; imageFrom(\u0026#39;myapp/backend\u0026#39;).Tag == chartFrom(\u0026#39;my-helm-chart\u0026#39;).Version subscriptions: - image: repoURL: myapp/frontend allowTags: ^v\\d+\\.\\d+\\.\\d+$ # Only semver tags - image: repoURL: myapp/backend allowTags: ^v\\d+\\.\\d+\\.\\d+$ - chart: repoURL: https://charts.example.com name: my-app-chartBenefits:\nYour CI still tags everything with unified versions (v1.5.0) Kargo only creates Freight when all components have matching versions You get Kargo\u0026rsquo;s promotion workflow and state tracking You get the simplicity of unified version communication Summary \u0026amp; Recommendation#\rHow Kargo Keeps Things in Sync:\nWarehouses watch multiple repositories When changes are detected, Freight bundles are created Each Freight contains specific versions (commits, tags, digests) of all subscribed artifacts This bundle is promoted as an atomic unit Key Difference from Unified Versioning:\nUnified tags: Everything shares one version ‚Üí simple but inflexible Kargo Freight: Each artifact has its own version, bundled together ‚Üí flexible but more complex My Recommendation: For your scenario (Helm charts, Dockerfiles, and code in separate repos), I\u0026rsquo;d suggest:\nStart with unified tagging + Kargo (hybrid approach) to get the best of both Use Freight creation criteria to ensure compatible combinations As your team matures, consider splitting into multiple Warehouses for different promotion cadences Always use image digests in Freight (which Kargo does automatically) for immutability Does this clarify the difference? Would you like me to dive deeper into any specific aspect or show you a complete working example for your specific repository structure?\n"},{"id":20,"href":"/musings/docs/misc/gitops-platform-safety/","title":"GitOps Safety Architecture for Platform Evolution","section":"Miscellaneous","content":"GitOps Safety Architecture for Platform Evolution#\rThis is the #1 problem that kills platform adoption. You\u0026rsquo;re absolutely right‚Äîone bad XRD or Promise can destroy trust instantly. Here\u0026rsquo;s the battle-tested approach:\nThe Core Problem#\rDeveloper requests database via Kratix Promise\r‚Üì\rYou update the Promise to add new features\r‚Üì\rCrossplane reconciles ALL existing databases\r‚Üì\rBug in new Composition = 50 databases get recreated\r‚Üì\rDevelopers lose data, platform team loses credibility Architecture 1: The Platform-as-Layers Model#\rRepository Structure#\rplatform-infra/ # REPO 1: Platform Team owns\r‚îú‚îÄ‚îÄ bootstrap/\r‚îÇ ‚îî‚îÄ‚îÄ argocd-root.yaml # The one true root\r‚îú‚îÄ‚îÄ core-platform/ # Tier 0: Never changes\r‚îÇ ‚îú‚îÄ‚îÄ crossplane/\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ crossplane-install.yaml\r‚îÇ ‚îú‚îÄ‚îÄ kratix/\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ kratix-install.yaml\r‚îÇ ‚îî‚îÄ‚îÄ argocd/\r‚îÇ ‚îî‚îÄ‚îÄ argocd-config.yaml\r‚îú‚îÄ‚îÄ platform-capabilities/ # Tier 1: Changes carefully\r‚îÇ ‚îú‚îÄ‚îÄ stable/ # Production Promises/XRDs\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ postgres-v1/\r‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ xrd.yaml\r‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ composition-prod.yaml\r‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ composition-dev.yaml\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ s3-bucket-v1/\r‚îÇ ‚îî‚îÄ‚îÄ beta/ # Testing new versions\r‚îÇ ‚îî‚îÄ‚îÄ postgres-v2/\r‚îÇ ‚îú‚îÄ‚îÄ xrd.yaml\r‚îÇ ‚îî‚îÄ‚îÄ composition.yaml\r‚îî‚îÄ‚îÄ workload-instances/ # Tier 2: Developer requests\r‚îî‚îÄ‚îÄ team-a/\r‚îî‚îÄ‚îÄ payment-db-claim.yaml\rplatform-dev/ # REPO 2: Sandbox for platform team\r‚îî‚îÄ‚îÄ experimental/\r‚îî‚îÄ‚îÄ postgres-v3-experimental/ # Break things here safelyArgoCD Application Hierarchy#\r# bootstrap/argocd-root.yaml apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: platform-root namespace: argocd spec: project: platform source: repoURL: https://github.com/org/platform-infra path: bootstrap/apps destination: server: https://kubernetes.default.svc syncPolicy: automated: prune: false # NEVER auto-prune the root! selfHeal: false # Manual sync only# bootstrap/apps/core-platform.yaml apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: core-platform spec: syncPolicy: automated: prune: false # Core infra = manual changes only selfHeal: true # Fix drift, but don\u0026#39;t delete syncOptions: - CreateNamespace=true# bootstrap/apps/stable-capabilities.yaml apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: stable-capabilities spec: syncPolicy: automated: prune: true # Safe to prune old versions selfHeal: true syncOptions: - RespectIgnoreDifferences=true ignoreDifferences: - group: apiextensions.crossplane.io kind: CompositeResourceDefinition jsonPointers: - /status # Ignore status changes - group: platform.kratix.io kind: Promise managedFieldsManagers: - kratix # Let Kratix manage its fieldsKey insight: Different sync policies for different tiers. Core = manual, capabilities = automated but careful, workloads = fully automated.\nArchitecture 2: Immutable Versioned Capabilities#\rThe Versioning Strategy#\r# DON\u0026#39;T DO THIS - Breaks existing claims when you update apiVersion: apiextensions.crossplane.io/v1 kind: CompositeResourceDefinition metadata: name: xpostgresdatabases.platform.example.com # Collision!# DO THIS - New versions coexist with old apiVersion: apiextensions.crossplane.io/v1 kind: CompositeResourceDefinition metadata: name: xpostgresdatabases.platform.example.com spec: group: platform.example.com names: kind: XPostgresDatabase plural: xpostgresdatabases claimNames: kind: PostgresDatabase plural: postgresdatabases versions: - name: v1alpha1 # Old version - keep supporting served: true referenceable: true schema: {...} - name: v1beta1 # New version - opt-in served: true referenceable: true schema: # New features here properties: spec: properties: version: # New field type: string backupSchedule: # New field type: stringComposition Selection Strategy#\r# Composition for v1alpha1 (old claims use this) apiVersion: apiextensions.crossplane.io/v1 kind: Composition metadata: name: postgres.platform.example.com.v1alpha1 labels: crossplane.io/xrd: xpostgresdatabases.platform.example.com provider: aws version: v1alpha1 # Explicit version label spec: compositeTypeRef: apiVersion: platform.example.com/v1alpha1 kind: XPostgresDatabase # ... existing composition logic# New composition for v1beta1 apiVersion: apiextensions.crossplane.io/v1 kind: Composition metadata: name: postgres.platform.example.com.v1beta1 labels: crossplane.io/xrd: xpostgresdatabases.platform.example.com provider: aws version: v1beta1 spec: compositeTypeRef: apiVersion: platform.example.com/v1beta1 # Points to new version kind: XPostgresDatabase # ... new improved compositionKey insight: Old claims keep using old Compositions. They don\u0026rsquo;t break when you add v1beta1.\nArchitecture 3: Blast Radius Containment#\rNamespace Isolation for Platform Capabilities#\r# Different namespaces = different failure domains apiVersion: v1 kind: Namespace metadata: name: platform-stable # Production XRDs/Promises --- apiVersion: v1 kind: Namespace metadata: name: platform-beta # Testing new versions --- apiVersion: v1 kind: Namespace metadata: name: platform-canary # Platform team\u0026#39;s canary# Install XRDs in different namespaces # Stable version apiVersion: pkg.crossplane.io/v1 kind: Configuration metadata: name: postgres-config-v1 namespace: platform-stable spec: package: registry.example.com/postgres:v1.2.3# Beta version apiVersion: pkg.crossplane.io/v1 kind: Configuration metadata: name: postgres-config-v2 namespace: platform-beta spec: package: registry.example.com/postgres:v2.0.0-beta1Kratix Promise Isolation#\r# Stable Promise - production ready apiVersion: platform.kratix.io/v1alpha1 kind: Promise metadata: name: postgres-v1 namespace: platform-stable labels: stability: stable version: v1 spec: destinationSelectors: - matchLabels: environment: production # Only prod clusters get stable# Beta Promise - testing apiVersion: platform.kratix.io/v1alpha1 kind: Promise metadata: name: postgres-v2 namespace: platform-beta labels: stability: beta version: v2 spec: destinationSelectors: - matchLabels: environment: development # Only dev clusters for betaKey insight: Physical separation prevents cross-contamination. Beta capabilities can\u0026rsquo;t affect prod workloads.\nArchitecture 4: The Promotion Pipeline#\rStage 1: Platform Team Sandbox#\r# platform-dev repo, branch: feature/postgres-v2 # ArgoCD App for platform team\u0026#39;s testing cluster only apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: platform-experiments namespace: argocd spec: project: platform-team-only source: repoURL: https://github.com/org/platform-dev path: experimental targetRevision: HEAD # Tracks all branches destination: server: https://platform-dev-cluster # Isolated cluster! namespace: experiments syncPolicy: automated: prune: true selfHeal: trueTimeline: 2-5 days of breaking things safely\nStage 2: Canary Deployment#\r# After testing in sandbox, promote to canary namespace # Create ONE test claim using new version apiVersion: platform.example.com/v1beta1 kind: PostgresDatabase metadata: name: canary-test-db namespace: platform-canary labels: app: platform-canary-test spec: version: v1beta1 size: smallValidation checklist:\n# Automated tests run against canary - name: Validate Canary script: | # Check database was created kubectl wait --for=condition=Ready \\ postgresdatabase/canary-test-db --timeout=300s # Verify connection kubectl run -it --rm test-connection \\ --image=postgres:15 --restart=Never -- \\ psql $CONNECTION_STRING -c \u0026#34;SELECT 1\u0026#34; # Check no unintended side effects kubectl get postgresdatabase -A | \\ grep -v canary | grep -v Ready \u0026amp;\u0026amp; exit 1 # Passed? Promote to betaTimeline: 1-2 days monitoring\nStage 3: Beta (Opt-in for Early Adopters)#\r# Merge to platform-infra repo, beta/ directory # beta/argocd-app.yaml apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: platform-beta-capabilities spec: source: repoURL: https://github.com/org/platform-infra path: platform-capabilities/beta destination: namespace: platform-beta syncPolicy: automated: prune: true selfHeal: true syncWave: 20 # After stable (wave 10)Exposure strategy:\n# Only show beta capabilities to opted-in teams # Backstage template with annotation apiVersion: scaffolder.backstage.io/v1beta3 kind: Template metadata: name: postgres-v2-beta annotations: backstage.io/beta: \u0026#34;true\u0026#34; # Hidden unless user enables beta featuresTimeline: 2-4 weeks with friendly teams\nStage 4: Stable Promotion#\r# Move from beta/ to stable/ git mv platform-capabilities/beta/postgres-v2 \\ platform-capabilities/stable/postgres-v2 # Update ArgoCD app to point to stable # stable/argocd-app.yaml apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: platform-stable-capabilities spec: source: path: platform-capabilities/stable syncWave: 10 # Deploys before betaDeprecation of v1:\n# Don\u0026#39;t delete v1 immediately! # Mark as deprecated in stable/postgres-v1/ apiVersion: apiextensions.crossplane.io/v1 kind: CompositeResourceDefinition metadata: name: xpostgresdatabases.platform.example.com annotations: deprecated: \u0026#34;true\u0026#34; deprecation.message: \u0026#34;Use v1beta1. v1alpha1 will be removed in 90 days\u0026#34; sunset.date: \u0026#34;2025-06-01\u0026#34; Architecture 5: Safety Mechanisms#\r1. Resource Reconciliation Protection#\r# Composition with deletion protection apiVersion: apiextensions.crossplane.io/v1 kind: Composition metadata: name: postgres.platform.example.com spec: resources: - name: rds-instance base: apiVersion: rds.aws.upbound.io/v1beta1 kind: Instance metadata: annotations: # Prevent accidental deletion crossplane.io/external-name: preserve spec: deletionPolicy: Orphan # Don\u0026#39;t delete cloud resource forProvider: skipFinalSnapshot: false # Always take snapshot2. Drift Detection Alerts#\r# ArgoCD notification when platform drifts apiVersion: v1 kind: ConfigMap metadata: name: argocd-notifications-cm data: trigger.platform-drift: | - when: app.status.sync.status == \u0026#39;OutOfSync\u0026#39; oncePer: app.metadata.name send: [platform-team-slack] template.platform-drift: | message: | ‚ö†Ô∏è PLATFORM DRIFT DETECTED App: {{.app.metadata.name}} Diff: {{.app.status.sync.comparisonResult.diff}} This may indicate someone manually modified platform resources. Review immediately: {{.context.argocdUrl}}/applications/{{.app.metadata.name}}3. Pre-Sync Validation#\r# ArgoCD PreSync hook to validate changes apiVersion: batch/v1 kind: Job metadata: name: validate-platform-changes annotations: argocd.argoproj.io/hook: PreSync argocd.argoproj.io/hook-delete-policy: BeforeHookCreation spec: template: spec: containers: - name: validator image: your-registry/platform-validator:v1 command: - /bin/sh - -c - | # Check for dangerous changes # 1. Ensure no XRD deletions without migration if kubectl get xrd -o yaml | grep \u0026#34;deletionTimestamp\u0026#34;; then echo \u0026#34;ERROR: XRD deletion detected without migration plan\u0026#34; exit 1 fi # 2. Validate all XRDs still compile for xrd in /manifests/xrds/*.yaml; do kubectl apply --dry-run=server -f $xrd || exit 1 done # 3. Check for breaking schema changes /scripts/check-schema-compatibility.sh || exit 1 echo \u0026#34;‚úì Platform changes validated\u0026#34; restartPolicy: Never4. Automatic Rollback Triggers#\r# ArgoCD app with auto-rollback apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: platform-stable-capabilities spec: syncPolicy: automated: prune: true selfHeal: true retry: limit: 2 backoff: duration: 5s factor: 2 maxDuration: 1m # Auto-rollback on health degradation ignoreDifferences: - group: apiextensions.crossplane.io kind: CompositeResourceDefinition jsonPointers: - /status/conditions Architecture 6: Testing Strategy#\rUnit Tests for Compositions#\r# Use crossplane/composition-testing cat \u0026lt;\u0026lt;EOF | kubectl apply --dry-run=client -f - -o yaml apiVersion: platform.example.com/v1beta1 kind: PostgresDatabase metadata: name: test-db spec: size: medium environment: dev EOF# Python test using crossplane render import subprocess import yaml def test_postgres_composition(): # Render composition with test claim result = subprocess.run([ \u0026#39;crossplane\u0026#39;, \u0026#39;beta\u0026#39;, \u0026#39;render\u0026#39;, \u0026#39;xrd.yaml\u0026#39;, \u0026#39;composition.yaml\u0026#39;, \u0026#39;claim.yaml\u0026#39; ], capture_output=True, text=True) resources = yaml.safe_load_all(result.stdout) # Assert expected resources created resource_types = [r[\u0026#39;kind\u0026#39;] for r in resources] assert \u0026#39;Instance\u0026#39; in resource_types # RDS assert \u0026#39;SubnetGroup\u0026#39; in resource_types assert \u0026#39;SecurityGroup\u0026#39; in resource_types # Assert correct sizing for r in resources: if r[\u0026#39;kind\u0026#39;] == \u0026#39;Instance\u0026#39;: assert r[\u0026#39;spec\u0026#39;][\u0026#39;forProvider\u0026#39;][\u0026#39;instanceClass\u0026#39;] == \u0026#39;db.t3.medium\u0026#39;Integration Tests in Canary#\r# Tekton pipeline for canary testing apiVersion: tekton.dev/v1beta1 kind: Pipeline metadata: name: test-new-capability spec: tasks: - name: create-test-claim taskSpec: steps: - name: apply-claim image: bitnami/kubectl script: | kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: platform.example.com/v1beta1 kind: PostgresDatabase metadata: name: test-${RANDOM} namespace: platform-canary spec: size: small EOF - name: wait-for-ready runAfter: [create-test-claim] taskSpec: steps: - name: wait image: bitnami/kubectl script: | kubectl wait --for=condition=Ready \\ postgresdatabase -n platform-canary \\ --all --timeout=600s - name: validate-connection runAfter: [wait-for-ready] taskSpec: steps: - name: test-connection image: postgres:15 script: | # Get connection secret SECRET=$(kubectl get postgresdatabase -n platform-canary \\ -o jsonpath=\u0026#39;{.items[0].status.connectionSecret}\u0026#39;) # Test connection psql $(kubectl get secret $SECRET -o jsonpath=\u0026#39;{.data.uri}\u0026#39; | base64 -d) \\ -c \u0026#34;SELECT version();\u0026#34; - name: cleanup runAfter: [validate-connection] taskSpec: steps: - name: delete image: bitnami/kubectl script: | kubectl delete postgresdatabase -n platform-canary --all The Safety Checklist#\rBefore promoting ANY platform capability to production:\n## Platform Capability Promotion Checklist ### Phase 1: Development (platform-dev cluster) - [ ] XRD compiles without errors - [ ] Composition renders successfully with test claims - [ ] Unit tests pass (`crossplane render`) - [ ] No unexpected resources created - [ ] Deletion completes cleanly (no orphaned resources) ### Phase 2: Canary (production cluster, isolated namespace) - [ ] Single test claim created successfully - [ ] Cloud resources provisioned correctly - [ ] Connection/credentials work - [ ] No impact on existing claims (monitored for 24h) - [ ] Resource cleanup works (delete claim, verify cloud resources removed) ### Phase 3: Beta (production cluster, opt-in) - [ ] Deployed to platform-beta namespace - [ ] At least 2 friendly teams using it - [ ] Running in production for 2+ weeks - [ ] No escalated issues - [ ] Metrics look healthy (provision time, error rate) - [ ] Deprecation plan for old version documented ### Phase 4: Stable (production cluster, default) - [ ] Moved to platform-stable namespace - [ ] Documentation updated - [ ] Backstage templates updated - [ ] Old version marked deprecated (if applicable) - [ ] Migration guide published (if breaking changes) - [ ] Announcement sent to engineering teams ### Phase 5: Sunset (removing old version) - [ ] 90-day deprecation notice given - [ ] All existing claims migrated - [ ] Old Composition removed - [ ] Old XRD version marked as not served Real-World Example: Database Upgrade#\rScenario: You want to add automated backups to PostgresDatabase XRD\n‚ùå Dangerous Approach#\r# Modifying existing v1alpha1 XRD # This triggers reconciliation of ALL existing databases! spec: versions: - name: v1alpha1 schema: properties: spec: properties: backupRetention: # NEW FIELD type: integer default: 7 # Uh oh, all DBs just got modified!‚úÖ Safe Approach#\r# Create v1alpha2 with new feature spec: versions: - name: v1alpha1 # OLD - still served served: true referenceable: true schema: {...} # Unchanged - name: v1alpha2 # NEW - opt-in served: true referenceable: true schema: properties: spec: properties: backupRetention: type: integer default: 7Migration path:\nDeploy v1alpha2 to beta namespace Create test claim with apiVersion: v1alpha2 Validate backups work Document migration: \u0026ldquo;To enable backups, update your claim to v1alpha2\u0026rdquo; Old claims keep working on v1alpha1 (no backups, but stable) Teams migrate when ready After 90 days, deprecate v1alpha1 Monitoring \u0026amp; Alerts#\r# Prometheus alerts for platform health apiVersion: v1 kind: ConfigMap metadata: name: platform-alerts data: alerts.yaml: | groups: - name: platform-stability rules: - alert: CrossplaneReconciliationFailures expr: | rate(crossplane_reconcile_errors_total[5m]) \u0026gt; 0.1 for: 5m annotations: summary: \u0026#34;Crossplane reconciliation errors detected\u0026#34; - alert: UnhealthyXRClaims expr: | count(crossplane_claim_status{condition=\u0026#34;Ready\u0026#34;,status=\u0026#34;False\u0026#34;}) \u0026gt; 5 for: 10m annotations: summary: \u0026#34;Multiple claims unhealthy\u0026#34; - alert: PlatformDrift expr: | argocd_app_sync_status{name=~\u0026#34;platform-.*\u0026#34;,sync_status=\u0026#34;OutOfSync\u0026#34;} == 1 for: 15m annotations: summary: \u0026#34;Platform components drifted from Git\u0026#34;Dashboard metrics to track:\nClaims created per day (by version) Time to ready (by XRD type) Error rate (by Composition) Resource churn (creates vs deletes) Platform component sync status This architecture lets you innovate rapidly while keeping production stable. The key is immutability (versions never change) and isolation (blast radius containment).\n"},{"id":21,"href":"/musings/docs/misc/tasks/","title":"High-Level Plan: Crossplane EKS Management Cluster Implementation","section":"Miscellaneous","content":"High-Level Plan: Crossplane EKS Management Cluster Implementation#\rOverview#\rThis project explores different approaches to creating a Kubernetes management cluster using AWS EKS with Crossplane 2.1. The management cluster will be configured to provision and manage workload clusters. The primary goal is educational - to understand and compare different implementation approaches.\nStage 1: Approach Overview \u0026amp; Comparison ‚úÖ COMPLETE#\rProvide a conceptual overview of the three main approaches and their variations:\nApproach 1: Direct Managed Resources#\rPros: Simple, transparent, direct mapping to AWS resources, easy to debug Cons: Verbose, repetitive, no abstraction, difficult to maintain at scale Approach 2: Composite Resources (XRDs)#\rPros: Reusable abstractions, parameterized, simplified interface, organizational best practices Cons: Additional complexity layer, learning curve, indirection can obscure troubleshooting Approach 3: Alternative Architectural Patterns#\r3a: Opinionated Platform\nPros: Fastest time to value, built-in best practices, reduced configuration errors Cons: Less flexibility, may not match org requirements, potential vendor lock-in 3b: Multi-Cloud Abstraction\nPros: Cloud portability, simplified mental model, easier multi-cloud strategy Cons: Lowest common denominator, abstraction leaks, debugging complexity 3c: Fargate-First Architecture\nPros: Zero node management, automatic scaling, perfect for ephemeral workloads Cons: Higher per-pod costs, Fargate limitations, less control 3d: vCluster-Based Multi-Tenancy\nPros: Dramatically reduced cost, faster provisioning, strong multi-tenancy isolation Cons: Not true cluster isolation, networking complexity, resource contention possible 3e: Kubernetes-Native Tooling\nPros: Cloud portability, consistent tooling, rich K8s ecosystem features Cons: Additional operational burden, more infrastructure to maintain, higher resource costs 3f: ECS Control Plane Alternative\nPros: Lower management cluster costs, simpler management plane, fine-grained IAM Cons: Unusual architecture, limited community support, mixing orchestrators Deliverable: Summary comparison table and architectural differences analysis\nStage 2: Architecture Design ‚úÖ COMPLETE#\rDefine the management cluster architecture and how it differs across approaches.\nReference Network Architecture#\rVPC CIDR: 10.0.0.0/16 Public Subnet: 10.0.0.0/22 (NAT Gateway, IGW) Management Cluster Private Subnet: 10.0.4.0/22 Workload Cluster Private Subnets: /22 blocks starting at 10.0.8.0/22 Single AZ deployment (us-east-1a default) Management Cluster Resource Stack#\r15 core AWS resources (VPC, subnets, IGW, NAT, route tables, security groups, IAM roles, EKS cluster, node group) Crossplane-specific resources (ProviderConfig, namespaces) Workload Cluster Template Structure#\r8 resources per workload cluster (reuses VPC/IGW/NAT from management) Parameterized templates (cluster name, subnet CIDR, instance type, node count) Resource Dependency Diagrams#\rMermaid diagrams for each approach showing resource relationships Comparison table showing resource counts per approach Deliverable: Architecture design document with diagrams and resource specifications\nStage 3: Implementation Patterns üîÑ IN PROGRESS#\rDescribe the structural and organizational patterns for each approach using Crossplane 2.1 features.\nFor Each Approach, Cover:#\rApproach 1: Direct Managed Resources\nManifest organization and file structure Resource reference patterns (Crossplane 2.1 style) Dependency management Key decisions Approach 2: Composite Resources (XRDs)\nXRD design (XR-only pattern in 2.1) Composition functions and pipeline architecture Modern composition mode Key decisions Approach 3a-3f: Alternative Patterns\nProvider/platform selection Crossplane 2.1-specific implementations Configuration packages and composition functions Key decisions for each pattern Crossplane 2.1 Specific Patterns#\rComposition functions Provider families Resource management (deletion policies, connection secrets) Namespace scoping Deliverable: Implementation patterns document describing manifest structure and key decisions\nStage 4: Decision Framework#\rCreate criteria for choosing between approaches based on organizational needs.\nDecision Criteria#\rTeam expertise and organizational maturity Scale considerations (1 cluster vs 100 clusters) Maintenance burden Flexibility vs simplicity trade-offs Migration paths between approaches Cost implications Operational complexity Time to production Use Case Scenarios#\rStartup/small team with limited K8s experience Enterprise with dedicated platform team Multi-cloud organization Cost-sensitive development environments Compliance-heavy industries Fast-moving product teams Decision Matrix#\rMap approaches to scenarios Highlight trade-offs Provide guidance on choosing between options Deliverable: Decision framework document with criteria, scenarios, and recommendations\nStage 5: Reference Implementation#\rProduce complete manifests and templates for all approaches as appendix material.\nFor Each Approach:#\rComplete Crossplane manifests ProviderConfig setup Management cluster provisioning manifests Workload cluster templates (Jinja2-based) Provider installation and configuration Template Structure#\rJinja2 templates for parameterization Default values Usage instructions (conceptual: cat template | jinja2 render | kubectl apply -f -) Organization#\rSeparate directories per approach README files with setup instructions Dependency documentation Deliverable: Complete reference implementation with all manifests and templates\nFinal Deliverables#\rPrimary Content (Main Document)#\rProse-focused architectural decision record Conceptual explanations of approaches Trade-offs and decision criteria When to use each approach Suitable for blog post or ADR Appendix (Reference Implementation)#\rComplete Crossplane 2.1 manifests Jinja2 templates for workload clusters Installation and usage instructions Serves as validation and testing mechanism Current Status#\r‚úÖ Stage 1: Complete - Approach overview and comparison ‚úÖ Stage 2: Complete - Architecture design with diagrams üîÑ Stage 3: Outline created, awaiting review before fleshing out details ‚è≥ Stage 4: Not started ‚è≥ Stage 5: Not started Key Constraints \u0026amp; Assumptions#\rBootstrap cluster (Kind with Crossplane 2.1 and ArgoCD) already exists Focus on Crossplane manifests, not deployment mechanism AWS credentials via ProviderConfig (no IRSA) Same AWS account/region for all clusters Single AZ deployment (no HA/DR) Managed node groups, t3.medium instances Workload clusters in own /22 subnets within management VPC Simple security groups and network ACLs (concept focus, not production-hardened) Templates use Jinja2 (decoupled from Helm/Kustomize complexity) No ArgoCD self-management discussion (future work) "},{"id":22,"href":"/musings/docs/misc/backstage-kratix-crossplane-argocd-blueprint/","title":"Implementation Blueprint: Crossplane + Kratix + Backstage + ArgoCD","section":"Miscellaneous","content":"Implementation Blueprint: Crossplane + Kratix + Backstage + ArgoCD#\rYour Current State#\r‚úÖ ArgoCD already deployed - this is actually your secret weapon for quick wins\nPhase 1: Quick Wins (Week 1-2) - Immediate Developer Value#\rStep 1.1: Deploy Crossplane (Day 1-2)#\r# Install Crossplane kubectl create namespace crossplane-system helm install crossplane --namespace crossplane-system \\ crossplane-stable/crossplane # Install AWS Provider (or your cloud) kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: pkg.crossplane.io/v1 kind: Provider metadata: name: provider-aws spec: package: xpkg.upbound.io/upbound/provider-aws:v0.40.0 EOFQuick Win: Within 2 days, developers can request S3 buckets via YAML instead of AWS console tickets.\n# Developer submits this to Git, ArgoCD syncs it apiVersion: s3.aws.upbound.io/v1beta1 kind: Bucket metadata: name: my-app-data spec: forProvider: region: us-east-1Why this is fast:\nArgoCD already handles the Git‚ÜíKubernetes sync Crossplane just needs cloud credentials No UI needed yet - developers use Git Immediate value: Self-service infrastructure Step 1.2: Create First Composite Resources (Week 1)#\rThe game-changer: Abstract complexity behind simple APIs\n# platform-team creates this ONCE apiVersion: apiextensions.crossplane.io/v1 kind: CompositeResourceDefinition metadata: name: xpostgresdatabases.platform.example.com spec: group: platform.example.com names: kind: XPostgresDatabase plural: xpostgresdatabases claimNames: kind: PostgresDatabase # What developers request plural: postgresdatabases versions: - name: v1alpha1 schema: openAPIV3Schema: type: object properties: spec: type: object properties: size: type: string enum: [small, medium, large] # Simple for devs! environment: type: string enum: [dev, staging, prod]Quick Win: Developers request databases like this:\napiVersion: platform.example.com/v1alpha1 kind: PostgresDatabase metadata: name: my-app-db spec: size: medium environment: devPlatform team\u0026rsquo;s Composition maps this to:\nRDS instance (right-sized based on \u0026ldquo;medium\u0026rdquo;) VPC security group Parameter group Secret with credentials Backup policy Why this is fast:\nPick ONE common use case (database or S3 bucket) Build one good Composition Developers see immediate value Timeline: 3-5 days to first working Composition Common pitfall to avoid: Don\u0026rsquo;t try to make it perfect. Start with small/medium/large instead of exposing 50 RDS parameters.\nPhase 2: The Integration Layer (Week 2-4) - Kratix#\rStep 2.1: Install Kratix (Day 8-10)#\r# Install Kratix on your platform cluster kubectl apply -f https://raw.githubusercontent.com/syntasso/kratix/main/distribution/kratix.yamlKey concept: Kratix is your workflow engine. It sits between developer requests and infrastructure execution.\nStep 2.2: Your First Promise (Week 2-3)#\rStart with something developers request constantly. Example: \u0026ldquo;I need a complete dev environment\u0026rdquo;\napiVersion: platform.kratix.io/v1alpha1 kind: Promise metadata: name: dev-environment spec: api: apiVersion: marketplace.kratix.io/v1alpha1 kind: DevEnvironment schema: properties: appName: type: string gitRepo: type: string workflows: promise: configure: - apiVersion: platform.kratix.io/v1alpha1 kind: Pipeline spec: containers: - image: your-registry/env-provisioner:v1 # This container does the work: # 1. Create Crossplane PostgresDatabase claim # 2. Create Crossplane S3Bucket claim # 3. Create namespace # 4. Generate ArgoCD Application manifest # 5. Write everything to GitOps repoWhat this Promise does:\nDeveloper requests DevEnvironment with just app name Kratix pipeline runs, orchestrating: Crossplane claims for infra (DB, S3) Kubernetes namespace ArgoCD Application pointing to their repo Outputs go to Git repo ArgoCD syncs everything Quick Win Timeline:\nWeek 2: Basic Promise working (just provisions one thing) Week 3: Enhanced Promise (full environment with DB + storage + networking) Why this takes longer than pure Crossplane:\nNeed to write pipeline containers Design workflow logic But the payoff is HUGE: One request ‚Üí Complete environment Phase 3: Developer UX Layer (Week 4-8) - Backstage#\rStep 3.1: Minimal Backstage Setup (Week 4-5)#\rnpx @backstage/create-app@latest # Select PostgreSQL for production setupQuick Win Strategy: Don\u0026rsquo;t try to do everything. Start with TWO features:\nFeature 1: Software Catalog (Week 4)#\r# catalog-info.yaml in each service repo apiVersion: backstage.io/v1alpha1 kind: Component metadata: name: payment-service annotations: backstage.io/kubernetes-label-selector: \u0026#39;app=payment-service\u0026#39; crossplane.io/claim: \u0026#39;postgresdatabases/payment-db\u0026#39; spec: type: service lifecycle: production owner: team-paymentsImmediate value: Developers see all services, owners, and dependencies in one place.\nFeature 2: Kubernetes Plugin (Week 5)#\r# app-config.yaml kubernetes: serviceLocatorMethod: type: \u0026#39;multiTenant\u0026#39; clusterLocatorMethods: - type: \u0026#39;config\u0026#39; clusters: - name: production url: ${K8S_PROD_URL} authProvider: \u0026#39;serviceAccount\u0026#39;Immediate value: Developers see their pod status, logs, and resource consumption without kubectl.\nWhat to skip initially:\n‚ùå Custom themes/branding ‚ùå Complex RBAC ‚ùå Scaffolder templates (do this in Phase 4) ‚úÖ Just get catalog + k8s plugin working Step 3.2: Crossplane Integration (Week 6-7)#\rUse community plugins:\nyarn add @backstage/plugin-kubernetes-backend yarn add @vrabbi/backstage-plugin-crossplane # Community pluginWhat developers now see in Backstage:\nList of all their Crossplane claims Status of provisioned infrastructure Connection details (from secrets) Quick Win: Developer navigates to their service in Backstage ‚Üí sees linked database ‚Üí clicks to view credentials. No more searching through AWS console or Kubernetes secrets.\nStep 3.3: Scaffolder Templates (Week 7-8)#\rNow you connect Backstage ‚Üí Kratix ‚Üí Crossplane:\n# templates/dev-environment-template.yaml apiVersion: scaffolder.backstage.io/v1beta3 kind: Template metadata: name: create-dev-environment title: New Development Environment spec: parameters: - title: Application Details properties: appName: type: string team: type: string ui:field: OwnerPicker steps: - id: create-kratix-request name: Request Environment action: kubernetes:apply input: manifest: | apiVersion: marketplace.kratix.io/v1alpha1 kind: DevEnvironment metadata: name: ${{ parameters.appName }}-dev spec: appName: ${{ parameters.appName }} team: ${{ parameters.team }}Developer experience:\nGoes to Backstage Clicks \u0026ldquo;Create\u0026rdquo; ‚Üí \u0026ldquo;New Dev Environment\u0026rdquo; Fills in app name Clicks button Gets complete environment (DB, S3, namespace, ArgoCD app) in 5 minutes Phase 4: Polish \u0026amp; Scale (Week 8-16)#\rWeek 8-10: GitOps Repository Structure#\rplatform-gitops/\r‚îú‚îÄ‚îÄ infrastructure/\r‚îÇ ‚îú‚îÄ‚îÄ crossplane-compositions/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ postgres.yaml\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ s3.yaml\r‚îÇ ‚îî‚îÄ‚îÄ kratix-promises/\r‚îÇ ‚îî‚îÄ‚îÄ dev-environment.yaml\r‚îú‚îÄ‚îÄ workloads/\r‚îÇ ‚îú‚îÄ‚îÄ team-a/\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ payment-service/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ namespace.yaml\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ argocd-app.yaml\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ crossplane-claims.yaml\r‚îÇ ‚îî‚îÄ‚îÄ team-b/\r‚îî‚îÄ‚îÄ bootstrap/\r‚îî‚îÄ‚îÄ argocd-apps.yaml # App of Apps patternKey insight: ArgoCD manages EVERYTHING, including Crossplane and Kratix configs.\nWeek 10-12: Self-Service Guardrails#\rAdd policy enforcement:\n# Kratix pipeline with validation - name: validate-request image: your-registry/policy-checker:v1 command: - /bin/sh - -c - | # Check team has budget for \u0026#39;large\u0026#39; database # Verify environment naming conventions # Ensure required labels existWeek 12-16: Observability Integration#\rConnect the dots:\n# Backstage shows: - Service catalog entry - Linked infrastructure (Crossplane claims) - Deployment status (ArgoCD sync state) - Runtime health (Kubernetes pods) - Cost allocation (cloud resource tags) Timeline Summary#\rPhase Timeline Effort Developer Value Crossplane basics Week 1 Low ‚≠ê‚≠ê‚≠ê Immediate self-service First Composition Week 1-2 Medium ‚≠ê‚≠ê‚≠ê‚≠ê Simple, powerful APIs Kratix + First Promise Week 2-3 Medium ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Complete environments Backstage catalog Week 4-5 Low ‚≠ê‚≠ê‚≠ê Visibility Backstage K8s plugin Week 5 Low ‚≠ê‚≠ê‚≠ê‚≠ê Troubleshooting Crossplane in Backstage Week 6-7 Medium ‚≠ê‚≠ê‚≠ê‚≠ê Infrastructure visibility Scaffolder templates Week 7-8 High ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê True self-service Polish \u0026amp; Scale Week 8-16 High ‚≠ê‚≠ê‚≠ê‚≠ê Production ready Critical Success Factors#\r‚úÖ Do These First (Quick Wins)#\rStart with Crossplane + one Composition (Week 1)\nPick the #1 developer pain point (usually databases) Make it work end-to-end with ArgoCD Don\u0026rsquo;t wait for Backstage Use Git as the UI initially\nDevelopers already know Git ArgoCD is already watching repos Add Backstage later for discoverability Choose one powerful Kratix Promise (Week 2-3)\n\u0026ldquo;Complete dev environment\u0026rdquo; is ideal Demonstrates the platform\u0026rsquo;s value proposition Shows why platform team isn\u0026rsquo;t a bottleneck ‚ùå Avoid These Traps#\rDon\u0026rsquo;t build Backstage first\nIt\u0026rsquo;s just a UI with no backend capabilities Developers get frustrated by pretty UI that can\u0026rsquo;t do anything Infrastructure first, UI later Don\u0026rsquo;t try to model everything in Crossplane\nStart with 2-3 Compositions maximum Perfect is the enemy of shipped Iterate based on actual usage Don\u0026rsquo;t customize Backstage heavily at first\nUse default theme Use community plugins Focus on integration, not branding Don\u0026rsquo;t skip the GitOps repo design\nMessy repos = merge conflicts = team friction Design structure in Week 1, enforce it The 2-Week \u0026ldquo;Prove It\u0026rdquo; Plan#\rIf you need to show value FAST:\nWeek 1: Pure Crossplane#\rDay 1-2: Install Crossplane, configure AWS provider Day 3-4: Build S3 bucket Composition Day 5: Developers self-service buckets via Git/ArgoCD Demo: \u0026ldquo;We eliminated the AWS access request ticket queue\u0026rdquo; Week 2: Add Kratix#\rDay 1-2: Install Kratix Day 3-5: Build \u0026ldquo;Static Website\u0026rdquo; Promise (S3 + CloudFront) Demo: Developer commits one YAML, gets complete hosting stack Backstage comes in Week 3-4 as the \u0026ldquo;cherry on top\u0026rdquo; for discoverability and metrics.\nThis approach means developers see tangible value (self-service infra) within 1 week, before you\u0026rsquo;ve even touched Backstage.\n"},{"id":23,"href":"/musings/docs/misc/kargo-git-tags/","title":"In subsequent stages, you can read this metadata","section":"Miscellaneous","content":"Let me search more specifically for Git tag creation capabilities in Kargo.Based on my research, I can now provide you with a comprehensive answer about creating tags when Freight reaches a stage.\nCan Kargo Create Tags for All Components When Freight Reaches a Stage?#\rShort Answer: Yes, but not automatically out-of-the-box. Kargo doesn\u0026rsquo;t have a built-in \u0026ldquo;create git tags for all artifacts\u0026rdquo; feature, but you can implement this yourself using Kargo\u0026rsquo;s promotion steps and metadata capabilities.\nWhat Kargo Does Provide#\r1. Git Commits (Built-in)#\rThe git-commit step commits all changes in a working tree to its checked out branch, and every promotion creates a traceable Git commit with metadata about what was promoted.\nStandard Promotion Flow:\nsteps: - uses: git-clone # ... clone repo - uses: yaml-update # ... update manifests with new image versions - uses: git-commit as: commit config: path: ./out message: | Promoted to ${{ ctx.stage }} - frontend: ${{ imageFrom(\u0026#39;myapp/frontend\u0026#39;).Tag }} - backend: ${{ imageFrom(\u0026#39;myapp/backend\u0026#39;).Tag }} - config: ${{ commitFrom(\u0026#39;github.com/myorg/config\u0026#39;).ID }} - uses: git-push # ... push changesResult: Each promotion creates a Git commit in your GitOps repository that records exactly what was deployed.\n2. Freight and Stage Metadata#\rYou can propagate important context between Stages using a set-metadata promotion step, and arbitrary metadata attached to Freight and Stage resources can be retrieved using stageMetadata() and freightMetadata() expression functions.\nHow to Create Tags for Component Tracking#\rSince Kargo doesn\u0026rsquo;t create tags automatically, here are the approaches you can use:\nApproach 1: Create Git Tags in Your Promotion Process (Recommended)#\rYou can use bash commands within Kargo\u0026rsquo;s promotion steps to create Git tags.\napiVersion: kargo.akuity.io/v1alpha1 kind: Stage metadata: name: production spec: promotionTemplate: spec: vars: - name: gitopsRepo value: https://github.com/myorg/gitops.git - name: releaseTag # Compose a release tag from component versions value: | release-${{ ctx.stage }}- fe-${{ imageFrom(\u0026#39;myapp/frontend\u0026#39;).Tag }}- be-${{ imageFrom(\u0026#39;myapp/backend\u0026#39;).Tag }} steps: # 1. Clone and update manifests - uses: git-clone config: repoURL: ${{ vars.gitopsRepo }} checkout: - branch: main path: ./repo # 2. Update YAML with new versions - uses: yaml-update config: path: ./repo/manifests/production.yaml updates: - key: frontend.image.tag value: ${{ imageFrom(\u0026#39;myapp/frontend\u0026#39;).Tag }} - key: backend.image.tag value: ${{ imageFrom(\u0026#39;myapp/backend\u0026#39;).Tag }} # 3. Commit the changes - uses: git-commit as: commit config: path: ./repo message: | Promoted to production - Frontend: ${{ imageFrom(\u0026#39;myapp/frontend\u0026#39;).Tag }} - Backend: ${{ imageFrom(\u0026#39;myapp/backend\u0026#39;).Tag }} - Chart: ${{ chartFrom(\u0026#39;my-chart\u0026#39;).Version }} - Config: ${{ commitFrom(\u0026#39;github.com/myorg/config\u0026#39;).ID }} # 4. Create Git tag using bash - uses: bash config: script: | #!/bin/bash cd ./repo # Create annotated tag with component details git tag -a \u0026#34;${{ vars.releaseTag }}\u0026#34; -m \u0026#34;Production Release Freight: ${{ ctx.freight.name }} Stage: ${{ ctx.stage }} Promoted: $(date -u +%Y-%m-%dT%H:%M:%SZ) Components: - frontend: ${{ imageFrom(\u0026#39;myapp/frontend\u0026#39;).Tag }} (${{ imageFrom(\u0026#39;myapp/frontend\u0026#39;).Digest }}) - backend: ${{ imageFrom(\u0026#39;myapp/backend\u0026#39;).Tag }} (${{ imageFrom(\u0026#39;myapp/backend\u0026#39;).Digest }}) - helm-chart: ${{ chartFrom(\u0026#39;my-chart\u0026#39;).Version }} - config: ${{ commitFrom(\u0026#39;github.com/myorg/config\u0026#39;).ID }}\u0026#34; # 5. Push commit and tags - uses: git-push config: path: ./repo targetBranch: main # 6. Push the tag separately - uses: bash config: script: | #!/bin/bash cd ./repo git push origin \u0026#34;${{ vars.releaseTag }}\u0026#34;Result:\nGit commit shows the changes Git tag captures the complete component snapshot Tag is named with all component versions: release-production-fe-v2.1.0-be-v3.0.1 Approach 2: Store Component Manifest in Git#\rCreate a dedicated \u0026ldquo;release manifest\u0026rdquo; file that gets versioned:\nsteps: # ... clone, update manifests steps ... # Create release manifest file - uses: bash config: script: | #!/bin/bash cat \u0026gt; ./repo/releases/production-$(date +%Y%m%d-%H%M%S).yaml \u0026lt;\u0026lt;EOF apiVersion: v1 kind: ReleaseManifest metadata: stage: ${{ ctx.stage }} freight: ${{ ctx.freight.name }} timestamp: $(date -u +%Y-%m-%dT%H:%M:%SZ) components: frontend: image: myapp/frontend tag: ${{ imageFrom(\u0026#39;myapp/frontend\u0026#39;).Tag }} digest: ${{ imageFrom(\u0026#39;myapp/frontend\u0026#39;).Digest }} backend: image: myapp/backend tag: ${{ imageFrom(\u0026#39;myapp/backend\u0026#39;).Tag }} digest: ${{ imageFrom(\u0026#39;myapp/backend\u0026#39;).Digest }} helm: chart: my-chart version: ${{ chartFrom(\u0026#39;my-chart\u0026#39;).Version }} config: repo: github.com/myorg/config commit: ${{ commitFrom(\u0026#39;github.com/myorg/config\u0026#39;).ID }} EOF - uses: git-commit config: path: ./repo message: \u0026#34;Release manifest for production deployment\u0026#34; - uses: git-push config: path: ./repoResult: Every production deployment creates a YAML file tracking all component versions.\nApproach 3: Use Freight Metadata to Track Releases#\rArbitrary metadata can be attached to Freight and Stage resources using the set-metadata promotion step.\nsteps: # ... promotion steps ... # Tag this Freight with production metadata - uses: set-metadata config: target: freight metadata: productionReleaseTag: release-${{ ctx.stage }}-${{ timestamp() }} productionDeployedAt: ${{ timestamp() }} componentVersions: | { \u0026#34;frontend\u0026#34;: \u0026#34;${{ imageFrom(\u0026#39;myapp/frontend\u0026#39;).Tag }}\u0026#34;, \u0026#34;backend\u0026#34;: \u0026#34;${{ imageFrom(\u0026#39;myapp/backend\u0026#39;).Tag }}\u0026#34;, \u0026#34;chart\u0026#34;: \u0026#34;${{ chartFrom(\u0026#39;my-chart\u0026#39;).Version }}\u0026#34; }Access later:\n# In subsequent stages, you can read this metadata - uses: bash config: script: | echo \u0026#34;Previous production release: ${{ freightMetadata(\u0026#39;productionReleaseTag\u0026#39;) }}\u0026#34; Approach 4: External Release Tracking System#\rUse Kargo\u0026rsquo;s notification capabilities or webhooks to record releases externally:\nsteps: # ... promotion steps ... # Send release info to external system - uses: http-post config: url: https://your-release-tracker.com/api/releases body: | { \u0026#34;stage\u0026#34;: \u0026#34;${{ ctx.stage }}\u0026#34;, \u0026#34;freight\u0026#34;: \u0026#34;${{ ctx.freight.name }}\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;${{ timestamp() }}\u0026#34;, \u0026#34;components\u0026#34;: { \u0026#34;frontend\u0026#34;: { \u0026#34;tag\u0026#34;: \u0026#34;${{ imageFrom(\u0026#39;myapp/frontend\u0026#39;).Tag }}\u0026#34;, \u0026#34;digest\u0026#34;: \u0026#34;${{ imageFrom(\u0026#39;myapp/frontend\u0026#39;).Digest }}\u0026#34; }, \u0026#34;backend\u0026#34;: { \u0026#34;tag\u0026#34;: \u0026#34;${{ imageFrom(\u0026#39;myapp/backend\u0026#39;).Tag }}\u0026#34;, \u0026#34;digest\u0026#34;: \u0026#34;${{ imageFrom(\u0026#39;myapp/backend\u0026#39;).Digest }}\u0026#34; } } }Or send to Slack:\n- uses: send-message config: destination: slack channel: \u0026#34;#releases\u0026#34; message: | üöÄ Production Release Complete Freight: ${{ ctx.freight.alias }} Components: ‚Ä¢ Frontend: ${{ imageFrom(\u0026#39;myapp/frontend\u0026#39;).Tag }} ‚Ä¢ Backend: ${{ imageFrom(\u0026#39;myapp/backend\u0026#39;).Tag }} ‚Ä¢ Chart: ${{ chartFrom(\u0026#39;my-chart\u0026#39;).Version }} Comparison: Kargo\u0026rsquo;s Approach vs. Traditional Versioning#\rAspect Traditional Tagging Kargo Built-in Custom Kargo Implementation Unified version tag ‚úÖ Yes (v1.5.0 everywhere) ‚ùå No automatic tags ‚úÖ Can create custom tags Component-level versions ‚ùå Often lost ‚úÖ Freight tracks each ‚úÖ Can expose in tags/manifests Traceability ‚ö†Ô∏è Requires external tracking ‚úÖ Freight + Git commits ‚úÖ‚úÖ Tags + Freight + commits External visibility ‚úÖ Easy (just check tags) ‚ö†Ô∏è Need Kargo access ‚úÖ Custom tags visible externally Rollback ease ‚úÖ Simple (checkout v1.4.0) ‚úÖ Promote old Freight ‚úÖ Checkout tag or Freight Audit trail ‚ö†Ô∏è Tags can be deleted/moved ‚úÖ Immutable Freight ‚úÖ Git tags + Freight Best Practice Recommendation#\rFor maximum traceability outside of Kargo, I recommend a hybrid approach:\n1. Let Kargo do what it does best:#\rTrack component versions in Freight Create Git commits for every promotion Use Freight metadata for runtime tracking 2. Add custom tagging for external visibility:#\rapiVersion: kargo.akuity.io/v1alpha1 kind: PromotionTask metadata: name: production-release-with-tagging spec: vars: - name: releaseVersion # Generate semantic version or use custom format value: v${{ timestamp(\u0026#39;2006.01.02\u0026#39;) }}-${{ ctx.freight.alias }} steps: # Standard promotion steps - uses: git-clone # ... clone repo - uses: yaml-update # ... update manifests - uses: git-commit as: commit config: path: ./repo message: | Release ${{ vars.releaseVersion }} to production Freight: ${{ ctx.freight.name }} (${{ ctx.freight.alias }}) Component Versions: - Frontend: ${{ imageFrom(\u0026#39;myapp/frontend\u0026#39;).Tag }} @ ${{ imageFrom(\u0026#39;myapp/frontend\u0026#39;).Digest }} - Backend: ${{ imageFrom(\u0026#39;myapp/backend\u0026#39;).Tag }} @ ${{ imageFrom(\u0026#39;myapp/backend\u0026#39;).Digest }} - Chart: ${{ chartFrom(\u0026#39;my-chart\u0026#39;).Version }} - Config: ${{ commitFrom(\u0026#39;github.com/myorg/config\u0026#39;).ID }} # Create release tag - uses: bash config: script: | #!/bin/bash cd ./repo # Create comprehensive annotated tag git tag -a \u0026#34;${{ vars.releaseVersion }}\u0026#34; \\ -m \u0026#34;Production Release ${{ vars.releaseVersion }} Kargo Freight: ${{ ctx.freight.name }} Freight Alias: ${{ ctx.freight.alias }} Deployed: $(date -u +%Y-%m-%dT%H:%M:%SZ) Component Manifest: ================== Frontend Image: Repository: myapp/frontend Tag: ${{ imageFrom(\u0026#39;myapp/frontend\u0026#39;).Tag }} Digest: ${{ imageFrom(\u0026#39;myapp/frontend\u0026#39;).Digest }} Backend Image: Repository: myapp/backend Tag: ${{ imageFrom(\u0026#39;myapp/backend\u0026#39;).Tag }} Digest: ${{ imageFrom(\u0026#39;myapp/backend\u0026#39;).Digest }} Helm Chart: Name: my-chart Version: ${{ chartFrom(\u0026#39;my-chart\u0026#39;).Version }} Configuration: Repo: github.com/myorg/config Commit: ${{ commitFrom(\u0026#39;github.com/myorg/config\u0026#39;).ID }} \u0026#34; - uses: git-push config: path: ./repo # Push tags - uses: bash config: script: | cd ./repo git push origin \u0026#34;${{ vars.releaseVersion }}\u0026#34; # Also create release manifest file - uses: bash config: script: | cat \u0026gt; ./repo/releases/${{ vars.releaseVersion }}.json \u0026lt;\u0026lt;EOF { \u0026#34;version\u0026#34;: \u0026#34;${{ vars.releaseVersion }}\u0026#34;, \u0026#34;freight\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;${{ ctx.freight.name }}\u0026#34;, \u0026#34;alias\u0026#34;: \u0026#34;${{ ctx.freight.alias }}\u0026#34; }, \u0026#34;stage\u0026#34;: \u0026#34;${{ ctx.stage }}\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;$(date -u +%Y-%m-%dT%H:%M:%SZ)\u0026#34;, \u0026#34;components\u0026#34;: { \u0026#34;frontend\u0026#34;: { \u0026#34;image\u0026#34;: \u0026#34;myapp/frontend\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;${{ imageFrom(\u0026#39;myapp/frontend\u0026#39;).Tag }}\u0026#34;, \u0026#34;digest\u0026#34;: \u0026#34;${{ imageFrom(\u0026#39;myapp/frontend\u0026#39;).Digest }}\u0026#34; }, \u0026#34;backend\u0026#34;: { \u0026#34;image\u0026#34;: \u0026#34;myapp/backend\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;${{ imageFrom(\u0026#39;myapp/backend\u0026#39;).Tag }}\u0026#34;, \u0026#34;digest\u0026#34;: \u0026#34;${{ imageFrom(\u0026#39;myapp/backend\u0026#39;).Digest }}\u0026#34; }, \u0026#34;chart\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;my-chart\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;${{ chartFrom(\u0026#39;my-chart\u0026#39;).Version }}\u0026#34; }, \u0026#34;config\u0026#34;: { \u0026#34;repo\u0026#34;: \u0026#34;github.com/myorg/config\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;${{ commitFrom(\u0026#39;github.com/myorg/config\u0026#39;).ID }}\u0026#34; } } } EOF git add releases/${{ vars.releaseVersion }}.json git commit -m \u0026#34;Add release manifest for ${{ vars.releaseVersion }}\u0026#34; git push What This Gives You#\rInside Kargo:\n‚úÖ Full Freight tracking with all component versions ‚úÖ Immutable audit trail ‚úÖ Easy promotion/rollback via UI Outside Kargo:\n‚úÖ Git tags anyone can see: git tag -l ‚úÖ Detailed tag annotations: git show v2025.01.15-brave-eagle ‚úÖ Release manifest files for programmatic access ‚úÖ Standard Git operations work: git checkout v2025.01.15-brave-eagle External Systems Can:\nQuery Git tags to see what\u0026rsquo;s in production Parse release manifest JSON files Build dashboards from Git history Integrate with existing release processes that expect tags Summary#\rDirect Answer:\nKargo doesn\u0026rsquo;t automatically create Git tags for component combinations BUT you can easily add this yourself using bash steps in promotion templates Kargo DOES track everything in Freight objects and Git commits Recommended Approach:\nUse Kargo\u0026rsquo;s Freight for internal tracking (it\u0026rsquo;s excellent at this) Add custom Git tags in your promotion process for external visibility Optionally create release manifest files for programmatic access Use Freight metadata for runtime tracking between stages This hybrid approach gives you the best of both worlds: Kargo\u0026rsquo;s powerful promotion orchestration PLUS traditional Git tags for external systems that expect them.\nDoes this answer your question? Would you like me to provide a complete working example for your specific multi-repo setup?\n"},{"id":24,"href":"/musings/docs/misc/in-browser-editing-tools/","title":"In-Browser Editing Documentation Solutions","section":"Miscellaneous","content":"Based on my research, here\u0026rsquo;s a comprehensive breakdown of open-source options for in-browser editing documentation sites:\nIn-Browser Editing Documentation Solutions#\rCategory 1: Wiki-Like Solutions (Instant Browser Editing)#\rThese are true wikis with WYSIWYG or simple markup editors directly in the browser.\nTop Open-Source Wiki Options:#\r1. BookStack ‚≠ê (Recommended for ease of use)#\rWhat it is: Modern, Laravel-based wiki with a book metaphor (Shelves ‚Üí Books ‚Üí Chapters ‚Üí Pages)\nPros:\n‚úÖ Beautiful, modern interface - Most polished UI among open-source wikis ‚úÖ Easy to use - WYSIWYG editor (TinyMCE) with no learning curve ‚úÖ Simple installation - PHP/Laravel, straightforward setup ‚úÖ Excellent permissions - Granular role-based access control ‚úÖ Good documentation structure - Intuitive hierarchy ‚úÖ Markdown support - Optional Markdown editor with live preview ‚úÖ Export options - PDF, HTML, plain text, Markdown ‚úÖ Authentication - LDAP, SAML2, OpenID Connect, OAuth ‚úÖ MFA support - Can be enforced per role ‚úÖ Draw.io integration - Built-in diagram editor ‚úÖ Active development - Well-maintained with growing community ‚úÖ Audit logs - Track all changes Cons:\n‚ùå Opinionated structure - Book/Chapter/Page hierarchy can feel limiting ‚ùå No real-time collaboration - Can\u0026rsquo;t edit simultaneously with others ‚ùå Limited customization - Visual theme options are basic ‚ùå No inline comments - Missing modern collaboration features ‚ùå Performance at scale - May slow with very large instances ‚ùå Requires PHP/MySQL - Not as lightweight as flat-file solutions Best for: Teams wanting a beautiful, easy-to-use wiki without complexity; great for technical documentation, SOPs, and knowledge bases\nTech stack: PHP, Laravel, MySQL/MariaDB\n2. Wiki.js ‚≠ê (Recommended for developers)#\rWhat it is: Modern, Node.js-based wiki with Git sync and multiple editor options\nPros:\n‚úÖ Multiple editors - Markdown, WYSIWYG, HTML, WikiText, API docs editor ‚úÖ Git integration - Auto-sync with GitHub, GitLab, etc. ‚úÖ Modern interface - Clean, beautiful UI with light/dark mode ‚úÖ Built-in search - Good search engine out of the box ‚úÖ Authentication - LDAP, SAML, OAuth, many social providers ‚úÖ Fast performance - Node.js is speedy ‚úÖ Flexible storage - PostgreSQL, MySQL, MariaDB, MS SQL, SQLite ‚úÖ Developer-friendly - Appeals to technical teams ‚úÖ Good documentation - Well-documented project Cons:\n‚ùå No native WYSIWYG - Visual editor exists but less polished than BookStack ‚ùå Requires Node.js - Different tech stack than typical LAMP setups ‚ùå Less intuitive - Steeper learning curve than BookStack ‚ùå Smaller community - Newer project, fewer resources ‚ùå Limited templates - Not as much structure guidance ‚ùå Permission complexity - Can be tricky to configure Best for: Developer teams who want Git integration and Markdown-first workflow; teams comfortable with Node.js\nTech stack: Node.js, Vue.js, Database (multiple options)\n3. DokuWiki#\rWhat it is: Lightweight, no-database wiki using flat files\nPros:\n‚úÖ No database required - Stores everything as text files ‚úÖ Easy backup - Just copy files ‚úÖ Simple installation - Drop it on PHP server and go ‚úÖ Large plugin ecosystem - 1500+ plugins ‚úÖ Access control - Built-in ACLs ‚úÖ Clean syntax - Easy markup language ‚úÖ Mature project - Been around since 2004 Cons:\n‚ùå No WYSIWYG by default - Requires plugin for visual editing ‚ùå Dated interface - Looks old-fashioned ‚ùå Not scalable for high traffic - Flat files have limits ‚ùå Plugin dependency - Need plugins for basic features (categories, rich text) ‚ùå Less modern - Feels legacy compared to BookStack/Wiki.js Best for: Small teams, personal wikis, simple documentation needs; teams who want easy backups and no database\nTech stack: PHP (no database)\n4. MediaWiki#\rWhat it is: The software that runs Wikipedia - extremely powerful but complex\nPros:\n‚úÖ Battle-tested - Powers Wikipedia and thousands of wikis ‚úÖ Highly scalable - Handles massive content and traffic ‚úÖ Extensive features - Everything you could want ‚úÖ Large ecosystem - Thousands of extensions ‚úÖ Semantic capabilities - Can add structured data (Semantic MediaWiki) ‚úÖ Strong version control - Excellent revision history ‚úÖ BlueSpice version - Enterprise features available Cons:\n‚ùå Steep learning curve - Very complex for non-technical users ‚ùå Dated interface - Feels old unless customized ‚ùå Complex setup - Requires significant configuration ‚ùå Extension management - Need to install many extensions for basic features ‚ùå Not beginner-friendly - Overwhelming for most teams ‚ùå Maintenance overhead - Requires technical expertise Best for: Large organizations with technical teams; projects needing Wikipedia-level features and scale\nTech stack: PHP, MySQL\n5. XWiki#\rWhat it is: Enterprise-grade wiki platform with application development capabilities\nPros:\n‚úÖ Very powerful - Can build custom applications ‚úÖ Strong WYSIWYG - Good visual editor ‚úÖ Extensible - 750+ extensions ‚úÖ Structured data - Database-like capabilities within wiki ‚úÖ Enterprise features - Advanced workflows, permissions ‚úÖ Office integration - Import from Word, Excel Cons:\n‚ùå Complex - Steep learning curve ‚ùå Heavy - Resource-intensive (Java-based) ‚ùå Overwhelming - Too many features for simple needs ‚ùå Smaller user base - Less community support ‚ùå Setup complexity - Requires Java expertise Best for: Large enterprises needing custom applications and workflows; teams with Java expertise\nTech stack: Java, Database\nQuick Comparison Table: Wiki Solutions#\rFeature BookStack Wiki.js DokuWiki MediaWiki XWiki Ease of Use ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê ‚≠ê‚≠ê Modern UI ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê ‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê WYSIWYG Editor ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê ‚≠ê (plugin) ‚≠ê‚≠ê (ext) ‚≠ê‚≠ê‚≠ê‚≠ê Setup Simplicity ‚≠ê‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê ‚≠ê‚≠ê Scalability ‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê‚≠ê Customization ‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Best For Small-medium teams Developers Simple needs Large scale Enterprise Category 2: Other Open-Source Documentation Solutions#\rThese are NOT traditional wikis but support browser-based editing:\nStatic Site Generators (Minimal In-Browser Editing)#\rDocusaurus, MkDocs, Jekyll, Hugo\n‚ùå No real in-browser editing - edit locally, push to Git ‚úÖ Great for versioned documentation ‚úÖ Fast, secure static sites ‚ùå Not wiki-like; requires Git workflow CMS-Based Solutions#\rGitBook (self-hosted)\n‚úÖ Beautiful interface ‚úÖ Git integration ‚ùå Newer versions are SaaS-only ‚ùå Open-source version is outdated Outline\n‚úÖ Modern, Notion-like interface ‚úÖ Real-time collaboration ‚úÖ Slack integration ‚ö†Ô∏è More complex to self-host ‚úÖ Open-source alternative worth exploring My Recommendations#\rFor most teams: Start with BookStack - it\u0026rsquo;s the sweet spot of ease-of-use, modern interface, and features.\nFor developer teams: Try Wiki.js - especially if you want Git sync and Markdown-first workflow.\nFor simplicity: Use DokuWiki - if you want no database and easy maintenance.\nFor scale: Consider MediaWiki - but only if you have technical resources.\nFor enterprise: Evaluate XWiki - if you need custom applications.\nAll of these support true in-browser editing (no Git workflow required), which seems to be your primary requirement. BookStack and Wiki.js offer the best modern experience with instant editing.\n"},{"id":25,"href":"/musings/docs/misc/initial-comparison/","title":"Initial Comparison","section":"Miscellaneous","content":"Based on my extensive experience integrating these tools, here\u0026rsquo;s how each solves distinct problems and creates multiplicative value when combined:\nOpen Source Developer Portals#\rBackstage (Apache 2.0, CNCF)#\rProblem it solves: Developer cognitive overload and tool sprawl. In organizations with 50+ microservices, developers waste hours finding documentation, identifying service owners, and navigating different tools.\nWhat it does: Creates a unified UI layer (the \u0026ldquo;single pane of glass\u0026rdquo;) with a software catalog, documentation hub (TechDocs), and template scaffolding for standardized service creation.\nCritical insight: Backstage is just the UI layer. It doesn\u0026rsquo;t provision infrastructure or deploy anything‚Äîit\u0026rsquo;s the developer-facing interface to your platform capabilities.\nKratix (Apache 2.0)#\rProblem it solves: Platform teams becoming ticket-takers instead of product teams. The \u0026ldquo;central ops team bottleneck\u0026rdquo; where every infrastructure request requires manual intervention.\nWhat it does: Platform orchestration framework that sits between the developer portal (like Backstage) and infrastructure tools (like Crossplane/Terraform). Introduces \u0026ldquo;Promises\u0026rdquo;‚Äîcomposable platform capabilities that teams can own and evolve independently.\nCritical insight: Kratix enables \u0026ldquo;platform democracy\u0026rdquo;‚Äîdomain teams can contribute platform capabilities within guardrails, not just consume them. This prevents platform team burnout at scale.\nOpen Source Platform Orchestrators#\rCrossplane (Apache 2.0, CNCF)#\rProblem it solves: The Terraform/cloud provider API sprawl. Every cloud resource requires different tooling, state management, and workflows.\nWhat it does: Extends Kubernetes to manage any infrastructure using the Kubernetes API. Defines \u0026ldquo;Composite Resources\u0026rdquo; (XRs) that bundle multiple cloud resources behind a single, team-designed API.\nCritical insight: Crossplane turns infrastructure into Kubernetes-native APIs. A developer requests a \u0026ldquo;Database\u0026rdquo; (custom resource), Crossplane provisions RDS + security groups + secrets, all reconciled continuously like pods.\nKubeVela (Apache 2.0, CNCF)#\rProblem it solves: Application deployment complexity across hybrid infrastructure. The disconnect between \u0026ldquo;how developers think about apps\u0026rdquo; vs \u0026ldquo;how infrastructure thinks about resources.\u0026rdquo;\nWhat it does: Implements the Open Application Model (OAM). Lets developers define apps with ApplicationConfiguration that describes workload + dependencies‚ÄîKubeVela handles rendering to Kubernetes/Terraform/anything.\nCritical insight: Application-centric vs infrastructure-centric. KubeVela abstracts how things deploy. Same app definition works on Kubernetes, edge, or cloud functions.\nKusionStack/Kusion (Apache 2.0)#\rProblem it solves: Configuration drift and environment inconsistency. The \u0026ldquo;works in dev, breaks in prod\u0026rdquo; problem at enterprise scale.\nWhat it does: Intent-driven orchestrator using \u0026ldquo;AppConfiguration\u0026rdquo; as single source of truth. Platform engineers define modules/workspaces; developers only specify intent. Kusion generates environment-specific configs.\nCritical insight: Separation of concerns‚Äîdevelopers focus on what (intent), platform team controls how (modules). Prevents YAML sprawl and environment-specific hacks.\nMultiplicative Combination Patterns#\rPattern 1: The Golden Triangle (Backstage + Crossplane + ArgoCD/Flux)#\rMultiplicative effect: 10x reduction in infrastructure provisioning time + 90% elimination of configuration drift\nHow it works:\nBackstage provides the developer UX with templates Developer clicks \u0026ldquo;Create Database\u0026rdquo; in Backstage Backstage scaffolder generates a Crossplane Claim (YAML) and commits to Git ArgoCD/Flux (GitOps) syncs the Claim to Kubernetes Crossplane provisions actual cloud resources (RDS, VPC, etc.) Status flows back up: ArgoCD shows sync state, Crossplane provides resource status, Backstage displays in catalog Why multiplicative: Each tool does ONE thing well. Backstage doesn\u0026rsquo;t know about AWS APIs. Crossplane doesn\u0026rsquo;t render UIs. GitOps ensures everything is auditable and reversible. Together they create guaranteed infrastructure-as-code with self-service.\nPattern 2: Platform Democracy Stack (Kratix + Crossplane + Backstage)#\rMultiplicative effect: Enables 100+ developers across 20+ teams to contribute platform capabilities without central team bottleneck\nHow it works:\nKratix sits between Backstage (top) and Crossplane (bottom) Platform team defines Kratix Promises (e.g., \u0026ldquo;GitOps Pipeline\u0026rdquo; or \u0026ldquo;ML Platform\u0026rdquo;) Each Promise is a workflow that orchestrates multiple tools (Crossplane for infra, Terraform for config, scripts for setup) Domain teams can create their own Promises within guardrails Backstage exposes Promises as self-service templates Crossplane handles the actual resource provisioning Why multiplicative: Kratix enables platform composition. ML team creates \u0026ldquo;ML Environment\u0026rdquo; Promise using existing \u0026ldquo;Database\u0026rdquo; + \u0026ldquo;S3 Bucket\u0026rdquo; + \u0026ldquo;Jupyter Notebook\u0026rdquo; Promises. Platform team maintains primitives, domain teams compose solutions. Scales to 1000s of developers without linear growth in platform team.\nPattern 3: Multi-Cloud/Hybrid Stack (KubeVela + Crossplane + Backstage)#\rMultiplicative effect: Deploy same app to AWS, Azure, edge, and on-prem with zero code changes\nHow it works:\nDeveloper defines app in KubeVela ApplicationConfiguration (workload + traits) Crossplane provides the infrastructure APIs (databases, networks, clusters) KubeVela renders deployment based on target environment policies Backstage shows unified view across all environments Why multiplicative: KubeVela handles application semantics (scaling, rollout strategy), Crossplane handles infrastructure semantics (which cloud, which region). Developer writes ONE config, platform team controls deployment strategy and infrastructure choices per environment.\nPattern 4: Enterprise Governance Stack (Kusion + Crossplane + Backstage)#\rMultiplicative effect: 100% compliance with security/cost/regulatory policies while maintaining developer velocity\nHow it works:\nPlatform team defines Kusion Modules (blessed patterns for databases, apps, networks) Developers write minimal AppConfiguration (just the intent) Kusion generates full configurations based on environment (dev uses small DB, prod uses HA cluster) Generated configs use Crossplane for provisioning Backstage provides audit trail and visibility Why multiplicative: Kusion enforces standards at generation time (can\u0026rsquo;t deploy non-compliant configs), Crossplane enforces at provisioning time (can\u0026rsquo;t create resources outside policies), Backstage provides visibility. Creates security-by-default without blocking developers.\nKey Architectural Insight#\rThe real power comes from layered separation of concerns:\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\r‚îÇ Backstage (Developer Portal) ‚îÇ ‚Üê What developers see\r‚îÇ \u0026#34;I want a production database\u0026#34; ‚îÇ\r‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\r‚îÇ\r‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\r‚îÇ Kratix (Platform Orchestrator) ‚îÇ ‚Üê Workflow coordination\r‚îÇ \u0026#34;Provision DB + Configure + Alert\u0026#34;‚îÇ\r‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\r‚îÇ\r‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\r‚îÇ Crossplane/KubeVela (Infra Mgmt) ‚îÇ ‚Üê Actual provisioning\r‚îÇ \u0026#34;Create RDS + VPC + Security\u0026#34; ‚îÇ\r‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\r‚îÇ\r‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\r‚îÇ ArgoCD/Flux (GitOps) ‚îÇ ‚Üê Continuous reconciliation\r‚îÇ \u0026#34;Ensure desired = actual state\u0026#34; ‚îÇ\r‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îòEach layer handles ONE concern. When combined, you get:\nSelf-service without chaos (Backstage UI + Kratix orchestration) Infrastructure without YAML hell (Crossplane APIs) Compliance without tickets (Kusion/policy) Auditability without paperwork (GitOps) This is why organizations see 10-100x productivity improvements‚Äînot from any single tool, but from eliminating the friction between them.\n"},{"id":26,"href":"/musings/docs/misc/comprehensive-guide/","title":"Kargo and ArgoCD: A Comprehensive Guide","section":"Miscellaneous","content":"Kargo and ArgoCD: A Comprehensive Guide#\rExecutive Summary#\rThis document provides a comprehensive overview of Kargo and ArgoCD, two complementary tools in the GitOps ecosystem. While ArgoCD excels at continuous deployment by syncing Kubernetes clusters to desired states stored in Git repositories, it lacks built-in promotion workflows between environments. Kargo fills this gap by orchestrating progressive rollouts across multiple stages (dev, staging, production) while maintaining GitOps principles.\nOverview#\rArgoCD: Continuous Deployment#\rArgoCD is a declarative, GitOps continuous delivery tool for Kubernetes. It continuously monitors Git repositories and ensures that the actual state of your Kubernetes cluster matches the desired state declared in Git.\nCore Capabilities:\nSyncs Kubernetes clusters to Git repository states Supports multiple configuration management tools (Kustomize, Helm, Jsonnet, plain YAML) Provides automated deployment with rollback capabilities Offers application health monitoring and visualization Enables automated or manual sync policies What ArgoCD Does Well:\nDeploying applications to Kubernetes clusters Maintaining desired state from Git Detecting configuration drift Providing deployment visualization What ArgoCD Doesn\u0026rsquo;t Provide:\nNo concept of \u0026ldquo;promotion\u0026rdquo; between environments No orchestration of changes from dev ‚Üí staging ‚Üí production No built-in artifact version tracking across stages No progressive delivery workflows Kargo: Continuous Promotion#\rKargo is a continuous promotion platform that orchestrates the lifecycle of applications across multiple stages using GitOps principles. Built by the creators of Argo, Kargo automates progressive rollouts while integrating seamlessly with ArgoCD.\nCore Capabilities:\nMulti-stage promotion orchestration (dev ‚Üí staging ‚Üí production) Artifact tracking across environments (container images, Helm charts, Git commits) State-driven promotion without relying on CI pipelines Integration with ArgoCD for deployment execution Audit trails and promotion history The Complementary Relationship:\nArgoCD = The deployment engine (handles the \u0026ldquo;how\u0026rdquo; of deploying) Kargo = The promotion orchestrator (handles the \u0026ldquo;when\u0026rdquo; and \u0026ldquo;what\u0026rdquo; gets promoted) Core Concepts#\r1. Kargo Fundamental Concepts#\rWarehouses#\rWarehouses are the source of Freight. They monitor repositories (container registries, Git repositories, Helm chart repositories) for new artifact versions. When new artifacts are discovered, Warehouses produce Freight.\nKey Characteristics:\nSubscribe to one or more repositories Discover new artifact versions automatically or on-demand Apply version constraints (semver, lexical, newest) Can be configured for automatic or manual Freight creation Support webhook triggers for immediate discovery Freight#\rFreight represents a bundle of specific artifact versions that move through your promotion pipeline as a unit. Think of Freight as a \u0026ldquo;meta-artifact\u0026rdquo; or a shipping container that holds multiple artifacts together.\nWhat Freight Contains:\nSpecific container image versions (with digests, not just tags) Specific Git commit SHAs Specific Helm chart versions Any combination of the above Key Characteristics:\nImmutable once created (except alias and status fields) Has a unique SHA-1 hash identifier Has a human-friendly alias (e.g., \u0026ldquo;brave-eagle\u0026rdquo;) References artifacts by immutable identifiers (image digests, commit SHAs) Promoted as an atomic unit across stages Critical Insight: Freight is NOT a single version number applied to everything. Instead, it\u0026rsquo;s a tested, verified combination of specific artifact versions.\nStages#\rStages represent promotion targets in your application lifecycle. While stages often correspond to environments (dev, staging, production), they technically represent any desired state that needs to be altered through promotion.\nKey Characteristics:\nDefine what Freight they accept (from Warehouses or upstream Stages) Specify how to promote Freight (promotion mechanisms) Can be configured for automatic or manual promotion Track health status of deployed applications Form pipelines by subscribing to upstream Stages Promotions#\rPromotions are the execution of moving Freight into a Stage. A Promotion is a Kubernetes resource created when Freight is promoted to a Stage.\nWhat Happens During Promotion:\nExecute a series of discrete steps (git clone, update manifests, commit, push) Update GitOps repository with new artifact versions Trigger ArgoCD Application sync Monitor deployment health Mark Stage as verified upon success Promotion Mechanisms:\nGit repository updates (modify Kustomize overlays, Helm values) ArgoCD Application updates (trigger syncs, update revisions) Custom scripts and workflows Pull request workflows Projects#\rProjects are units of tenancy for organizing and managing promotion pipelines. Every Kargo Project is associated with a Kubernetes namespace, making it easy to manage access using standard Kubernetes RBAC.\n2. How Kargo and ArgoCD Work Together#\rThe Integration Model#\rKargo and ArgoCD integrate through a declarative, Kubernetes-native approach. Kargo does NOT call ArgoCD\u0026rsquo;s REST API; instead, it modifies ArgoCD Application Custom Resources directly via the Kubernetes API.\nPrimary Interaction Methods:\nCustom Resource Modification: Kargo patches ArgoCD Application resources to update fields like targetRevision or trigger sync operations Status Monitoring: Kargo continuously watches Application health and sync status Authorization via Annotations: Applications explicitly authorize Stages to manage them Authorization Model:\nApplications must be annotated to grant Kargo permission:\nkargo.akuity.io/authorized-stage: \u0026#34;\u0026lt;project-name\u0026gt;:\u0026lt;stage-name\u0026gt;\u0026#34;This annotation-based delegation ensures that only authorized Stages can modify specific Applications. Since the annotation can only be added by someone authorized to update the Application, Kargo interprets it as explicit permission.\nTypical Promotion Workflow#\rArtifact Discovery: Warehouse detects new container image in registry Freight Creation: Warehouse creates Freight referencing the new image version Git Update: Developer approves promotion; Kargo updates GitOps repository Application Patch: Kargo patches ArgoCD Application\u0026rsquo;s targetRevision field ArgoCD Sync: ArgoCD detects change and syncs cluster to new state Health Monitoring: Kargo monitors Application health Stage Verification: Once healthy, Freight is marked as verified for next Stage Progressive Promotion: Verified Freight can be promoted to subsequent Stages Key Insight: ArgoCD focuses on deployment mechanics (syncing Git to cluster), while Kargo handles promotion logic (orchestrating what gets deployed when).\n3. Multi-Repository Artifact Management#\rThe Challenge#\rIn modern microservices architectures, applications often have:\nApplication code in multiple repositories Dockerfiles in a separate repository Helm charts in another repository Configuration spread across multiple locations How does Kargo keep all these in sync?\nKargo\u0026rsquo;s Solution: Freight Bundles#\rKargo creates Freight bundles that reference specific versions of ALL artifacts from multiple repositories. A single Warehouse can subscribe to multiple repositories, and when it discovers new versions, it produces Freight containing one version of each artifact.\nExample Freight Bundle:\nFrontend image: myapp/frontend:v2.1.0 (digest: sha256:abc\u0026hellip;) Backend image: myapp/backend:v3.0.1 (digest: sha256:def\u0026hellip;) Helm chart: my-chart:v0.5.2 Config commit: 7f4e8c2a9b1d3e5f6a8c9d0e1f2a3b4c5d6e7f8a Being referenced by a single Freight resource, these artifacts are promoted from stage to stage together as a unit.\nComparison to Traditional Unified Versioning#\rTraditional Approach (Unified Version Tags):\nTag everything with the same version (e.g., v1.5.0) Simple mental model and easy communication BUT: Tight coupling, requires rebuilding everything for any change BUT: Can\u0026rsquo;t promote just a config change or single service update Kargo Approach (Freight Bundles):\nEach artifact maintains its own version Bundled together as tested combinations Independent versioning with coordinated promotion Selective promotion capabilities Key Trade-offs:\nUnified versioning: Simple but inflexible Kargo bundles: Flexible but more complex Handling Incompatible Combinations#\rThe Race Condition Problem:\nWhen Warehouses subscribe to multiple repositories, there\u0026rsquo;s a risk of creating incompatible Freight:\nCI pushes frontend:v2.0.0 Kargo discovers it immediately Kargo creates Freight with frontend:v2.0.0 + backend:v1.5.0 (old) This combination is incompatible Later backend:v2.0.0 arrives, but Freight already exists Solutions:\nFreight Creation Criteria: Use expressions to ensure only compatible combinations create Freight Multiple Warehouses: Separate fast-moving artifacts (images) from slow-moving ones (config) Manual Freight Assembly: Disable automatic creation and manually select compatible versions Gatekeeper Stage: Create initial stage where failures are acceptable, filtering bad combinations 4. Combining and Manipulating Freight#\rCan You Combine Freight After Creation?#\rShort Answer: No direct merging, but several approaches achieve similar outcomes.\nWhy No Direct Merging:\nFreight is immutable by design (for audit trails and reproducibility) Merging would break the clean Warehouse ‚Üí Freight ‚Üí Stage lineage Could create untested combinations that bypass verification Available Approaches:\nManual Freight Assembly: Configure Warehouse to discover multiple versions but not auto-create Freight; manually select which versions to bundle\nFreight Cloning: Use existing Freight as a template, replacing only specific artifacts (useful for hotfixes)\nMultiple Warehouses with Stage-Level Combination: Different Warehouses track different artifact types; Stages request Freight from multiple Warehouses and combine during promotion\nFreight Creation Criteria: Define expressions that control when automatic Freight creation occurs\nUse Cases:\nHotfixes: Clone production Freight, replace one buggy component Testing: Create \u0026ldquo;what-if\u0026rdquo; combinations without new builds Different Cadences: Promote images frequently, config changes rarely 5. External Release Tracking#\rThe Challenge#\rWhile Kargo provides excellent internal tracking through Freight objects and Git commits, organizations often need to track releases in external systems or create unified version numbers for communication purposes.\nCreating Tags for External Tracking#\rKargo doesn\u0026rsquo;t automatically create Git tags for component combinations, but you can implement custom tagging in promotion workflows.\nApproaches:\nGit Tags in Promotion Process: Use bash steps to create annotated tags containing all component versions\nRelease Manifest Files: Create versioned YAML/JSON files in Git documenting exact component versions for each release\nFreight Metadata: Attach metadata to Freight resources using the set-metadata promotion step\nExternal System Integration: Send release information to external tracking systems via HTTP calls or notifications\nBenefits of Hybrid Approach:\nKargo tracks components internally (Freight + commits) Git tags provide external visibility Release manifests enable programmatic access External systems can query Git without Kargo access Trade-offs:\nInternal tracking (Freight): Rich, immutable, tied to promotion workflow External tags: Simple, universally accessible, but requires custom implementation 6. Promotion Templates and Tasks#\rPromotion Templates#\rPromotion Templates define how Kargo transitions Freight into a Stage by executing a series of discrete, composable steps. Each Stage has a promotion template that specifies exactly what actions to take when Freight is promoted.\nCommon Promotion Steps:\ngit-clone: Clone Git repository and checkout specific commits/branches git-clear: Clear working directory contents yaml-update: Update YAML file values (image tags, versions) kustomize-set-image: Update Kustomize image references helm-update-image: Update Helm values files git-commit: Commit all changes to working tree git-push: Push commits to remote repository argocd-update: Update ArgoCD Applications and trigger syncs Promotion Tasks#\rPromotion Tasks are reusable sets of promotion steps that can be shared across multiple Stages or Projects. They come in two forms:\nPromotionTask: Project-scoped, available within a single Project ClusterPromotionTask: Cluster-scoped, available across all Projects Benefits:\nStandardize common workflows Reduce duplication Centralize best practices Enable organizational patterns 7. Warehouse Configuration Patterns#\rSubscription Types#\rWarehouses can subscribe to:\nContainer Image Repositories: Monitor for new image tags Git Repositories: Monitor for new commits or tags Helm Chart Repositories: Monitor for new chart versions Selection Strategies#\rDifferent strategies for selecting which artifacts to track:\nFor Git Repositories:\nNewestFromBranch: Latest commit from specified branch SemVer: Commit referenced by tag matching semantic versioning constraint Lexical: Latest tag by lexical ordering For Container Images:\nSemVer: Images with semantic version tags Lexical: Latest by lexical tag ordering NewestBuild: Most recently built image Freight Creation Policies#\rAutomatic: Warehouse creates Freight whenever new artifacts are discovered (default)\nManual: Warehouse discovers artifacts but doesn\u0026rsquo;t create Freight; user manually selects versions\nAutomatic with Criteria: Warehouse creates Freight only when expression evaluates to true (prevents incompatible combinations)\n8. Progressive Delivery Patterns#\rCommon Case Pattern#\rSingle Warehouse subscribes to both image repository and Git repository. Each discovery produces Freight with both an image version and config commit. Artifacts promoted together as a unit through all stages.\nUse Case: Most standard applications where image and config should stay coordinated.\nGrouped Services Pattern#\rSingle Warehouse subscribes to multiple service image repositories. All services promoted together as a tightly-coupled unit.\nUse Case: Microservices that have strong interdependencies and must be deployed together.\nIndependent Pipelines Pattern#\rMultiple Warehouses track different artifact types. Each produces Freight independently, creating parallel promotion pipelines through the same Stages.\nUse Case: Promote images rapidly (multiple times daily) while promoting config changes slowly and deliberately.\nGatekeeper Stage Pattern#\rInitial Stage allows failures to filter out bad Freight combinations before they reach important environments.\nUse Case: Prevent incompatible artifact combinations from progressing through pipeline.\n9. Verification and Health Monitoring#\rStage Health#\rStage health is determined by multiple factors:\nHealth of ArgoCD Applications managed by the Stage Verification checks configured on the Stage Promotion completion status Custom health indicators Key Insight: Stage health is NOT just Application health. It encompasses all health checks registered during promotion, including custom verification.\nHealth Check Registration#\rWhen the argocd-update step completes successfully, it registers health checks that continuously monitor the Application. This enables Kargo to factor ArgoCD Application health into Stage health without requiring Kargo to understand Application health directly.\nVerification#\rStages can define verification processes that must pass before Freight is considered verified:\nArgo Rollouts AnalysisRun resources Kubernetes Jobs for custom tests External validation systems 10. Storage and Branch Strategies#\rStage-Specific Branches#\rKargo commonly uses stage-specific branches in GitOps repositories:\nstage/dev contains rendered manifests for dev environment stage/staging contains rendered manifests for staging stage/production contains rendered manifests for production Benefits:\nClear separation of environment states Easy rollback (checkout previous commit on branch) Complete history per environment No merge conflicts between environments Important: These are NOT Git Flow branches requiring merging. Each branch is independent storage for rendered state.\nSingle Branch with Paths#\rAlternative approach uses single branch (main/trunk) with different paths:\nbuilds/dev/ contains dev manifests builds/staging/ contains staging manifests builds/production/ contains production manifests Key Requirement: Warehouse must use path filters to avoid triggering new Freight from its own promotion outputs (creating feedback loops).\nArchitectural Principles#\rGitOps Compliance#\rBoth Kargo and ArgoCD adhere to GitOps principles:\nDeclarative: Desired state defined declaratively Versioned: All state stored in Git with full history Immutable: Freight and commits are immutable Automated: Changes automatically reconciled Auditable: Complete trail of who changed what when Separation of Concerns#\rArgoCD Responsibilities:\nMonitor Git repositories for changes Sync Kubernetes clusters to Git state Detect and remediate drift Report application health Kargo Responsibilities:\nTrack artifact versions across repositories Create Freight bundles of compatible versions Orchestrate promotion workflows Update Git repositories with new versions Coordinate ArgoCD syncs Neither Tool Requires the Other:\nArgoCD works without Kargo (manual promotions) Kargo can work with other CD tools (though optimized for ArgoCD) State-Driven vs. Event-Driven#\rTraditional CI/CD: Event-driven (push events trigger deployments)\nKargo + ArgoCD: State-driven (pull-based, continuous reconciliation)\nBenefits:\nMore reliable (no lost events) Self-healing (automatically recovers from failures) Auditable (current state always in Git) Predictable (deterministic outcomes) Multi-Tenancy#\rKargo Projects:\nMap to Kubernetes namespaces Isolated with Kubernetes RBAC Can delegate authority via annotations Support for multiple projects per cluster ArgoCD Applications:\nCan be namespaced for isolation Controlled by ArgoCD RBAC Kargo respects ArgoCD boundaries Best Practices#\r1. Repository Structure#\rRecommended Approach:\nSeparate source code repositories from GitOps repositories Use GitOps repository exclusively for deployment manifests Structure GitOps repo with base configuration and environment-specific overlays Use Kustomize or Helm for configuration management 2. Freight Creation#\rGuidelines:\nUse automatic Freight creation with criteria for coordinated releases Use manual Freight assembly when compatibility is complex Implement gatekeeper Stages for filtering bad combinations Leverage Freight aliases for human-friendly identification 3. Promotion Workflows#\rRecommendations:\nStart with automatic promotions to dev/test Require manual approval for production Implement verification checks before allowing promotion Use stage-specific branches for clear separation Document promotion process in Promotion Tasks 4. Health and Verification#\rBest Practices:\nConfigure appropriate health checks on ArgoCD Applications Implement automated verification where possible Use Argo Rollouts for progressive delivery patterns Monitor Stage health continuously Set realistic timeout values for verifications 5. Security and Access Control#\rGuidelines:\nUse Kubernetes RBAC for access control Leverage annotation-based authorization for delegation Restrict who can approve production promotions Audit promotion history regularly Use sealed secrets or external secret management 6. External Tracking#\rRecommendations:\nImplement Git tags for major releases to production Create release manifest files for programmatic access Use Freight metadata for runtime tracking Send notifications to external systems for visibility Maintain both internal (Freight) and external (tags) tracking Common Pitfalls and Solutions#\rPitfall 1: Freight with Incompatible Versions#\rProblem: Warehouse creates Freight with frontend:v2.0 + backend:v1.5 that are incompatible\nSolutions:\nUse Freight creation criteria expressions Implement gatekeeper Stage Use manual Freight assembly Tag components with matching versions Pitfall 2: Feedback Loops#\rProblem: Warehouse triggers on its own promotion outputs, creating infinite loop\nSolution: Use path filters (includePaths/excludePaths) to ignore promotion output directories\nPitfall 3: Lost Freight Context#\rProblem: Can\u0026rsquo;t determine which versions are in production outside Kargo\nSolutions:\nCreate Git tags during production promotions Generate release manifest files Send notifications to external tracking systems Use Freight metadata Pitfall 4: Slow Artifact Discovery#\rProblem: Warehouses take too long to discover new artifacts\nSolutions:\nConfigure webhooks for immediate discovery Adjust polling intervals Optimize image selection strategies Use discovery limits to reduce processing Pitfall 5: ArgoCD Application Not Syncing#\rProblem: ArgoCD doesn\u0026rsquo;t sync after Kargo promotion\nCommon Causes:\nMissing authorization annotation Incorrect targetRevision format ArgoCD sync policy prevents auto-sync Application health checks failing Solutions:\nVerify authorization annotation Check ArgoCD Application logs Ensure sync policy allows syncs Review Application health status Appendices#\rAppendix A: Complete Warehouse Examples#\rA.1 Single Image Subscription#\rapiVersion: kargo.akuity.io/v1alpha1 kind: Warehouse metadata: name: nginx-warehouse namespace: my-project spec: subscriptions: - image: repoURL: public.ecr.aws/nginx/nginx semverConstraint: ^1.26.0 discoveryLimit: 5Explanation:\nMonitors nginx image repository Only considers versions matching ^1.26.0 (1.26.x) Keeps last 5 discovered versions available Creates Freight automatically when new versions discovered A.2 Multi-Repository Subscription with Criteria#\rapiVersion: kargo.akuity.io/v1alpha1 kind: Warehouse metadata: name: coordinated-release namespace: my-project spec: freightCreationPolicy: Automatic freightCreationCriteria: expression: | imageFrom(\u0026#39;myapp/frontend\u0026#39;).Tag == imageFrom(\u0026#39;myapp/backend\u0026#39;).Tag \u0026amp;\u0026amp; imageFrom(\u0026#39;myapp/backend\u0026#39;).Tag == chartFrom(\u0026#39;my-chart\u0026#39;).Version subscriptions: - image: repoURL: myapp/frontend semverConstraint: ^2.0.0 - image: repoURL: myapp/backend semverConstraint: ^2.0.0 - chart: repoURL: https://charts.example.com name: my-chart semverConstraint: ^2.0.0Explanation:\nSubscribes to two images and one Helm chart Only creates Freight when all three have matching version tags Prevents incompatible combinations Requires coordinated release versioning A.3 Manual Freight Assembly Configuration#\rapiVersion: kargo.akuity.io/v1alpha1 kind: Warehouse metadata: name: manual-assembly namespace: my-project spec: freightCreationPolicy: Manual subscriptions: - image: repoURL: myapp/frontend discoveryLimit: 10 - image: repoURL: myapp/backend discoveryLimit: 10 - git: repoURL: https://github.com/myorg/config.git commitSelectionStrategy: SemVerExplanation:\nDiscovers artifacts but doesn\u0026rsquo;t auto-create Freight Keeps 10 versions of each image available User manually selects compatible combination via UI/CLI Ideal when compatibility rules are complex A.4 Git Repository with Path Filters#\rapiVersion: kargo.akuity.io/v1alpha1 kind: Warehouse metadata: name: monorepo-app namespace: my-project spec: subscriptions: - git: repoURL: https://github.com/myorg/monorepo.git commitSelectionStrategy: NewestFromBranch branch: main includePaths: - apps/my-service excludePaths: - apps/my-service/README.md - apps/my-service/docsExplanation:\nMonitors only specific paths in monorepo Creates Freight only when changes affect my-service Excludes documentation changes Prevents unnecessary Freight creation Appendix B: Stage Configuration Examples#\rB.1 Development Stage with Auto-Promotion#\rapiVersion: kargo.akuity.io/v1alpha1 kind: Stage metadata: name: dev namespace: my-project spec: # Subscribe to Warehouse requestedFreight: - origin: kind: Warehouse name: my-warehouse sources: direct: true # Auto-promote new Freight promotionTemplate: spec: vars: - name: gitopsRepo value: https://github.com/myorg/gitops.git - name: imageRepo value: myapp/service steps: # Clone GitOps repository - uses: git-clone config: repoURL: ${{ vars.gitopsRepo }} checkout: - branch: main path: ./src - branch: stage/dev create: true path: ./out # Clear output directory - uses: git-clear config: path: ./out # Update image in Kustomize - uses: kustomize-set-image as: update-image config: path: ./src/overlays/dev images: - image: ${{ vars.imageRepo }} tag: ${{ imageFrom(vars.imageRepo).Tag }} # Build Kustomize manifests - uses: kustomize-build config: path: ./src/overlays/dev outPath: ./out # Commit changes - uses: git-commit as: commit config: path: ./out message: | Updated dev to ${{ imageFrom(vars.imageRepo).Tag }} Image: ${{ vars.imageRepo }}:${{ imageFrom(vars.imageRepo).Tag }} Digest: ${{ imageFrom(vars.imageRepo).Digest }} Freight: ${{ ctx.freight.name }} # Push to stage branch - uses: git-push config: path: ./out # Update ArgoCD Application - uses: argocd-update config: apps: - name: myapp-dev namespace: argocd sources: - repoURL: ${{ vars.gitopsRepo }} desiredRevision: ${{ outputs.commit.commit }}Explanation:\nAutomatically promotes new Freight from Warehouse Clones GitOps repo to two paths (source and output) Updates image tag in Kustomize overlay Builds manifests and commits to stage/dev branch Triggers ArgoCD sync to new commit B.2 Production Stage with Manual Approval#\rapiVersion: kargo.akuity.io/v1alpha1 kind: Stage metadata: name: production namespace: my-project spec: # Subscribe to upstream staging Stage requestedFreight: - origin: kind: Warehouse name: my-warehouse sources: stages: - staging # Require manual promotion (no auto-promotion) promotionTemplate: spec: vars: - name: gitopsRepo value: https://github.com/myorg/gitops.git - name: releaseTag value: release-${{ timestamp(\u0026#34;2006.01.02-150405\u0026#34;) }} steps: # Standard promotion steps - uses: git-clone config: repoURL: ${{ vars.gitopsRepo }} checkout: - branch: main path: ./src - branch: stage/production create: true path: ./out - uses: git-clear config: path: ./out - uses: kustomize-set-image config: path: ./src/overlays/production images: - image: myapp/frontend tag: ${{ imageFrom(\u0026#39;myapp/frontend\u0026#39;).Tag }} - image: myapp/backend tag: ${{ imageFrom(\u0026#39;myapp/backend\u0026#39;).Tag }} - uses: kustomize-build config: path: ./src/overlays/production outPath: ./out - uses: git-commit as: commit config: path: ./out message: | Production Release ${{ vars.releaseTag }} Freight: ${{ ctx.freight.alias }} Frontend: ${{ imageFrom(\u0026#39;myapp/frontend\u0026#39;).Tag }} Backend: ${{ imageFrom(\u0026#39;myapp/backend\u0026#39;).Tag }} - uses: git-push config: path: ./out # Create release tag - uses: bash config: script: | #!/bin/bash cd ./out git tag -a \u0026#34;${{ vars.releaseTag }}\u0026#34; -m \u0026#34;Production Release Freight: ${{ ctx.freight.name }} Promoted: $(date -u +%Y-%m-%dT%H:%M:%SZ) Components: - Frontend: ${{ imageFrom(\u0026#39;myapp/frontend\u0026#39;).Tag }} - Backend: ${{ imageFrom(\u0026#39;myapp/backend\u0026#39;).Tag }}\u0026#34; git push origin \u0026#34;${{ vars.releaseTag }}\u0026#34; - uses: argocd-update config: apps: - name: myapp-production namespace: argocdExplanation:\nOnly promotes Freight verified in staging Requires manual trigger (no auto-promotion) Updates multiple images Creates Git tag for release tracking Provides external visibility via tags B.3 Stage Using Multiple Warehouses#\rapiVersion: kargo.akuity.io/v1alpha1 kind: Stage metadata: name: staging namespace: my-project spec: # Request Freight from two different Warehouses requestedFreight: - origin: kind: Warehouse name: app-images sources: direct: true - origin: kind: Warehouse name: infrastructure-config sources: direct: true promotionTemplate: spec: vars: - name: gitopsRepo value: https://github.com/myorg/gitops.git steps: - uses: git-clone config: repoURL: ${{ vars.gitopsRepo }} checkout: # Checkout from both Warehouses\u0026#39; subscribed repos - commit: ${{ commitFrom(\u0026#39;github.com/myorg/helm-charts\u0026#39;).ID }} path: ./charts - branch: stage/staging create: true path: ./out - uses: git-clear config: path: ./out # Update with images from first Warehouse - uses: helm-update-image config: path: ./charts/values.yaml images: - image: myapp/frontend key: frontend.image.tag value: ${{ imageFrom(\u0026#39;myapp/frontend\u0026#39;).Tag }} - image: myapp/backend key: backend.image.tag value: ${{ imageFrom(\u0026#39;myapp/backend\u0026#39;).Tag }} # Template Helm chart - uses: helm-template config: path: ./charts outPath: ./out releaseName: myapp-staging - uses: git-commit config: path: ./out message: \u0026#34;Updated staging\u0026#34; - uses: git-push config: path: ./out - uses: argocd-update config: apps: - name: myapp-stagingExplanation:\nCombines Freight from two Warehouses One Warehouse tracks images (fast-moving) Other Warehouse tracks infrastructure config (slow-moving) Allows independent promotion cadences Appendix C: ArgoCD Application Examples#\rC.1 Basic Application with Kargo Authorization#\rapiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: myapp-dev namespace: argocd annotations: # Authorize Kargo Stage to manage this Application kargo.akuity.io/authorized-stage: my-project:dev spec: project: default source: repoURL: https://github.com/myorg/gitops.git targetRevision: stage/dev path: . destination: server: https://kubernetes.default.svc namespace: myapp-dev syncPolicy: automated: prune: true selfHeal: true syncOptions: - CreateNamespace=trueExplanation:\nAnnotation grants dev Stage permission to update Application Points to stage/dev branch (updated by Kargo) Auto-sync enabled so ArgoCD syncs automatically Creates namespace if it doesn\u0026rsquo;t exist C.2 ApplicationSet for Multiple Stages#\rapiVersion: argoproj.io/v1alpha1 kind: ApplicationSet metadata: name: myapp-all-stages namespace: argocd spec: generators: - list: elements: - stage: dev - stage: staging - stage: production template: metadata: name: myapp-{{stage}} annotations: # Dynamic authorization per stage kargo.akuity.io/authorized-stage: my-project:{{stage}} spec: project: default source: repoURL: https://github.com/myorg/gitops.git targetRevision: stage/{{stage}} path: . destination: server: https://kubernetes.default.svc namespace: myapp-{{stage}} syncPolicy: syncOptions: - CreateNamespace=trueExplanation:\nCreates three Applications (dev, staging, production) Each authorized to its corresponding Kargo Stage Each points to its own stage-specific branch Reduces duplication in Application definitions C.3 Multi-Source Application#\rapiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: myapp-production namespace: argocd annotations: kargo.akuity.io/authorized-stage: my-project:production spec: project: default sources: # Helm chart from chart repository - repoURL: https://charts.example.com chart: myapp targetRevision: 2.1.0 helm: valueFiles: - $values/overlays/production/values.yaml # Values from Git repository - repoURL: https://github.com/myorg/gitops.git targetRevision: stage/production ref: values destination: server: https://kubernetes.default.svc namespace: myapp-production syncPolicy: automated: prune: true selfHeal: trueExplanation:\nUses Helm chart from chart repository Overlays values from GitOps repository Kargo updates targetRevision of values source Allows separating chart version from configuration Appendix D: Promotion Task Examples#\rD.1 Reusable Git Update Task#\rapiVersion: kargo.akuity.io/v1alpha1 kind: PromotionTask metadata: name: standard-git-update namespace: my-project spec: vars: - name: gitopsRepo # Required variable, must be provided when task is used - name: stagePath # Required variable for path to stage overlay - name: imageRepo # Required variable for image repository steps: - uses: git-clone config: repoURL: ${{ vars.gitopsRepo }} checkout: - branch: main path: ./src - branch: stage/${{ ctx.stage }} create: true path: ./out - uses: git-clear config: path: ./out - uses: kustomize-set-image as: update-image config: path: ./src/${{ vars.stagePath }} images: - image: ${{ vars.imageRepo }} tag: ${{ imageFrom(vars.imageRepo).Tag }} - uses: kustomize-build config: path: ./src/${{ vars.stagePath }} outPath: ./out - uses: git-commit as: commit config: path: ./out messageFromSteps: - update-image - uses: git-push config: path: ./out - uses: compose-output as: output config: commit: ${{ task.outputs.commit.commit }}Explanation:\nDefines reusable promotion workflow Accepts variables for customization Outputs commit SHA for use in Stage Can be referenced by multiple Stages D.2 Using Promotion Task in Stage#\rapiVersion: kargo.akuity.io/v1alpha1 kind: Stage metadata: name: dev namespace: my-project spec: requestedFreight: - origin: kind: Warehouse name: my-warehouse sources: direct: true promotionTemplate: spec: steps: # Reference the PromotionTask - task: name: standard-git-update vars: - name: gitopsRepo value: https://github.com/myorg/gitops.git - name: stagePath value: overlays/dev - name: imageRepo value: myapp/service # Use output from task - uses: argocd-update config: apps: - name: myapp-dev sources: - repoURL: https://github.com/myorg/gitops.git desiredRevision: ${{ outputs[\u0026#39;standard-git-update\u0026#39;].commit }}Explanation:\nReferences PromotionTask by name Provides required variables Accesses task outputs in subsequent steps Enables code reuse across Stages D.3 Pull Request Workflow Task#\rapiVersion: kargo.akuity.io/v1alpha1 kind: ClusterPromotionTask metadata: name: pr-workflow spec: vars: - name: gitopsRepo - name: prTitle default: \u0026#34;Kargo Promotion\u0026#34; - name: prBody default: \u0026#34;Automated promotion from Kargo\u0026#34; steps: # Standard git operations - uses: git-clone config: repoURL: ${{ vars.gitopsRepo }} checkout: - branch: main path: ./repo # Make changes to manifests # ... (update steps omitted for brevity) - uses: git-commit config: path: ./repo message: ${{ vars.prTitle }} # Push to feature branch - uses: git-push as: push config: path: ./repo generateTargetBranch: true # Open pull request - uses: git-open-pr as: open-pr config: repoURL: ${{ vars.gitopsRepo }} sourceBranch: ${{ outputs.push.branch }} targetBranch: main title: ${{ vars.prTitle }} description: | ${{ vars.prBody }} Freight: ${{ ctx.freight.alias }} Stage: ${{ ctx.stage }} # Wait for PR to be merged - uses: git-wait-for-pr config: repoURL: ${{ vars.gitopsRepo }} prNumber: ${{ outputs[\u0026#39;open-pr\u0026#39;].pr.id }} # Merge PR automatically (optional) - uses: git-merge-pr config: repoURL: ${{ vars.gitopsRepo }} prNumber: ${{ outputs[\u0026#39;open-pr\u0026#39;].pr.id }} wait: true - uses: compose-output as: output config: prNumber: ${{ outputs[\u0026#39;open-pr\u0026#39;].pr.id }} merged: trueExplanation:\nOpens pull request instead of direct push Waits for PR approval/checks Optionally auto-merges after checks pass Provides audit trail through PR system Cluster-scoped so available to all Projects Appendix E: Advanced Configuration Examples#\rE.1 Freight Clone for Hotfix#\rUsing Freight cloning capability to create hotfix Freight:\n# Production has Freight with these artifacts: # - frontend:v2.0.0 # - backend:v2.0.0 # - database:v1.5.0 # Hotfix scenario: Critical bug in backend:v2.0.0 # Build and push backend:v2.0.1 # Clone production Freight via UI/CLI: # 1. Select production Freight # 2. Click \u0026#34;Clone Freight\u0026#34; # 3. Select backend artifact # 4. Change version to v2.0.1 # 5. Create new Freight # Result: New Freight with: # - frontend:v2.0.0 (unchanged) # - backend:v2.0.1 (hotfix) # - database:v1.5.0 (unchanged) # Promote hotfix Freight directly to production # (bypassing dev/staging for emergency)Use Case:\nCritical production bug requiring immediate fix Don\u0026rsquo;t want to wait for full pipeline Keep all other components at production versions Skip lower environments for urgency E.2 Release Tagging with Complete Manifest#\rapiVersion: kargo.akuity.io/v1alpha1 kind: Stage metadata: name: production namespace: my-project spec: promotionTemplate: spec: vars: - name: releaseVersion value: v${{ timestamp(\u0026#34;2006.01.02\u0026#34;) }}-${{ ctx.freight.alias }} steps: # ... standard promotion steps ... # Create comprehensive release tag - uses: bash config: script: | #!/bin/bash cd ./out # Create annotated tag with full component manifest git tag -a \u0026#34;${{ vars.releaseVersion }}\u0026#34; -m \u0026#34;Production Release ${{ vars.releaseVersion }} Kargo Freight: ${{ ctx.freight.name }} Freight Alias: ${{ ctx.freight.alias }} Promoted At: $(date -u +%Y-%m-%dT%H:%M:%SZ) Promoted By: ${{ ctx.promotion.createdBy }} Component Manifest: ================== Frontend: Image: myapp/frontend:${{ imageFrom(\u0026#39;myapp/frontend\u0026#39;).Tag }} Digest: ${{ imageFrom(\u0026#39;myapp/frontend\u0026#39;).Digest }} Backend: Image: myapp/backend:${{ imageFrom(\u0026#39;myapp/backend\u0026#39;).Tag }} Digest: ${{ imageFrom(\u0026#39;myapp/backend\u0026#39;).Digest }} Database Migrations: Image: myapp/migrations:${{ imageFrom(\u0026#39;myapp/migrations\u0026#39;).Tag }} Digest: ${{ imageFrom(\u0026#39;myapp/migrations\u0026#39;).Digest }} Helm Chart: Name: myapp Version: ${{ chartFrom(\u0026#39;myapp\u0026#39;).Version }} Configuration: Repository: github.com/myorg/config Commit: ${{ commitFrom(\u0026#39;github.com/myorg/config\u0026#39;).ID }} Commit Message: ${{ commitFrom(\u0026#39;github.com/myorg/config\u0026#39;).Message }} \u0026#34; git push origin \u0026#34;${{ vars.releaseVersion }}\u0026#34; # Create machine-readable release manifest - uses: bash config: script: | #!/bin/bash mkdir -p ./out/releases cat \u0026gt; ./out/releases/${{ vars.releaseVersion }}.json \u0026lt;\u0026lt;EOF { \u0026#34;version\u0026#34;: \u0026#34;${{ vars.releaseVersion }}\u0026#34;, \u0026#34;freight\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;${{ ctx.freight.name }}\u0026#34;, \u0026#34;alias\u0026#34;: \u0026#34;${{ ctx.freight.alias }}\u0026#34; }, \u0026#34;stage\u0026#34;: \u0026#34;${{ ctx.stage }}\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;$(date -u +%Y-%m-%dT%H:%M:%SZ)\u0026#34;, \u0026#34;promotedBy\u0026#34;: \u0026#34;${{ ctx.promotion.createdBy }}\u0026#34;, \u0026#34;components\u0026#34;: { \u0026#34;frontend\u0026#34;: { \u0026#34;image\u0026#34;: \u0026#34;myapp/frontend\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;${{ imageFrom(\u0026#39;myapp/frontend\u0026#39;).Tag }}\u0026#34;, \u0026#34;digest\u0026#34;: \u0026#34;${{ imageFrom(\u0026#39;myapp/frontend\u0026#39;).Digest }}\u0026#34; }, \u0026#34;backend\u0026#34;: { \u0026#34;image\u0026#34;: \u0026#34;myapp/backend\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;${{ imageFrom(\u0026#39;myapp/backend\u0026#39;).Tag }}\u0026#34;, \u0026#34;digest\u0026#34;: \u0026#34;${{ imageFrom(\u0026#39;myapp/backend\u0026#39;).Digest }}\u0026#34; }, \u0026#34;migrations\u0026#34;: { \u0026#34;image\u0026#34;: \u0026#34;myapp/migrations\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;${{ imageFrom(\u0026#39;myapp/migrations\u0026#39;).Tag }}\u0026#34;, \u0026#34;digest\u0026#34;: \u0026#34;${{ imageFrom(\u0026#39;myapp/migrations\u0026#39;).Digest }}\u0026#34; }, \u0026#34;chart\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;myapp\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;${{ chartFrom(\u0026#39;myapp\u0026#39;).Version }}\u0026#34; }, \u0026#34;config\u0026#34;: { \u0026#34;repository\u0026#34;: \u0026#34;github.com/myorg/config\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;${{ commitFrom(\u0026#39;github.com/myorg/config\u0026#39;).ID }}\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;${{ commitFrom(\u0026#39;github.com/myorg/config\u0026#39;).Message }}\u0026#34; } } } EOF git add releases/${{ vars.releaseVersion }}.json git commit -m \u0026#34;Add release manifest for ${{ vars.releaseVersion }}\u0026#34; git pushBenefits:\nGit tag provides human-readable release notes JSON manifest enables programmatic access External tools can query releases without Kargo Complete audit trail of what was deployed Immutable record in Git history E.3 Notification Integration#\rapiVersion: kargo.akuity.io/v1alpha1 kind: Stage metadata: name: production namespace: my-project spec: promotionTemplate: spec: steps: # ... standard promotion steps ... # Send Slack notification - uses: send-message config: destination: slack channel: \u0026#34;#releases\u0026#34; message: | üöÄ **Production Release Deployed** **Release:** ${{ vars.releaseVersion }} **Freight:** ${{ ctx.freight.alias }} **Promoted By:** ${{ ctx.promotion.createdBy }} **Components:** ‚Ä¢ Frontend: `${{ imageFrom(\u0026#39;myapp/frontend\u0026#39;).Tag }}` ‚Ä¢ Backend: `${{ imageFrom(\u0026#39;myapp/backend\u0026#39;).Tag }}` ‚Ä¢ Chart: `${{ chartFrom(\u0026#39;myapp\u0026#39;).Version }}` **Links:** ‚Ä¢ [ArgoCD App](https://argocd.example.com/applications/myapp-production) ‚Ä¢ [Git Tag](https://github.com/myorg/gitops/releases/tag/${{ vars.releaseVersion }}) # Send to external tracking system - uses: bash config: script: | #!/bin/bash curl -X POST https://release-tracker.example.com/api/releases \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -H \u0026#34;Authorization: Bearer ${RELEASE_TRACKER_TOKEN}\u0026#34; \\ -d \u0026#39;{ \u0026#34;version\u0026#34;: \u0026#34;${{ vars.releaseVersion }}\u0026#34;, \u0026#34;environment\u0026#34;: \u0026#34;production\u0026#34;, \u0026#34;freight\u0026#34;: \u0026#34;${{ ctx.freight.name }}\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;\u0026#39;$(date -u +%Y-%m-%dT%H:%M:%SZ)\u0026#39;\u0026#34;, \u0026#34;components\u0026#34;: { \u0026#34;frontend\u0026#34;: \u0026#34;${{ imageFrom(\u0026#39;myapp/frontend\u0026#39;).Tag }}\u0026#34;, \u0026#34;backend\u0026#34;: \u0026#34;${{ imageFrom(\u0026#39;myapp/backend\u0026#39;).Tag }}\u0026#34; } }\u0026#39;Use Cases:\nTeam awareness via Slack/Teams Integration with release management tools Triggering downstream automation Compliance reporting Appendix F: Troubleshooting Guide#\rF.1 Freight Not Being Created#\rSymptoms:\nWarehouse shows no Freight New artifact versions not detected Diagnostic Steps:\nCheck Warehouse status: kubectl get warehouse -n my-project my-warehouse -o yaml Look for errors in status: status: conditions: - type: Ready status: \u0026#34;False\u0026#34; reason: DiscoveryFailed message: \u0026#34;Failed to list images: unauthorized\u0026#34;Common Causes:\nMissing Credentials:\n# Create secret for private registry apiVersion: v1 kind: Secret metadata: name: registry-creds namespace: my-project labels: kargo.akuity.io/cred-type: image stringData: repoURL: ghcr.io username: myuser password: ghp_xxxxxxxxxxxxxIncorrect semver Constraint:\n# Wrong: No versions match constraint semverConstraint: ^3.0.0 # But only v2.x.x versions exist # Fix: Adjust constraint to match actual versions semverConstraint: ^2.0.0Freight Creation Criteria Not Met:\n# Check if expression prevents creation freightCreationCriteria: expression: | imageFrom(\u0026#39;frontend\u0026#39;).Tag == imageFrom(\u0026#39;backend\u0026#39;).Tag # If tags don\u0026#39;t match, no Freight createdF.2 Promotion Failing#\rSymptoms:\nPromotion stuck in Running state Promotion shows Failed status Diagnostic Steps:\nGet Promotion details: kubectl get promotion -n my-project \u0026lt;promotion-name\u0026gt; -o yaml Check promotion status for errors: status: phase: Failed error: \u0026#34;git-push step failed: authentication required\u0026#34; stepResults: - name: git-clone status: Succeeded - name: git-commit status: Succeeded - name: git-push status: Failed error: \u0026#34;remote: Permission denied\u0026#34;Common Causes:\nMissing Git Credentials:\n# Create Git credentials secret apiVersion: v1 kind: Secret metadata: name: git-creds namespace: my-project labels: kargo.akuity.io/cred-type: git stringData: repoURL: https://github.com/myorg/gitops.git username: myuser password: ghp_xxxxxxxxxxxxx # GitHub PATArgoCD Application Not Authorized:\n# Application missing annotation apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: myapp-dev annotations: # ADD THIS: kargo.akuity.io/authorized-stage: my-project:devIncorrect Expression Syntax:\n# Wrong: value: ${{ imageFrom(vars.imageRepo).Tag }} # Right: value: ${{ imageFrom(vars.imageRepo).Tag }}F.3 ArgoCD Application Not Syncing#\rSymptoms:\nPromotion succeeds but Application stays OutOfSync Application shows old revision Diagnostic Steps:\nCheck Application status: kubectl get application -n argocd myapp-dev -o yaml Check sync status: status: sync: status: OutOfSync revision: old-commit-sha operationState: phase: Failed message: \u0026#34;ComparisonError: failed to get app details\u0026#34;Common Causes:\nSync Policy Prevents Auto-Sync:\n# Application configured for manual sync spec: syncPolicy: {} # No automated sync! # Fix: Enable auto-sync spec: syncPolicy: automated: prune: true selfHeal: trueTarget Revision Mismatch:\n# Kargo pushed to stage/dev branch # But Application points elsewhere spec: source: targetRevision: main # Should be stage/dev! # Kargo argocd-update should set this correctlyArgoCD Can\u0026rsquo;t Access Repository:\n# Check ArgoCD repository connection kubectl get secret -n argocd | grep repo # Add repository to ArgoCD if missing argocd repo add https://github.com/myorg/gitops.git \\ --username myuser \\ --password ghp_xxxxxxxxxxxxxF.4 Stage Health Reporting Degraded#\rSymptoms:\nStage shows Unhealthy status Promotion succeeded but Stage not healthy Diagnostic Steps:\nCheck Stage status: kubectl get stage -n my-project dev -o yaml Review health status: status: health: status: Unhealthy issues: - \u0026#34;Application myapp-dev is Degraded\u0026#34; currentFreight: name: abc123 verificationHistory: - phase: FailedCommon Causes:\nApplication Truly Unhealthy:\n# Check ArgoCD Application kubectl get application -n argocd myapp-dev # Check pods in application namespace kubectl get pods -n myapp-dev # Common: ImagePullBackOff, CrashLoopBackOffVerification Failing:\n# Check verification configuration spec: verification: analysisTemplates: - name: smoke-tests # If analysis fails, Stage becomes unhealthy # Check AnalysisRun status kubectl get analysisrun -n my-projectHealth Check Timeout:\n# Increase timeout in promotion step steps: - uses: argocd-update config: apps: - name: myapp-dev timeout: 10m # Default might be too shortAppendix G: Expression Reference#\rKargo uses expressions extensively in promotion templates. Here\u0026rsquo;s a quick reference:\nG.1 Context Variables#\r# Current Stage name ${{ ctx.stage }} # \u0026#34;dev\u0026#34;, \u0026#34;production\u0026#34;, etc. # Current Freight ${{ ctx.freight.name }} # SHA-1 hash identifier ${{ ctx.freight.alias }} # Human-friendly alias # Current Promotion ${{ ctx.promotion.name }} # Promotion resource name ${{ ctx.promotion.createdBy }} # Who triggered promotionG.2 Artifact Functions#\r# Image from Freight ${{ imageFrom(\u0026#39;myapp/frontend\u0026#39;).Tag }} # \u0026#34;v2.1.0\u0026#34; ${{ imageFrom(\u0026#39;myapp/frontend\u0026#39;).Digest }} # \u0026#34;sha256:abc...\u0026#34; # Git commit from Freight ${{ commitFrom(\u0026#39;github.com/myorg/repo\u0026#39;).ID }} # Commit SHA ${{ commitFrom(\u0026#39;github.com/myorg/repo\u0026#39;).Message }} # Commit message # Helm chart from Freight ${{ chartFrom(\u0026#39;my-chart\u0026#39;).Version }} # \u0026#34;1.2.3\u0026#34; # With Warehouse filter ${{ imageFrom(\u0026#39;myapp/frontend\u0026#39;, warehouse(\u0026#39;my-warehouse\u0026#39;)).Tag }}G.3 Variable References#\r# Global variables ${{ vars.gitopsRepo }} ${{ vars.imageRepo }} # Step outputs ${{ outputs.commit.commit }} # From step with alias \u0026#34;commit\u0026#34; ${{ outputs[\u0026#39;update-image\u0026#39;].message }} # Square bracket notation # Task outputs (in PromotionTask) ${{ task.outputs[\u0026#39;step-name\u0026#39;].value }}G.4 Utility Functions#\r# Timestamp ${{ timestamp() }} # Unix timestamp ${{ timestamp(\u0026#34;2006.01.02\u0026#34;) }} # Formatted: 2025.01.15 ${{ timestamp(\u0026#34;2006-01-02T15:04:05\u0026#34;) }} # ISO format # Quote for YAML safety ${{ quote(imageFrom(\u0026#39;myapp/frontend\u0026#39;).Tag) }}G.5 Freight Creation Criteria Examples#\r# Tags must match imageFrom(\u0026#39;frontend\u0026#39;).Tag == imageFrom(\u0026#39;backend\u0026#39;).Tag # Version greater than imageFrom(\u0026#39;myapp\u0026#39;).Tag \u0026gt; \u0026#39;2.0.0\u0026#39; # Commit after date commitFrom(\u0026#39;github.com/myorg/repo\u0026#39;).CommitterDate.After(date(\u0026#39;2024-01-01\u0026#39;)) # Multiple conditions imageFrom(\u0026#39;frontend\u0026#39;).Tag == imageFrom(\u0026#39;backend\u0026#39;).Tag \u0026amp;\u0026amp; imageFrom(\u0026#39;backend\u0026#39;).Tag == chartFrom(\u0026#39;my-chart\u0026#39;).Version # Tag doesn\u0026#39;t contain alpha/beta !(imageFrom(\u0026#39;myapp\u0026#39;).Tag contains \u0026#39;alpha\u0026#39;) \u0026amp;\u0026amp; !(imageFrom(\u0026#39;myapp\u0026#39;).Tag contains \u0026#39;beta\u0026#39;) Conclusion#\rKargo and ArgoCD together provide a complete GitOps solution for modern Kubernetes deployments. ArgoCD handles the continuous deployment of applications to clusters, while Kargo orchestrates the progressive promotion of changes through multiple stages. By maintaining strict GitOps principles, providing comprehensive audit trails, and enabling flexible promotion workflows, these tools empower teams to deploy confidently and reliably at scale.\nThe key to success is understanding the separation of concerns: ArgoCD syncs clusters to Git state, while Kargo orchestrates what gets promoted when. Together, they eliminate manual deployment processes, reduce errors, and provide complete visibility into your application lifecycle.\nAdditional Resources#\rKargo Documentation:\nOfficial Docs: https://docs.kargo.io GitHub Repository: https://github.com/akuity/kargo Quickstart Guide: https://docs.kargo.io/quickstart ArgoCD Documentation:\nOfficial Docs: https://argo-cd.readthedocs.io GitHub Repository: https://github.com/argoproj/argo-cd Getting Started: https://argo-cd.readthedocs.io/en/stable/getting_started Community:\nKargo Discord: https://discord.gg/kargo CNCF Slack #argo-cd channel Argo Project Community: https://argoproj.github.io/community/ Related Tools:\nArgo Rollouts: Progressive delivery strategies Argo Workflows: Workflow orchestration Argo Events: Event-driven automation "},{"id":27,"href":"/musings/docs/misc/kargo-argocd-interaction/","title":"Kargo promotion step","section":"Miscellaneous","content":"Perfect! Now I have comprehensive information. Let me provide a detailed answer about how Kargo interacts with ArgoCD.\nHow Kargo Interacts with ArgoCD#\rShort Answer: Both. Kargo uses a hybrid approach - it primarily modifies ArgoCD Application Custom Resources directly via the Kubernetes API, but it also monitors Application health and sync status. The interaction is declarative and Kubernetes-native, not through the ArgoCD REST API.\nThe Integration Architecture#\r1. Primary Interaction: Kubernetes Custom Resources#\rKargo updates Argo CD Application resources through the use of the argocd-update promotion step, which often entails little more than modifying an Application\u0026rsquo;s operation field to force the Application to be synced to recently updated desired state.\nKey Point: Kargo treats ArgoCD Applications as Kubernetes resources and modifies them using standard Kubernetes API operations (patch, update) - NOT through ArgoCD\u0026rsquo;s REST API.\n2. Authorization Model: Annotations#\rFor an Argo CD Application resource to be managed by a Kargo Stage, the Application must have an annotation of the following form: kargo.akuity.io/authorized-stage: \u0026ldquo;\u0026lt;project-name\u0026gt;:\u0026lt;stage-name\u0026gt;\u0026rdquo;.\nExample:\napiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: kargo-demo-test namespace: argocd annotations: kargo.akuity.io/authorized-stage: kargo-demo:test # Authorization spec: project: default source: repoURL: https://github.com/myorg/gitops-repo.git targetRevision: stage/test path: . destination: server: https://kubernetes.default.svc namespace: kargo-demo-testReasoning: Because an annotation such as this could only be added to an Application by a user who, themselves, is authorized to update that Application, Kargo interprets the presence of such an annotation as delegation of that user\u0026rsquo;s authority to do so.\nHow the Integration Works#\rMethod 1: Direct Application Resource Modification#\rKargo uses the Kubernetes API to patch Application resources:\n# Kargo promotion step steps: - uses: git-commit as: commit config: path: ./out message: \u0026#34;Updated manifests\u0026#34; - uses: git-push config: path: ./out # This modifies the Application CR via Kubernetes API - uses: argocd-update config: apps: - name: my-app sources: - repoURL: https://github.com/example/repo.git desiredRevision: ${{ outputs.commit.commit }}What happens:\nKargo controller uses its Kubernetes client Patches the Application CR in the argocd namespace Updates fields like spec.source.targetRevision or operation field ArgoCD controller detects the change and syncs This is Kubernetes-native - no ArgoCD API calls!\nMethod 2: Health Monitoring via Application Status#\rWhen a Promotion uses an argocd-update step to update an Application, a health check is registered for the Stage that the Promotion is targeting, used to continuously monitor the health of the Application.\nKargo reads Application resources to check:\n.status.health.status (Healthy, Degraded, Progressing, etc.) .status.sync.status (Synced, OutOfSync, etc.) .status.operationState (Running, Succeeded, Failed) Example of what Kargo reads:\napiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: my-app status: health: status: Healthy # Kargo monitors this sync: status: Synced # Kargo monitors this operationState: finishedAt: \u0026#34;2024-01-15T10:30:00Z\u0026#34; phase: Succeeded # Kargo monitors this What Kargo Modifies in Applications#\rCommon Modifications:#\r1. Update targetRevision (Most Common)#\r# Before spec: source: targetRevision: stage/test # After Kargo promotion spec: source: targetRevision: abc123def # Git commit SHA from promotion2. Trigger Sync via Operation Field#\r# Kargo adds this to force a sync spec: operation: initiatedBy: username: kargo sync: revision: abc123def3. Update Helm Values (Non-GitOps Mode)#\rThis example simply updates a \u0026ldquo;live\u0026rdquo; Argo CD Application resource to point its targetRevision field at a specific version of a Helm chart, which Argo CD will pull directly from the chart repository.\nsteps: - uses: argocd-update config: apps: - name: my-app sources: - repoURL: https://github.com/example/repo.git helm: images: - key: image.tag value: ${{ imageFrom(\u0026#34;my/image\u0026#34;).Tag }}Warning: This is not \u0026ldquo;real GitOps\u0026rdquo; since the state of the Application resource is not backed up in a Git repository.\nRBAC \u0026amp; Permissions#\rKargo\u0026rsquo;s Permissions#\rKargo controllers have the requisite RBAC permissions to perform such updates, but being a multi-tenant system, Kargo must also understand, internally, when it is acceptable to utilize those broad permissions to update a specific Application resource on behalf of a given Stage.\nKargo needs these Kubernetes permissions:\napiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: kargo-controller rules: # Read/watch Application resources - apiGroups: [\u0026#34;argoproj.io\u0026#34;] resources: [\u0026#34;applications\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] # Update Application resources - apiGroups: [\u0026#34;argoproj.io\u0026#34;] resources: [\u0026#34;applications\u0026#34;] verbs: [\u0026#34;patch\u0026#34;, \u0026#34;update\u0026#34;] # Read Application status - apiGroups: [\u0026#34;argoproj.io\u0026#34;] resources: [\u0026#34;applications/status\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;] Configuration Options#\rArgoCD Integration Configuration#\rBy default, Kargo expects Argo CD to be installed to the argocd namespace, which is also the default namespace it will use for Application resources if a namespace is not specified in the argocd-update Promotion step.\nHelm values to configure integration:\ncontroller: argocd: # Enable/disable ArgoCD integration integrationEnabled: true # Default namespace for Applications namespace: argocd # Only watch Applications in the configured namespace watchArgocdNamespaceOnly: trueDisabling Integration: When disabled, the controller will not watch Argo CD Application resources and disable Argo CD specific features.\nTypical Workflow: Step-by-Step#\rLet me show you exactly what happens:\nSetup Phase#\rUser creates ArgoCD Application with annotation: apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: myapp-production namespace: argocd annotations: kargo.akuity.io/authorized-stage: myproject:production spec: source: repoURL: https://github.com/myorg/gitops.git targetRevision: stage/production # Points to stage branch path: manifests destination: server: https://kubernetes.default.svc namespace: production User creates Kargo Stage: apiVersion: kargo.akuity.io/v1alpha1 kind: Stage metadata: name: production namespace: myproject spec: promotionTemplate: spec: steps: # ... git operations ... - uses: argocd-update config: apps: - name: myapp-production namespace: argocd # Optional, defaults to \u0026#39;argocd\u0026#39;Promotion Phase#\rWhat actually happens during promotion:\n1. User triggers promotion (or auto-promotion occurs)\r‚Üì\r2. Kargo executes promotion steps:\ra. git-clone: Clone GitOps repo\rb. yaml-update: Update image tags\rc. git-commit: Commit changes (outputs: commit SHA)\rd. git-push: Push to stage/production branch\r‚Üì\r3. argocd-update step executes:\r‚Üì\r4. Kargo controller makes Kubernetes API call:\rPATCH /apis/argoproj.io/v1alpha1/namespaces/argocd/applications/myapp-production\r{\r\u0026#34;spec\u0026#34;: {\r\u0026#34;source\u0026#34;: {\r\u0026#34;targetRevision\u0026#34;: \u0026#34;abc123def456\u0026#34; # New commit SHA\r}\r},\r\u0026#34;operation\u0026#34;: {\r\u0026#34;initiatedBy\u0026#34;: {\u0026#34;username\u0026#34;: \u0026#34;kargo\u0026#34;},\r\u0026#34;sync\u0026#34;: {\u0026#34;revision\u0026#34;: \u0026#34;abc123def456\u0026#34;}\r}\r}\r‚Üì\r5. ArgoCD controller detects Application change\r‚Üì\r6. ArgoCD syncs Application to new revision\r‚Üì\r7. ArgoCD updates Application status:\r{\r\u0026#34;status\u0026#34;: {\r\u0026#34;sync\u0026#34;: {\u0026#34;status\u0026#34;: \u0026#34;Synced\u0026#34;},\r\u0026#34;health\u0026#34;: {\u0026#34;status\u0026#34;: \u0026#34;Progressing\u0026#34;}\r}\r}\r‚Üì\r8. Kargo watches Application status (via Kubernetes watch)\r‚Üì\r9. When health becomes \u0026#34;Healthy\u0026#34;:\r- Kargo marks Stage as healthy\r- Promotion succeeds\r‚Üì\r10. Freight can now be promoted to next Stage Key Differences: Kargo vs. ArgoCD API#\rAspect Kargo\u0026rsquo;s Approach If Using ArgoCD API Communication Kubernetes API (Application CRs) ArgoCD REST API Authentication Kubernetes RBAC + Annotations ArgoCD tokens/sessions Authorization Annotation-based delegation ArgoCD RBAC policies Resource Type Native Kubernetes resources HTTP REST calls Idempotency Kubernetes declarative model Must handle manually Watching Kubernetes watch API ArgoCD webhooks/polling Integration Seamless (same cluster) Network dependency Advanced: Health Check Registration#\rThe argocd-update step is unique among all other built-in promotion steps in that, on successful completion, it will register health checks to be performed upon the target Stage on an ongoing basis.\nWhat this means:\nKargo doesn\u0026rsquo;t just update the Application once It continuously monitors the Application\u0026rsquo;s health Stage health reflects Application health If Application becomes unhealthy, Stage becomes unhealthy Stage health factors: It is important to note that Stage health is not determined solely by the health of the Application(s) that the Stage is managing. The health of the Stage is determined by the health of all Application resources that the Stage is managing, as well as any other indicators of health that are part of the Stage\u0026rsquo;s definition.\nNon-GitOps Mode (Less Common)#\rYou can also update Applications without modifying Git:\nsteps: # No git operations! - uses: argocd-update config: apps: - name: my-app sources: - repoURL: https://charts.example.com chart: my-chart targetRevision: 1.2.3 # Helm chart versionThis directly modifies the Application CR to point to a new Helm chart version, bypassing Git entirely.\nSummary#\rHow Kargo Interacts with ArgoCD:#\r‚úÖ Custom Resources (Primary):\nKargo patches Application CRs via Kubernetes API Modifies spec.source.targetRevision, spec.operation, etc. Uses standard Kubernetes client libraries ‚úÖ Status Monitoring (Secondary):\nKargo reads Application status via Kubernetes API Watches .status.health and .status.sync Registers continuous health checks ‚ùå NOT ArgoCD API:\nKargo does NOT call ArgoCD\u0026rsquo;s REST API Kargo does NOT use ArgoCD CLI Kargo does NOT need ArgoCD credentials Authorization Model:#\rUses annotation-based delegation Requires explicit kargo.akuity.io/authorized-stage annotation Respects Kubernetes RBAC Benefits of This Approach:#\rKubernetes-Native: Everything is a CR, managed declaratively GitOps Compliant: Applications themselves can be in Git No Network Dependency: Same cluster, same API server RBAC Integration: Uses standard Kubernetes permissions Loose Coupling: ArgoCD and Kargo remain independent Typical Pattern:#\rKargo promotes ‚Üí Updates Git ‚Üí Patches Application CR ‚Üí ArgoCD syncs ‚Üí Kargo monitors healthDoes this clarify the interaction model? Would you like me to dive deeper into any specific aspect, such as the exact Kubernetes API calls, RBAC configuration, or health monitoring implementation?\n"},{"id":28,"href":"/musings/docs/misc/presentation/","title":"Kargo: GitOps Promotion for ArgoCD","section":"Miscellaneous","content":"Kargo: GitOps Promotion for ArgoCD#\rSolving the Multi-Environment Deployment Gap\nWhat ArgoCD Doesn\u0026rsquo;t Do#\rArgoCD excels at continuous deployment - syncing Git state to Kubernetes clusters. However, there\u0026rsquo;s a critical gap:\nArgoCD syncs Git to Kubernetes Missing: Promotion workflow orchestration No dev ‚Üí staging ‚Üí prod concept No artifact version coordination Manual or scripted promotions required graph LR\rsubgraph Development\rD[ArgoCD]\rend\rsubgraph Staging\rS[ArgoCD]\rend\rsubgraph Production\rP[ArgoCD]\rend\rDevelopment --\u0026gt;|Manual?| Staging\rStaging --\u0026gt;|Manual?| Production\rstyle Development fill:#f5f5f5,stroke:#9e9e9e\rstyle Staging fill:#f5f5f5,stroke:#9e9e9e\rstyle Production fill:#f5f5f5,stroke:#9e9e9eThe Problem: Teams use manual processes, custom scripts, or CI/CD orchestration to promote between environments. This gap is precisely what Kargo addresses.\nKargo: Promotion Orchestration Layer#\rKargo and ArgoCD are complementary tools that work together:\nArgoCD: Deployment engine (how) Kargo: Promotion orchestrator (what, when) Declarative promotion workflows Multi-stage artifact coordination Built by Argo creators graph TB\rsubgraph Kargo Layer\rK[Promotion Logic]\rend\rsubgraph ArgoCD Layer\rA[Deployment Logic]\rend\rsubgraph Infrastructure\rI[Kubernetes Cluster]\rend\rK --\u0026gt;|Orchestrates| A\rA --\u0026gt;|Deploys to| I\rstyle Kargo Layer fill:#c8e6c9,stroke:#2e7d32\rstyle ArgoCD Layer fill:#bbdefb,stroke:#1976d2\rstyle Infrastructure fill:#e0e0e0,stroke:#757575Key Insight: Both tools remain independent and can function separately. Integration is Kubernetes-native through Custom Resources, not API calls.\nWarehouses: Artifact Discovery#\rWarehouses are the entry point of the Kargo workflow - the artifact discovery mechanism.\nMonitor repositories for new artifacts Images, Git commits, Helm charts Apply version constraints (semver) Automatic discovery with webhooks Produce Freight when versions found graph TD\rDocker[Docker Registry]\rGit[Git Repository]\rHelm[Helm Repository]\rW[Warehouse\u0026lt;br/\u0026gt;Artifact Discovery]\rF[Freight\u0026lt;br/\u0026gt;Bundle]\rDocker --\u0026gt;|Watches| W\rGit --\u0026gt;|Watches| W\rHelm --\u0026gt;|Watches| W\rW --\u0026gt;|Produces| F\rstyle W fill:#c8e6c9,stroke:#2e7d32\rstyle F fill:#bbdefb,stroke:#1976d2\rstyle Docker fill:#f5f5f5,stroke:#9e9e9e\rstyle Git fill:#f5f5f5,stroke:#9e9e9e\rstyle Helm fill:#f5f5f5,stroke:#9e9e9eHow it Works: Pull-based monitoring rather than push-based triggers (GitOps principle). Can subscribe to multiple repository types simultaneously. Warehouses don\u0026rsquo;t deploy - they discover and package into Freight.\nFreight: Immutable Artifact Bundles#\rCritical concept: Freight is NOT unified versioning across all components.\nBundle of specific artifact versions NOT a single version number Contains: digests, SHAs, chart versions Immutable and cryptographically identified Promoted as atomic unit graph TB\rsubgraph \u0026#34;Freight Bundle: abc123def456 (brave-eagle)\u0026#34;\rF1[frontend:v2.1.0\u0026lt;br/\u0026gt;@sha256:abc...]\rF2[backend:v3.0.1\u0026lt;br/\u0026gt;@sha256:def...]\rF3[chart: myapp:v0.5.2]\rend\rstyle F1 fill:#ffecb3,stroke:#ffa000\rstyle F2 fill:#c5e1a5,stroke:#689f38\rstyle F3 fill:#b39ddb,stroke:#673ab7Example YAML:\n# Freight bundle structure apiVersion: kargo.akuity.io/v1alpha1 kind: Freight metadata: name: abc123def456 # SHA-1 hash alias: brave-eagle # Human-friendly artifacts: images: - repoURL: myregistry.io/frontend tag: v2.1.0 digest: sha256:abc... - repoURL: myregistry.io/backend tag: v3.0.1 digest: sha256:def... charts: - name: myapp version: v0.5.2Key Point: Each artifact maintains its own version, bundled together as a tested combination. Immutability ensures reproducibility - Freight never changes after creation.\nStages: Promotion Targets#\rStages are promotion targets, though they typically map to environments.\nRepresent promotion targets (typically environments) Define Freight acceptance rules Specify promotion mechanisms Track health and verification Chain together to form pipelines graph LR\rFreight[Freight]\rDev[dev Stage]\rStaging[staging Stage]\rProd[prod Stage]\rFreight -.-\u0026gt;|flows through| Dev\rDev --\u0026gt;|Auto-promote| Staging\rStaging --\u0026gt;|Manual gate| Prod\rstyle Dev fill:#c8e6c9,stroke:#2e7d32\rstyle Staging fill:#fff9c4,stroke:#f9a825\rstyle Prod fill:#ffcdd2,stroke:#c62828\rstyle Freight fill:#bbdefb,stroke:#1976d2Configuration: Each Stage defines which Freight it accepts and how promotion executes. Promotion mechanisms are fully declarative via Kubernetes resources. Auto-promotion for lower environments, manual gates for production is the typical pattern.\nKubernetes-Native Integration#\rKey architectural decision: Kubernetes-native integration, not REST API calls.\nKargo patches Application Custom Resources No REST API calls Authorization via annotations Status monitoring via K8s watch Declarative, not imperative sequenceDiagram\rparticipant K as Kargo Controller\rparticipant CR as Application CR\rparticipant A as ArgoCD Controller\rK-\u0026gt;\u0026gt;CR: Patch CR (update spec)\rCR--\u0026gt;\u0026gt;K: Watch Status\rA--\u0026gt;\u0026gt;CR: Watch for changes\rA-\u0026gt;\u0026gt;K8s: Sync Cluster\rNote over K,CR: Via Kubernetes APIExample Authorization:\n# ArgoCD Application annotation apiVersion: argoproj.io/v1alpha1 kind: Application metadata: annotations: kargo.akuity.io/authorized-stage: \u0026#34;myproject:production\u0026#34;Design Benefits: Decoupled design with no tight coupling between systems. RBAC-based security model using standard Kubernetes mechanisms. ArgoCD sees standard CR modifications and doesn\u0026rsquo;t need Kargo awareness.\nEnd-to-End Promotion Flow#\rComplete picture of one promotion cycle from start to verification.\nWarehouse detects new artifact Freight created (immutable bundle) Promotion triggered (auto/manual) GitOps repo updated Application CR patched ArgoCD syncs cluster Health monitored Stage verified for next promotion graph TB\rW1[1. Warehouse\u0026lt;br/\u0026gt;detects artifact]\rW2[2. Freight\u0026lt;br/\u0026gt;created]\rW3[3. Promotion\u0026lt;br/\u0026gt;triggered]\rW4[4. GitOps Repo\u0026lt;br/\u0026gt;updated]\rW5[5. Application CR\u0026lt;br/\u0026gt;patched]\rW6[6. ArgoCD\u0026lt;br/\u0026gt;syncs cluster]\rW7[7. Health\u0026lt;br/\u0026gt;monitored]\rW8[8. Stage\u0026lt;br/\u0026gt;verified]\rW1 --\u0026gt; W2 --\u0026gt; W3 --\u0026gt; W4\rW4 --\u0026gt; W5 --\u0026gt; W6 --\u0026gt; W7 --\u0026gt; W8\rstyle W1 fill:#f5f5f5,stroke:#757575\rstyle W2 fill:#f5f5f5,stroke:#757575\rstyle W3 fill:#f5f5f5,stroke:#757575\rstyle W4 fill:#f5f5f5,stroke:#757575\rstyle W5 fill:#f5f5f5,stroke:#757575\rstyle W6 fill:#f5f5f5,stroke:#757575\rstyle W7 fill:#f5f5f5,stroke:#757575\rstyle W8 fill:#f5f5f5,stroke:#757575Workflow Characteristics: State-driven workflow, not event-driven - continuous reconciliation. Each step is declarative via Kubernetes resources. Failure at any step halts progression - safe by default. Entire workflow visible through kubectl and Kargo UI.\nCoordinating Multiple Artifact Sources#\rModern reality: polyrepo architectures are common.\nChallenge: Code, charts, config separate Single Warehouse subscribes to all Freight bundles specific version each Promoted together as unit Prevents version mismatches graph TB\rA[App Repo A\u0026lt;br/\u0026gt;Code]\rB[App Repo B\u0026lt;br/\u0026gt;Code]\rH[Helm Repo\u0026lt;br/\u0026gt;Charts]\rC[Config Repo\u0026lt;br/\u0026gt;Configuration]\rW[Warehouse\u0026lt;br/\u0026gt;Subscribes to all]\rF[Freight\u0026lt;br/\u0026gt;Bundle]\rA --\u0026gt; W\rB --\u0026gt; W\rH --\u0026gt; W\rC --\u0026gt; W\rW --\u0026gt;|Bundles\u0026lt;br/\u0026gt;versions| F\rstyle A fill:#f5f5f5,stroke:#9e9e9e\rstyle B fill:#f5f5f5,stroke:#9e9e9e\rstyle H fill:#f5f5f5,stroke:#9e9e9e\rstyle C fill:#f5f5f5,stroke:#9e9e9e\rstyle W fill:#c8e6c9,stroke:#2e7d32\rstyle F fill:#bbdefb,stroke:#1976d2Solution: Application code, infrastructure code, configuration often live in separate repositories. Kargo\u0026rsquo;s solution: Warehouse subscribes to multiple repositories simultaneously and Freight bundles one specific version from each. This is an alternative to unified versioning - independent artifact versions with coordinated promotion.\nPreventing Incompatible Combinations#\rReal-world problem: timing issues can create incompatible Freight.\nProblem: Race conditions create bad Freight Solution 1: Creation criteria (expressions) Solution 2: Manual assembly Solution 3: Multiple Warehouses Solution 4: Gatekeeper Stage Example Expression-Based Criteria:\n# Warehouse with Freight creation criteria apiVersion: kargo.akuity.io/v1alpha1 kind: Warehouse spec: freightCreationPolicy: Automatic freightCreationCriteria: expression: | imageFrom(\u0026#39;frontend\u0026#39;).Tag == imageFrom(\u0026#39;backend\u0026#39;).TagThe Problem: Frontend v2.0 discovered before backend v2.0 is ready, creates incompatible bundle.\nSolutions:\nSolution 1 (most powerful): Expression-based creation criteria Solution 2: Manual Freight assembly for complex compatibility rules Solution 3: Separate Warehouses for fast-moving vs slow-moving artifacts Solution 4: Gatekeeper Stage filters bad combinations before important environments Choose strategy based on coordination requirements and team workflow.\nWorking with Freight#\rFreight immutability is a feature, not a limitation - it ensures reproducibility.\nFreight is immutable (by design) Cannot merge Freight after creation Manual assembly: Select compatible versions Freight cloning: Template for hotfixes Multiple Warehouses: Parallel pipelines Strategies:\nStrategy Use Case Flexibility Manual Assembly Complex compatibility High Freight Cloning Hotfixes Medium Multiple Warehouses Different cadences High Design Context: Cannot merge existing Freight - would break audit trail and create untested combinations. Manual assembly: Warehouse discovers multiple versions, user selects compatible set. Freight cloning use case: Production hotfix - clone prod Freight, replace one buggy component.\nVisibility Beyond Kargo#\rEnterprise requirement: tracking releases beyond the Kargo system.\nKargo tracks internally (Freight, commits) External needs: tags, manifests, dashboards Promotion steps can create tags Release manifest files for automation Hybrid approach recommended graph LR\rsubgraph Internal Kargo\rI1[Freight resources]\rI2[Git commits]\rI3[Promotion history]\rI4[Health status]\rend\rsubgraph External Git/Tools\rE1[Git tags]\rE2[Release manifests]\rE3[Dashboards]\rE4[CMDB integration]\rend\rInternal Kargo \u0026lt;--\u0026gt;|Promotion\u0026lt;br/\u0026gt;creates both| External Git/Tools\rstyle Internal Kargo fill:#e3f2fd,stroke:#1976d2\rstyle External Git/Tools fill:#f5f5f5,stroke:#757575Approach: Kargo provides excellent internal tracking via Freight and Git commits. External visibility needs: Git tags for humans, manifests for automation. The promotion process can create both simultaneously. Hybrid approach: rich internal tracking plus external accessibility.\nArchitectural Trade-offs#\rBoth architectural approaches have merit - not one-size-fits-all.\nTraditional: Single version across artifacts Kargo: Independent versions, bundled promotion Unified: Simple, inflexible Freight: Flexible, more complex Both valid; context matters graph TB\rsubgraph Traditional Unified\rT[v1.5.0]\rT1[Frontend: v1.5.0]\rT2[Backend: v1.5.0]\rT3[Config: v1.5.0]\rT --\u0026gt; T1\rT --\u0026gt; T2\rT --\u0026gt; T3\rTP[‚úì Simple]\rTN[‚úó Inflexible]\rend\rsubgraph Kargo Freight Bundles\rK1[Frontend: v2.1.0]\rK2[Backend: v3.0.1]\rK3[Config: v1.8.2]\rKB[Freight: abc123def456]\rK1 --\u0026gt; KB\rK2 --\u0026gt; KB\rK3 --\u0026gt; KB\rKP[‚úì Flexible]\rKN[‚úó More complex]\rend\rstyle Traditional Unified fill:#f5f5f5,stroke:#757575\rstyle Kargo Freight Bundles fill:#e3f2fd,stroke:#1976d2Unified Versioning: Single version tag across all components (v1.5.0 everywhere). Simple to communicate, easy mental model. But: tight coupling, requires rebuilding everything for any change.\nFreight Approach: Each artifact maintains independent version. Flexible, handles independent lifecycles, selective promotion. But: more complex, requires coordination mechanism.\nMigration Path: Hybrid approaches possible - unified major versions, independent minor/patch. Migration path exists from unified to Freight-based promotion.\nEnsuring Safe Promotions#\rStage health encompasses more than just Application health.\nStage health ‚â† Application health Multiple health indicators aggregate Argo Rollouts integration Custom verification jobs Progressive promotion gates Health Checks Aggregate:\n‚úì Application sync status ‚úì Application health status ‚úì Verification results ‚úì Custom checks ‚Üí Stage health decision Integration: Works with Argo Rollouts for progressive delivery patterns. Custom verification via Kubernetes Jobs or external systems. Verification gates prevent premature promotion to next Stage. Failed verification stops progression - safe by default.\nDeployment Patterns with Kargo#\rNot one-size-fits-all - pattern selection based on requirements.\nCommon: Image + config together Grouped: Multiple images as unit Independent: Different cadences Gatekeeper: Filter bad combinations Choose based on coupling Pattern Overview:\n[Common Case] [Grouped Services]\rSingle Warehouse Multiple images\r‚Üì ‚Üì\rCombined Promoted\rFreight together\r[Independent] [Gatekeeper]\rMultiple Initial filter\rWarehouses stage firstPattern Selection:\nCommon case: Single Warehouse for tightly coupled components (image + config) Grouped services: Microservices with interdependencies promoted together Independent pipelines: Different promotion cadences for different artifact types Gatekeeper pattern: Initial Stage filters invalid combinations before important environments Selection criteria: Component coupling, change frequency, risk tolerance. Most teams start with common pattern, evolve to more complex as needs arise.\nGitOps Repository Structure#\rGitOps repository structure affects the promotion workflow.\nStage-specific branches (recommended) Single branch with paths (alternative) Format: stage/dev, stage/staging, stage/prod Not Git Flow‚Äîindependent storage Use path filters to prevent loops Stage-Specific Branches:\nmain (source)\r‚îú‚îÄ‚îÄ stage/dev (rendered)\r‚îú‚îÄ‚îÄ stage/staging (rendered)\r‚îî‚îÄ‚îÄ stage/prod (rendered)Single Branch Alternative:\nmain\r‚îú‚îÄ‚îÄ builds/dev/\r‚îú‚îÄ‚îÄ builds/staging/\r‚îî‚îÄ‚îÄ builds/prod/Recommended Approach: Stage-specific branches for rendered manifests. Clear separation of environment states with independent histories. Easy rollback per environment - checkout previous commit on branch. These are NOT merge-based Git Flow branches - they\u0026rsquo;re independent storage for rendered state.\nPath Filter Requirement: Prevent Warehouse from triggering on its own promotion outputs. Feedback loop prevention is critical for stability.\nProduction Recommendations#\rProduction-tested recommendations from real deployments.\nSeparate source from GitOps repos Use Freight creation criteria Auto-promote lower environments Manual gates for production Implement verification at each stage Checklist:\n‚úì Repository structure: Source ‚â† GitOps ‚úì Freight creation: Use criteria expressions ‚úì Promotion policy: Auto dev, manual prod ‚úì Verification: Define for each stage ‚úì Access control: RBAC for approvals Details:\nRepository separation: Source code repos separate from GitOps manifest repos. Prevents conflicts, enables different access controls. Freight creation criteria: Prevent incompatible combinations proactively Automation strategy: Auto-promote dev/test, manual approval for production Verification: At each Stage before allowing next promotion Security: RBAC-based access control for production promotion approvals. Least privilege via Kubernetes RBAC. Issues and Quick Fixes#\rCommon issues encountered in real deployments with quick fixes.\nIncompatible Freight ‚Üí Creation criteria Feedback loops ‚Üí Path filters Slow discovery ‚Üí Webhooks App not syncing ‚Üí Check annotation Lost context ‚Üí External tracking Troubleshooting Guide:\nIssue Symptom Solution Bad combos Version mismatch Use criteria Loops Infinite triggers Path filters Slow Delayed discovery Add webhooks No sync CR not updating Check annotation No visibility Can\u0026rsquo;t track externally Git tags Diagnostic Approach:\nIncompatible Freight: Use creation criteria expressions to prevent Feedback loops: Warehouse triggering on own promotion output - use path filters Slow artifact discovery: Add webhooks for immediate notification Application not syncing: Verify authorization annotation present and correct Lost context externally: Implement Git tagging in promotion process Most issues are configuration, not bugs. Check Warehouse status, Promotion logs, Application events.\nIntegrating Kargo into Existing Setup#\rLow-risk adoption strategy for existing ArgoCD environments.\nPrerequisites: K8s, ArgoCD, GitOps repo Start small: Single app, dev‚Üístaging Expand gradually: Add stages, apps No disruption to existing ArgoCD Incremental adoption possible Adoption Phases:\nPhase 1: Install + Single App\r‚Üì\rPhase 2: Add Stages\r‚Üì\rPhase 3: Multiple Apps\r‚Üì\rPhase 4: Advanced PatternsStrategy Details:\nPrerequisites already in place for most teams: Kubernetes cluster, ArgoCD, GitOps repositories Start with single non-critical application, two stages (dev, staging) ArgoCD continues working as-is - no disruption to existing deployments Learn Kargo patterns and concepts with low stakes Expand gradually: Add production stage, additional applications Advanced patterns come later: Multiple Warehouses, verification, complex criteria Team can learn incrementally without big-bang migration Parallel operation: Some apps via Kargo, others traditional while transitioning Key Takeaways#\rRecap core concepts: Kargo addresses the promotion gap in ArgoCD.\nKargo solves ArgoCD promotion gap Declarative, Kubernetes-native Freight bundles coordinate artifacts Progressive, verifiable promotions Resources: docs.kargo.io Key Points:\nArgoCD deploys, Kargo promotes Freight = immutable artifact bundles Stages = promotion targets Kubernetes-native integration Production-ready from Argo team Resources:\nDocumentation: docs.kargo.io GitHub: github.com/akuity/kargo Community: Discord, CNCF Slack Summary: Complementary relationship - deployment engine plus promotion orchestration. Freight concept critical: bundled artifacts, not unified versioning. Declarative workflows via Kubernetes resources - GitOps principles maintained.\nAppendix A1: Multi-Repository Warehouse Example#\rComplete Warehouse YAML with subscriptions to image, Git, and Helm repositories.\napiVersion: kargo.akuity.io/v1alpha1 kind: Warehouse metadata: name: my-warehouse namespace: my-project spec: # Automatic Freight creation with criteria freightCreationPolicy: Automatic freightCreationCriteria: expression: | imageFrom(\u0026#39;myapp/frontend\u0026#39;).Tag == imageFrom(\u0026#39;myapp/backend\u0026#39;).Tag subscriptions: # Container image subscription - image: repoURL: myregistry.io/myapp/frontend semverConstraint: ^2.0.0 discoveryLimit: 10 - image: repoURL: myregistry.io/myapp/backend semverConstraint: ^2.0.0 discoveryLimit: 10 # Git repository subscription - git: repoURL: https://github.com/myorg/config.git commitSelectionStrategy: NewestFromBranch branch: main includePaths: - config/ excludePaths: - config/README.md # Helm chart subscription - chart: repoURL: https://charts.example.com name: myapp semverConstraint: ^1.0.0Explanation: This Warehouse subscribes to multiple artifact types. Freight creation criteria ensures frontend and backend versions match before creating Freight. Discovery limits prevent excessive version tracking.\nAppendix A2: Production Stage Example#\rComplete Stage YAML with promotion template and manual approval.\napiVersion: kargo.akuity.io/v1alpha1 kind: Stage metadata: name: production namespace: my-project spec: # Request Freight from upstream staging Stage requestedFreight: - origin: kind: Warehouse name: my-warehouse sources: stages: - staging # Promotion template (manual trigger only) promotionTemplate: spec: vars: - name: gitopsRepo value: https://github.com/myorg/gitops.git steps: - uses: git-clone config: repoURL: ${{ vars.gitopsRepo }} checkout: - branch: stage/production create: true path: ./out - uses: kustomize-set-image config: path: ./out images: - image: myapp/frontend tag: ${{ imageFrom(\u0026#39;myapp/frontend\u0026#39;).Tag }} - uses: git-commit as: commit config: path: ./out message: \u0026#34;Production release\u0026#34; - uses: git-push config: path: ./out - uses: argocd-update config: apps: - name: myapp-production namespace: argocdExplanation: Production Stage accepts Freight verified in staging. Manual promotion required (no auto-promotion). Promotion template updates GitOps repo and triggers ArgoCD sync.\nAppendix A3: ArgoCD Application Authorization Example#\rComplete ArgoCD Application with Kargo authorization annotation.\napiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: myapp-production namespace: argocd annotations: # Authorize Kargo Stage to manage this Application kargo.akuity.io/authorized-stage: \u0026#34;my-project:production\u0026#34; spec: project: default source: repoURL: https://github.com/myorg/gitops.git targetRevision: stage/production path: . destination: server: https://kubernetes.default.svc namespace: myapp-production syncPolicy: automated: prune: true selfHeal: true syncOptions: - CreateNamespace=true # Optional: ignore differences ignoreDifferences: - group: apps kind: Deployment jsonPointers: - /spec/replicasExplanation: Authorization annotation grants production Stage permission to patch this Application. Auto-sync ensures ArgoCD deploys changes immediately. targetRevision points to stage-specific branch updated by Kargo.\nAppendix A4: Freight Creation Criteria Examples#\rExpression-based criteria to prevent incompatible combinations.\n# Tag matching - ensure versions match freightCreationCriteria: expression: | imageFrom(\u0026#39;frontend\u0026#39;).Tag == imageFrom(\u0026#39;backend\u0026#39;).Tag # Version constraints - only create for stable releases freightCreationCriteria: expression: | !(imageFrom(\u0026#39;myapp\u0026#39;).Tag contains \u0026#39;alpha\u0026#39;) \u0026amp;\u0026amp; !(imageFrom(\u0026#39;myapp\u0026#39;).Tag contains \u0026#39;beta\u0026#39;) # Date-based filtering - only recent commits freightCreationCriteria: expression: | commitFrom(\u0026#39;github.com/myorg/repo\u0026#39;).CommitterDate .After(date(\u0026#39;2025-01-01\u0026#39;)) # Combined criteria - versions match AND stable freightCreationCriteria: expression: | imageFrom(\u0026#39;frontend\u0026#39;).Tag == imageFrom(\u0026#39;backend\u0026#39;).Tag \u0026amp;\u0026amp; imageFrom(\u0026#39;backend\u0026#39;).Tag == chartFrom(\u0026#39;myapp\u0026#39;).Version \u0026amp;\u0026amp; !(imageFrom(\u0026#39;frontend\u0026#39;).Tag contains \u0026#39;rc\u0026#39;) # Specific version pattern - only patch updates freightCreationCriteria: expression: | imageFrom(\u0026#39;myapp\u0026#39;).Tag matches \u0026#39;^v2\\\\.1\\\\.\\\\d+$\u0026#39;Explanation: Criteria prevent Warehouse from creating Freight with incompatible artifact versions. Use expressions for tag matching, version filtering, date constraints, or complex combinations.\nAppendix A5: Complete Promotion Workflow#\rFull promotion template with all common steps.\npromotionTemplate: spec: vars: - name: gitopsRepo value: https://github.com/myorg/gitops.git steps: # Clone source and output branches - uses: git-clone config: repoURL: ${{ vars.gitopsRepo }} checkout: - branch: main path: ./src - branch: stage/dev create: true path: ./out # Clear output directory - uses: git-clear config: path: ./out # Update image tags - uses: kustomize-set-image as: update-image config: path: ./src/overlays/dev images: - image: myapp/frontend tag: ${{ imageFrom(\u0026#39;myapp/frontend\u0026#39;).Tag }} - image: myapp/backend tag: ${{ imageFrom(\u0026#39;myapp/backend\u0026#39;).Tag }} # Build manifests - uses: kustomize-build config: path: ./src/overlays/dev outPath: ./out # Commit changes - uses: git-commit as: commit config: path: ./out messageFromSteps: - update-image # Push to remote - uses: git-push config: path: ./out # Update ArgoCD Application - uses: argocd-update config: apps: - name: myapp-dev namespace: argocd sources: - repoURL: ${{ vars.gitopsRepo }} desiredRevision: ${{ outputs.commit.commit }}Explanation: Complete promotion workflow: clone repos, update images, build manifests, commit, push, and trigger ArgoCD. Uses step outputs (commit SHA) to update Application targetRevision.\nAppendix A6: Git Tag Creation for Releases#\rBash step for creating annotated tags and release manifests.\n# Git tag creation step in promotion template steps: # ... standard promotion steps ... # Create annotated Git tag - uses: bash config: script: | #!/bin/bash cd ./out # Generate release version RELEASE_TAG=\u0026#34;release-$(date +%Y.%m.%d-%H%M%S)\u0026#34; # Create annotated tag with component manifest git tag -a \u0026#34;$RELEASE_TAG\u0026#34; -m \u0026#34;Production Release Kargo Freight: ${{ ctx.freight.name }} Freight Alias: ${{ ctx.freight.alias }} Components: - Frontend: ${{ imageFrom(\u0026#39;myapp/frontend\u0026#39;).Tag }} - Backend: ${{ imageFrom(\u0026#39;myapp/backend\u0026#39;).Tag }} - Chart: ${{ chartFrom(\u0026#39;myapp\u0026#39;).Version }} Promoted: $(date -u +%Y-%m-%dT%H:%M:%SZ) Promoted By: ${{ ctx.promotion.createdBy }}\u0026#34; # Push tag git push origin \u0026#34;$RELEASE_TAG\u0026#34; # Optional: Create release manifest cat \u0026gt; release-manifest.json \u0026lt;\u0026lt;EOF { \u0026#34;version\u0026#34;: \u0026#34;$RELEASE_TAG\u0026#34;, \u0026#34;freight\u0026#34;: \u0026#34;${{ ctx.freight.name }}\u0026#34;, \u0026#34;components\u0026#34;: { \u0026#34;frontend\u0026#34;: \u0026#34;${{ imageFrom(\u0026#39;myapp/frontend\u0026#39;).Tag }}\u0026#34;, \u0026#34;backend\u0026#34;: \u0026#34;${{ imageFrom(\u0026#39;myapp/backend\u0026#39;).Tag }}\u0026#34; } } EOFExplanation: Create Git tags during production promotions for external tracking. Tags include complete component manifest, Freight reference, and timestamp. Release manifest enables programmatic access.\nAppendix A7: Multiple Warehouses Pattern#\rIndependent pipelines for different artifact cadences.\n# Warehouse 1: Application images (fast-moving) apiVersion: kargo.akuity.io/v1alpha1 kind: Warehouse metadata: name: app-images namespace: my-project spec: subscriptions: - image: repoURL: myregistry.io/myapp/frontend - image: repoURL: myregistry.io/myapp/backend --- # Warehouse 2: Infrastructure config (slow-moving) apiVersion: kargo.akuity.io/v1alpha1 kind: Warehouse metadata: name: infra-config namespace: my-project spec: subscriptions: - git: repoURL: https://github.com/myorg/infra-config.git branch: main --- # Stage accepts Freight from BOTH Warehouses apiVersion: kargo.akuity.io/v1alpha1 kind: Stage metadata: name: staging namespace: my-project spec: requestedFreight: - origin: kind: Warehouse name: app-images sources: direct: true - origin: kind: Warehouse name: infra-config sources: direct: trueExplanation: Use multiple Warehouses when artifacts have different change frequencies. Stage requests Freight from both Warehouses. Enables promoting images daily while config changes weekly.\nAppendix A8: Freight Cloning for Hotfixes#\rUsing Freight cloning to create emergency hotfix releases.\nScenario: Critical bug in production backend\nProduction Freight contains:\nfrontend:v2.0.0 backend:v2.0.0 (buggy) config:v1.5.0 Step 1: Build and push hotfix\ndocker build -t myregistry.io/myapp/backend:v2.0.1 . docker push myregistry.io/myapp/backend:v2.0.1Step 2: Clone production Freight via Kargo UI/CLI\nNavigate to production Stage Select current Freight Click \u0026ldquo;Clone Freight\u0026rdquo; Select backend artifact Change version to v2.0.1 Click \u0026ldquo;Create Freight\u0026rdquo; Result: New Freight with hotfix\nfrontend:v2.0.0 (unchanged) backend:v2.0.1 (hotfix) config:v1.5.0 (unchanged) Step 3: Promote hotfix Freight to production\nCan bypass lower environments for emergency Or test in dev/staging first if time permits Key Benefits: Freight cloning preserves all other artifacts. Only replaces the component needing hotfix. Maintains reproducibility and audit trail.\nAppendix A9: Verification with Argo Rollouts#\rStage verification using AnalysisTemplate.\n# Stage with verification via AnalysisTemplate apiVersion: kargo.akuity.io/v1alpha1 kind: Stage metadata: name: staging namespace: my-project spec: requestedFreight: - origin: kind: Warehouse name: my-warehouse sources: direct: true # Verification configuration verification: analysisTemplates: - name: smoke-tests # Run analysis after promotion analysisRunMetadata: labels: stage: staging promotionTemplate: # ... promotion steps ... --- # AnalysisTemplate for verification apiVersion: argoproj.io/v1alpha1 kind: AnalysisTemplate metadata: name: smoke-tests namespace: my-project spec: metrics: - name: http-check interval: 30s count: 5 successCondition: result == \u0026#39;200\u0026#39; provider: web: url: https://myapp-staging.example.com/health jsonPath: \u0026#34;{$.status}\u0026#34; - name: error-rate interval: 1m count: 3 successCondition: result \u0026lt; 0.05 provider: prometheus: address: http://prometheus:9090 query: | sum(rate(http_requests_total{status=~\u0026#34;5..\u0026#34;}[5m])) / sum(rate(http_requests_total[5m]))Explanation: Verification ensures Freight is healthy before marking as verified for next Stage. AnalysisTemplate defines health checks: HTTP endpoints, Prometheus metrics, custom tests. Failed verification prevents promotion.\nAppendix A10: Troubleshooting Common Issues#\rDiagnostic steps and solutions for frequent problems.\nIssue 1: Freight not being created\nCheck Warehouse status:\nkubectl get warehouse -n my-project my-warehouse -o yaml # Look for errors in status.conditionsSolution: Add credentials for private registries:\napiVersion: v1 kind: Secret metadata: name: registry-creds namespace: my-project labels: kargo.akuity.io/cred-type: image stringData: repoURL: ghcr.io username: myuser password: ghp_xxxxxxxxxxxxxIssue 2: Promotion failing - git push errors\nCheck for Git credentials:\napiVersion: v1 kind: Secret metadata: name: git-creds namespace: my-project labels: kargo.akuity.io/cred-type: git stringData: repoURL: https://github.com/myorg/gitops.git username: myuser password: ghp_xxxxxxxxxxxxxIssue 3: Application not syncing after promotion\nVerify authorization annotation:\nkubectl get application -n argocd myapp-dev -o yaml # Should have: kargo.akuity.io/authorized-stage: \u0026#34;project:stage\u0026#34;Issue 4: Check Promotion logs for detailed errors\nkubectl get promotion -n my-project \u0026lt;promotion-name\u0026gt; -o yaml # Review status.stepResults for failures Appendix A11: Expression Language Reference#\rComplete guide to Kargo expression syntax.\nContext Variables:\n${{ ctx.stage }} # Current Stage name\r${{ ctx.freight.name }} # Freight SHA identifier\r${{ ctx.freight.alias }} # Human-friendly alias\r${{ ctx.promotion.createdBy }} # Who triggered promotionArtifact Functions:\n${{ imageFrom(\u0026#39;myapp/frontend\u0026#39;).Tag }} # Image tag\r${{ imageFrom(\u0026#39;myapp/frontend\u0026#39;).Digest }} # Image digest\r${{ commitFrom(\u0026#39;github.com/org/repo\u0026#39;).ID }} # Git commit SHA\r${{ commitFrom(\u0026#39;github.com/org/repo\u0026#39;).Message }} # Commit message\r${{ chartFrom(\u0026#39;my-chart\u0026#39;).Version }} # Helm chart versionArtifact Functions with Warehouse Filter:\n${{ imageFrom(\u0026#39;myapp\u0026#39;, warehouse(\u0026#39;my-warehouse\u0026#39;)).Tag }}Variable References:\n${{ vars.gitopsRepo }} # Global variable\r${{ outputs.commit.commit }} # Step output (by alias)\r${{ outputs[\u0026#39;step-name\u0026#39;].value }} # Square bracket notation\r${{ task.outputs[\u0026#39;step\u0026#39;].value }} # Task outputUtility Functions:\n${{ timestamp() }} # Unix timestamp\r${{ timestamp(\u0026#34;2006.01.02\u0026#34;) }} # Formatted date\r${{ timestamp(\u0026#34;2006-01-02T15:04:05\u0026#34;) }} # ISO format\r${{ quote(imageFrom(\u0026#39;app\u0026#39;).Tag) }} # YAML-safe quotingFreight Creation Criteria Examples:\nimageFrom(\u0026#39;frontend\u0026#39;).Tag == imageFrom(\u0026#39;backend\u0026#39;).Tag\rimageFrom(\u0026#39;app\u0026#39;).Tag \u0026gt; \u0026#39;2.0.0\u0026#39;\r!(imageFrom(\u0026#39;app\u0026#39;).Tag contains \u0026#39;alpha\u0026#39;)\rcommitFrom(\u0026#39;repo\u0026#39;).CommitterDate.After(date(\u0026#39;2025-01-01\u0026#39;)) Appendix A12: Complete End-to-End Example#\rFull Kargo setup for e-commerce microservices application.\n# Complete Kargo Setup for E-Commerce Application # 1. Warehouse for microservices apiVersion: kargo.akuity.io/v1alpha1 kind: Warehouse metadata: name: ecommerce-warehouse namespace: ecommerce spec: subscriptions: - image: repoURL: myregistry.io/ecommerce/frontend - image: repoURL: myregistry.io/ecommerce/api - image: repoURL: myregistry.io/ecommerce/worker - chart: repoURL: https://charts.example.com name: ecommerce --- # 2. Dev Stage (auto-promote) apiVersion: kargo.akuity.io/v1alpha1 kind: Stage metadata: name: dev namespace: ecommerce spec: requestedFreight: - origin: kind: Warehouse name: ecommerce-warehouse sources: direct: true promotionTemplate: spec: steps: - uses: git-clone config: repoURL: https://github.com/myorg/ecommerce-gitops.git checkout: - branch: stage/dev create: true path: ./out - uses: helm-update-image config: path: ./out/values.yaml images: - image: ecommerce/frontend key: frontend.image.tag - image: ecommerce/api key: api.image.tag - uses: git-commit config: path: ./out - uses: git-push config: path: ./out - uses: argocd-update config: apps: - name: ecommerce-devExplanation: Complete example showing Warehouse tracking 3 microservices + Helm chart, dev Stage with auto-promotion. Production would add verification and manual approval. Demonstrates real-world multi-service setup.\nEnd of Document\n"},{"id":29,"href":"/musings/docs/misc/notion-overview/","title":"Notion: A Comprehensive Overview","section":"Miscellaneous","content":"Notion: A Comprehensive Overview#\rLet me give you a detailed breakdown of what Notion is and what it can do:\nWhat is Notion?#\rNotion is an all-in-one workspace that combines note-taking, task management, databases, wikis, and collaboration tools into a single, highly customizable platform. Think of it as a digital Swiss Army knife for productivity - you can use it to replace multiple tools like Evernote, Trello, Google Docs, Confluence, and Airtable.\nKey Features (2024-2025)#\rCore Capabilities:#\rFlexible page system - Create nested pages with unlimited hierarchy Block-based editor - Everything is a \u0026ldquo;block\u0026rdquo; (text, images, tables, databases, etc.) that you can drag and drop Databases - Create tables, boards (Kanban), calendars, galleries, lists, and timelines Templates - Extensive library of pre-built templates for everything from meeting notes to project management Real-time collaboration - Multiple users can edit simultaneously with comments, @mentions, and task assignments Recent Major Updates (2024):#\rNotion Calendar - Standalone calendar app that syncs with Google Calendar and Notion databases\nNotion Sites - Turn Notion pages into public websites with custom domains, SEO settings, and Google Analytics\nForms - Create customizable forms that feed directly into databases\nLayout Builder - Advanced page layout customization with columns and sections\nNotion AI (paid add-on) - AI assistant for writing, summarizing, brainstorming, and searching across your workspace and connected apps\nWebhooks \u0026amp; Automation - Connect Notion to external services and create automated workflows\nNotion Marketplace - Browse and install templates directly within the app\nComing in 2025:#\rNotion Mail - Integrated email client (currently in alpha) Notion Drive - File storage system to reduce reliance on Google Drive/Dropbox Offline Mode - The most requested feature for years Advanced brainstorming tools - Whiteboarding and mind-mapping capabilities Pricing (2024-2025)#\rFree: Unlimited pages/blocks for individuals, 5MB file upload limit, 10 guests Plus: $10/month (annual) - Unlimited uploads, 100 guests, 30-day version history Business: $18/month (annual) - SAML SSO, advanced permissions, unlimited version history Enterprise: Custom pricing - Advanced admin tools, audit logs, dedicated support Notion AI: Additional $8-10/user/month (now included in Business+ plans for some users) Pros#\r‚úÖ Incredible flexibility - Build almost anything you can imagine ‚úÖ All-in-one solution - Replace multiple apps with one tool ‚úÖ Beautiful, intuitive interface - Clean design that\u0026rsquo;s pleasant to use ‚úÖ Strong collaboration - Great for teams working together ‚úÖ Extensive templates - Thousands of free templates to get started ‚úÖ Cross-platform - Web, desktop (Windows/Mac), mobile (iOS/Android) ‚úÖ Affordable - Free plan is generous; paid plans are competitive ‚úÖ Active development - Constant improvements and new features ‚úÖ Large community - Tons of tutorials, templates, and support\nCons#\r‚ùå Steep learning curve - Overwhelming for new users; takes time to master ‚ùå No true offline mode - Requires internet connection (offline coming in 2025) ‚ùå Performance issues with large databases - Can slow down with complex workspaces ‚ùå Mobile app limitations - Not as feature-rich as desktop version ‚ùå Limited out-of-the-box solutions - Requires setup and customization ‚ùå Can be a productivity trap - Easy to spend hours tweaking instead of working ‚ùå No page-level password protection - Security limited to workspace level ‚ùå Export limitations - Can be challenging to migrate data out ‚ùå Limited specialized features - Not as powerful as dedicated tools (e.g., advanced project management like Jira)\nCommon Use Cases#\rPersonal knowledge management - Second brain, digital journal, habit tracking Team wikis - Company knowledge bases, documentation, SOPs Project management - Task tracking, roadmaps, sprints Content creation - Editorial calendars, content planning CRM - Lightweight customer relationship management Meeting notes - Centralized meeting documentation with action items Website creation - Simple websites and portfolios via Notion Sites Who Should Use Notion?#\rGreat for:\nIndividuals who want an all-in-one personal organization system Small to medium teams needing flexible collaboration tools Startups wanting to centralize their knowledge and processes Teams that value customization over out-of-the-box solutions Content creators, students, researchers Not ideal for:\nUsers who need robust offline access Teams requiring specialized, advanced project management (use Jira, Asana) Those who want zero setup time (Notion requires configuration) Users uncomfortable with a learning curve Organizations with strict data sovereignty requirements Notion vs. Traditional Wikis#\rAdvantages over wikis like Confluence:\nMuch more modern, beautiful interface Easier to use and navigate Better for personal use alongside team collaboration More affordable for small teams Faster to set up Disadvantages compared to wikis:\nLess robust permission controls (until Enterprise plan) No page-level access control Fewer enterprise-grade features Less mature integrations with enterprise tools Bottom Line#\rNotion is an exceptionally powerful and flexible tool that can genuinely replace multiple apps in your workflow. However, its strength (flexibility) is also its weakness - it requires investment to set up properly and can be overwhelming at first.\nFor documentation specifically, Notion excels at collaborative, living documentation where multiple people contribute and content changes frequently. It\u0026rsquo;s less ideal for versioned technical documentation that needs to be tightly coupled with code (where TechDocs/docs-as-code shines).\nMany teams successfully use both: Notion for collaborative knowledge bases, runbooks, and team processes, and TechDocs for technical API documentation that lives with the code.\n"},{"id":30,"href":"/musings/docs/misc/oss-portal-platform-orchestration-tools/","title":"Oss Portal Platform Orchestration Tools","section":"Miscellaneous","content":"Based on my search, here\u0026rsquo;s what I found for open-source tools in these two specific categories:\nOpen Source Developer Portals:#\rBackstage - Apache 2.0 (CNCF project) Kratix - Apache 2.0 (also serves as platform orchestrator) Torii - GPL-3.0 (from Qovery, but not production-ready) That\u0026rsquo;s it. These are the only truly open-source developer portals I found. Everything else in the developer portal space (Port, Cortex, OpsLevel, Configure8, Rely.io, Atlassian Compass, Roadie, etc.) is proprietary/commercial.\nOpen Source Platform Orchestrators:#\rKratix - Apache 2.0 (Kubernetes-based framework for platform-as-a-product) KusionStack/Kusion - Apache 2.0 (intent-driven platform orchestrator) KubeVela - Apache 2.0 (implements Open Application Model) Crossplane - Apache 2.0 (extends Kubernetes to manage cloud resources) Commercial Platform Orchestrators (for comparison):\nHumanitec - Commercial Cloudomation Engine - Commercial Key Finding: The open-source landscape for developer portals is extremely limited - essentially just Backstage and Kratix are production-ready. For platform orchestrators, there are more options (Kratix, Kusion, KubeVela, Crossplane), though Kratix serves dual purpose as both portal and orchestrator.\nThe article I found noted that platform orchestrators are a category \u0026ldquo;for which there are not many open source tools available (yet),\u0026rdquo; confirming this is a gap in the open-source ecosystem.\n"},{"id":31,"href":"/musings/docs/misc/oss-vs-closed-portal/","title":"Oss Vs Closed Portal","section":"Miscellaneous","content":"Based on my research, here are the open source tools from that list:\nFully Open Source:\nBackstage - Open-source framework from Spotify (Apache 2.0 license, contributed to CNCF) Kratix - Open-source framework for building platform-as-a-product That\u0026rsquo;s it - just those two are truly open source.\nAll the others are commercial/proprietary SaaS platforms:\nPort (commercial, though has some \u0026ldquo;open commercial\u0026rdquo; aspects) Cortex (closed source) OpsLevel (closed source) Configure8 (commercial) Atlassian Compass (proprietary) Rely.io (commercial) Roadie (commercial managed service built on top of open-source Backstage) Mia-Platform (commercial) Cycloid (commercial) Northflank (commercial) Calibo (commercial) So if you\u0026rsquo;re specifically looking for open-source options in this space, you\u0026rsquo;re really looking at Backstage or Kratix as your main choices.\n"},{"id":32,"href":"/musings/docs/misc/crossplane-solution/","title":"Pseudocode for composition function","section":"Miscellaneous","content":"Yes! Crossplane v2 opens up some very elegant solutions for this use case. Let me explain the key differences and how they apply.\nKey Crossplane v2 Improvements Relevant Here#\rCrossplane v2 introduces:\nComposition Functions (replacing Patch \u0026amp; Transform) - Write composition logic in real programming languages (Go, Python, etc.) instead of YAML patches Better lifecycle management - More control over resource ordering and dependencies Improved status handling - Better state tracking and conditions Function pipelines - Chain multiple functions together for complex workflows Solution 1: Custom XRD with Composition Function#\rThis is probably the cleanest approach. Define a Custom Resource (XRD) that represents your restore intent:\napiVersion: apiextensions.crossplane.io/v1 kind: CompositeResourceDefinition metadata: name: xdatabaserestores.ops.example.com spec: group: ops.example.com names: kind: XDatabaseRestore plural: xdatabaserestores claimNames: kind: DatabaseRestore plural: databaserestores versions: - name: v1alpha1 schema: openAPIV3Schema: type: object properties: spec: type: object properties: backupTimestamp: type: string databaseRef: type: string targetDeployments: type: array items: type: string status: type: object properties: phase: type: string lastAppliedBackup: type: string operationStarted: type: stringThen create a Composition Function (in Go, Python, etc.) that:\n# Pseudocode for composition function def compose(request): desired = request.observed.composite.spec.backupTimestamp applied = request.observed.composite.status.lastAppliedBackup if desired == applied: # Nothing to do, already in desired state return response(resources=[]) phase = request.observed.composite.status.phase if phase == \u0026#34;Complete\u0026#34;: # Start new operation return response(resources=[ disable_ingress_patch(), scale_deployments_to_zero(), restore_job(desired), ], status={\u0026#34;phase\u0026#34;: \u0026#34;InProgress\u0026#34;}) elif phase == \u0026#34;InProgress\u0026#34;: # Check if job completed job_status = get_job_status(request) if job_status == \u0026#34;Succeeded\u0026#34;: return response(resources=[ restore_deployments(), enable_ingress(), ], status={ \u0026#34;phase\u0026#34;: \u0026#34;Complete\u0026#34;, \u0026#34;lastAppliedBackup\u0026#34;: desired }) return response()Usage in Git:\napiVersion: ops.example.com/v1alpha1 kind: DatabaseRestore metadata: name: prod-restore spec: backupTimestamp: \u0026#34;2024-12-10T00:00:00Z\u0026#34; databaseRef: postgres-prod targetDeployments: - myapp - workerPros:#\rTruly declarative - GitOps-friendly, ArgoCD sees it as just another resource Full programming language - Complex logic, error handling, retries in Python/Go Built-in reconciliation - Crossplane reconciles automatically Idempotent by design - Composition functions are pure functions of desired state Rich status reporting - Can track detailed progress in .status Works without ArgoCD - Just needs Crossplane Clean separation - Business logic in function, state in cluster Cons:#\rRequires writing a composition function (but simpler than a full operator) Need to package and deploy the function Learning curve for Crossplane v2 function model Solution 2: Provider-Kubernetes with Job Orchestration#\rUse Crossplane\u0026rsquo;s Provider-Kubernetes (which can manage any Kubernetes resources) with a structured approach:\napiVersion: apiextensions.crossplane.io/v1 kind: Composition metadata: name: database-restore-workflow spec: compositeTypeRef: apiVersion: ops.example.com/v1alpha1 kind: XDatabaseRestore mode: Pipeline pipeline: - step: check-state functionRef: name: function-check-restore-state - step: create-prereq-resources functionRef: name: function-conditional-resources - step: create-restore-job functionRef: name: function-restore-job - step: create-postreq-resources functionRef: name: function-cleanup-resourcesEach function can conditionally create Kubernetes resources based on state:\n# Function output creates these only when needed - apiVersion: kubernetes.crossplane.io/v1alpha1 kind: Object spec: forProvider: manifest: apiVersion: batch/v1 kind: Job metadata: name: restore-db-20241210 annotations: restore.ops/timestamp: \u0026#34;2024-12-10T00:00:00Z\u0026#34;Pros:#\rLeverages Provider-Kubernetes for actual resource management Pipeline model makes sequencing clear Each function can be simple and focused Can conditionally create/skip resources based on state Cons:#\rStill requires writing functions Provider-Kubernetes adds another layer State tracking across pipeline steps needs careful design Solution 3: Crossplane + ProviderConfig with External System#\rIf you have an external job orchestration system (even something simple), use Crossplane to manage the \u0026ldquo;intent\u0026rdquo; and let external system do work:\napiVersion: apiextensions.crossplane.io/v1 kind: CompositeResourceDefinition metadata: name: xdatabaserestores.ops.example.com # ... XRD definition ... --- apiVersion: apiextensions.crossplane.io/v1 kind: Composition spec: resources: - name: restore-trigger base: apiVersion: kubernetes.crossplane.io/v1alpha2 kind: Object spec: forProvider: manifest: apiVersion: v1 kind: ConfigMap metadata: name: restore-intent data: backupTimestamp: \u0026#34;\u0026#34; # Patched from XR state: \u0026#34;pending\u0026#34;Then your Option 3 controller watches for this ConfigMap and does the work, updating the state back.\nPros:#\rCombines Crossplane\u0026rsquo;s declarative model with your simpler controller Can reuse existing tooling Clear separation: Crossplane = state management, controller = execution Easier than full composition function for simple cases Cons:#\rTwo systems to coordinate Not as \u0026ldquo;pure\u0026rdquo; as function-based approach Still need the controller logic Solution 4: Crossplane with Provider-Helm for Jobs#\rUse Provider-Helm to deploy Helm charts that include Jobs:\napiVersion: helm.crossplane.io/v1beta1 kind: Release metadata: name: database-restore-20241210 spec: forProvider: chart: name: database-restore repository: https://charts.example.com values: backupTimestamp: \u0026#34;2024-12-10T00:00:00Z\u0026#34; onlyIfChanged: true # Crossplane tracks if values changed, only upgrades if neededThe Helm chart contains Jobs with idempotent scripts.\nPros:#\rSimple if you already use Helm Helm\u0026rsquo;s release tracking provides some state management Can package restore logic in a chart Cons:#\rHelm isn\u0026rsquo;t really designed for operational workflows Less elegant than purpose-built solution State tracking still manual within the chart My Recommendation for Crossplane v2#\rStart with Solution 1: Custom XRD + Composition Function\nHere\u0026rsquo;s why it\u0026rsquo;s cleanest:\nTrue GitOps - Your git repo has:\napiVersion: ops.example.com/v1alpha1 kind: DatabaseRestore metadata: name: prod-restore spec: backupTimestamp: \u0026#34;2024-12-10T00:00:00Z\u0026#34;That\u0026rsquo;s it. Clean, declarative, obvious.\nComposition function handles complexity - All the imperative \u0026ldquo;how\u0026rdquo; lives in code, not YAML. You write proper error handling, logging, state machines in a real language.\nCrossplane does reconciliation - You don\u0026rsquo;t write a controller loop. Crossplane handles watching, reconciling, retrying.\nStatus is built-in - .status field shows exactly what happened:\nstatus: phase: Complete lastAppliedBackup: \u0026#34;2024-12-10T00:00:00Z\u0026#34; operationStarted: \u0026#34;2024-12-11T10:00:00Z\u0026#34; operationCompleted: \u0026#34;2024-12-11T10:15:00Z\u0026#34; Idempotent by design - Function receives current state, returns desired resources. Crossplane ensures idempotency.\nWorks everywhere - With or without ArgoCD. Just needs Crossplane.\nQuick Comparison to Option 3#\rAspect Option 3 (Deployment + ConfigMap) Crossplane XRD + Function Reconciliation Manual (polling loop) Automatic (Crossplane) Language Bash scripts Python/Go State tracking Manual in ConfigMap Built-in .status Idempotency Manual implementation Framework-provided GitOps Works, but hacky First-class Learning curve Low Medium Maintainability Scripts get messy Structured code The composition function approach is \u0026ldquo;just enough\u0026rdquo; structure without being a full operator. You get 95% of operator benefits with maybe 30% of the work.\nWould you like me to show a more concrete example of a composition function for this use case?\n"},{"id":33,"href":"/musings/docs/misc/recommendations/","title":"Recommendations","section":"Miscellaneous","content":"I\u0026rsquo;ll help you research tools for documenting domain object models in UML with your specific requirements. Let me search for current options that meet your criteria.Let me search for more specific information about diagram-as-code tools and their capabilities for selective view rendering.Now let me search for more information about how these tools handle multiple views and vector/bitmap export capabilities.Perfect! Now I have enough information to provide a comprehensive overview. Let me compile this for you.\nOverview of Text-Based UML Domain Modeling Tools#\rBased on my research, I\u0026rsquo;ve identified several excellent tools that meet at least 3 of your 4 requirements. Here\u0026rsquo;s a strategic overview organized by capability:\nTop Recommendations#\r1. PlantUML ‚≠ê Most Mature#\rMeets all 4 requirements:\n‚úÖ Text-based: Plain text .puml files with simple, human-readable syntax ‚úÖ Multiple diagram views: Can create multiple separate diagram files that reference common model elements ‚úÖ Open source: GPL/Apache/MIT licenses ‚úÖ High-quality exports: PNG, SVG, PDF, EPS, LaTeX PlantUML uses simple textual descriptions to create UML diagrams and can export to PNG, SVG, LaTeX, and EPS formats. The tool is highly versatile and supports class, sequence, component, deployment, and many other diagram types.\nStrengths:\nExtremely mature with large community Excellent export quality, especially SVG for vector graphics Can include/reference common definitions across multiple diagram files IDE integrations (VSCode, Eclipse, IntelliJ) Limitation for your use case:\nEach diagram is typically a separate text file rather than one unified model file, though you can use !include to reference shared definitions 2. Structurizr DSL ‚≠ê Best for Multiple Views from Single Model#\rMeets all 4 requirements:\n‚úÖ Text-based: DSL (Domain Specific Language) in text format ‚úÖ Multiple diagram views: THIS IS ITS STRENGTH - define model once, create multiple views ‚úÖ Open source: Apache License ‚úÖ Exports: Can export to PlantUML, Mermaid, and then to vector/bitmap formats The Structurizr DSL is model-based, making it possible to generate multiple diagrams at different levels of abstraction from a single DSL file. The Structurizr CLI can export views to PlantUML, Mermaid, WebSequenceDiagrams, and Ilograph formats.\nStrengths:\nPerfect match for your \u0026ldquo;multiple views\u0026rdquo; requirement - you define the model once, then create different views showing selected elements Designed specifically for the C4 model (Context, Container, Component, Code) Model is truly separated from views Can export to other formats for rendering Consideration:\nPrimarily focused on C4/architecture diagrams rather than detailed UML class modeling Requires Structurizr Lite or CLI for rendering 3. StarUML#\rMeets 3 of 4 requirements:\n‚úÖ Text-based: Models stored in JSON format ‚úÖ Open source: Originally open source, now has paid version (some features MIT licensed) ‚úÖ High-quality exports: PDF, SVG, PNG exports StarUML stores modeling data in a very simple JSON format, making it easy to generate custom code via CLI.\nStrengths:\nClean JSON format that\u0026rsquo;s readable and version-controllable Good UI for visual editing when needed Supports many diagram types Limitation:\nLess clear how to create selective views of the same model programmatically Licensing has changed over time (verify current status) Other Notable Options#\rMermaid (3 of 4)#\r‚úÖ Text-based (simple syntax) ‚úÖ Open source ‚úÖ Exports to SVG/PNG ‚ö†Ô∏è Multiple views: Limited - each diagram is separate Mermaid has experimental C4 diagram support with syntax compatible with PlantUML. Great for embedding in documentation but less suited for complex domain modeling with multiple views.\nComparison Matrix#\rTool Text Format Multiple Views from Model Open Source Vector Export Best For PlantUML Plain text (.puml) Via !include ‚úÖ GPL/Apache ‚úÖ SVG, EPS Detailed UML diagrams Structurizr DSL ‚úÖ Native support ‚úÖ Apache Via export chain Architecture/C4 models StarUML JSON Partial ‚ö†Ô∏è Mixed ‚úÖ SVG, PDF UI-first modeling Mermaid Simple text Limited ‚úÖ MIT ‚úÖ SVG Documentation embedding My Recommendation Based on Your Needs#\rFor pure UML domain modeling with multiple views: PlantUML with a structured approach:\nCreate a \u0026ldquo;model\u0026rdquo; folder with common class definitions using !include Create separate diagram files for different views that include/reference the model For architecture/system modeling where you want true \u0026ldquo;model once, view many\u0026rdquo;: Structurizr DSL is superior, though it\u0026rsquo;s more C4-focused than traditional UML.\nWhich approach interests you more? I can dive deeper into:\nHow to structure PlantUML for domain modeling with shared definitions How Structurizr\u0026rsquo;s view mechanism works Detailed export workflows for high-quality outputs Comparison with other tools I found (Modelio, Eclipse Papyrus, etc.) "},{"id":34,"href":"/musings/docs/misc/job-configmap-crossplane-abstraction/","title":"Shell script helper for Option 3","section":"Miscellaneous","content":"Great question - this is smart architectural thinking. You want to avoid painting yourself into a corner. Here\u0026rsquo;s how to abstract it:\nThe Key Abstraction: Separate Intent from Implementation#\rThe core idea is to define a stable interface (the \u0026ldquo;what\u0026rdquo;) that both solutions can implement, keeping the \u0026ldquo;how\u0026rdquo; hidden.\nStep 1: Define Your State Schema#\rCreate a clear contract for what restore state looks like. Use a ConfigMap or CRD that both solutions can work with:\napiVersion: v1 kind: ConfigMap metadata: name: database-restore-state labels: app.kubernetes.io/managed-by: restore-system app.kubernetes.io/component: state data: # The \u0026#34;API\u0026#34; - don\u0026#39;t change these field names desiredBackupTimestamp: \u0026#34;2024-12-10T00:00:00Z\u0026#34; targetDatabase: \u0026#34;postgres-prod\u0026#34; targetDeployments: \u0026#34;myapp,worker,api\u0026#34; # Implementation details - these can change lastAppliedBackupTimestamp: \u0026#34;2024-12-01T00:00:00Z\u0026#34; operationPhase: \u0026#34;Complete\u0026#34; operationId: \u0026#34;restore-20241210-001\u0026#34; message: \u0026#34;Restore completed successfully\u0026#34;OR (better) - create a minimal CRD now that Crossplane can adopt later:\napiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: name: databaserestores.ops.example.com spec: group: ops.example.com names: kind: DatabaseRestore plural: databaserestores scope: Namespaced versions: - name: v1alpha1 served: true storage: true schema: openAPIV3Schema: type: object properties: spec: type: object required: [backupTimestamp, database] properties: backupTimestamp: type: string format: date-time database: type: string deployments: type: array items: type: string ingressesToDisable: type: array items: type: string status: type: object properties: phase: type: string enum: [Pending, InProgress, Complete, Failed] lastAppliedBackup: type: string operationId: type: string message: type: string startTime: type: string completionTime: type: string subresources: status: {}This CRD is Crossplane-compatible but doesn\u0026rsquo;t require Crossplane yet.\nStep 2: Standardized Status Updates#\rBoth implementations must update status the same way:\n# Shell script helper for Option 3 update_status() { local phase=$1 local message=$2 kubectl patch databaserestore $RESTORE_NAME \\ --type=merge \\ --subresource=status \\ -p \u0026#34;{ \\\u0026#34;status\\\u0026#34;: { \\\u0026#34;phase\\\u0026#34;: \\\u0026#34;$phase\\\u0026#34;, \\\u0026#34;message\\\u0026#34;: \\\u0026#34;$message\\\u0026#34;, \\\u0026#34;lastUpdated\\\u0026#34;: \\\u0026#34;$(date -u +%Y-%m-%dT%H:%M:%SZ)\\\u0026#34; } }\u0026#34; } # Usage update_status \u0026#34;InProgress\u0026#34; \u0026#34;Disabling ingress...\u0026#34;Later, your Crossplane function updates the same fields:\ndef compose(request): # ... return response( status={ \u0026#34;phase\u0026#34;: \u0026#34;InProgress\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Disabling ingress...\u0026#34;, \u0026#34;lastUpdated\u0026#34;: now() } )Step 3: Modular Implementation Scripts#\rPut all your actual restore logic in a separate container image/scripts that both solutions can call:\nrestore-scripts/\r‚îú‚îÄ‚îÄ common.sh # Shared utilities\r‚îú‚îÄ‚îÄ pre-restore.sh # Disable ingress, scale down\r‚îú‚îÄ‚îÄ restore.sh # Actual backup restore\r‚îú‚îÄ‚îÄ post-restore.sh # Scale up, enable ingress\r‚îî‚îÄ‚îÄ rollback.sh # Emergency rollbackOption 3 controller calls these:\n#!/bin/bash source /scripts/common.sh DESIRED=$(kubectl get databaserestore $NAME -o jsonpath=\u0026#39;{.spec.backupTimestamp}\u0026#39;) APPLIED=$(kubectl get databaserestore $NAME -o jsonpath=\u0026#39;{.status.lastAppliedBackup}\u0026#39;) if [ \u0026#34;$DESIRED\u0026#34; != \u0026#34;$APPLIED\u0026#34; ]; then update_status \u0026#34;InProgress\u0026#34; \u0026#34;Starting restore\u0026#34; /scripts/pre-restore.sh \u0026#34;$NAME\u0026#34; || exit 1 /scripts/restore.sh \u0026#34;$DESIRED\u0026#34; || exit 1 /scripts/post-restore.sh \u0026#34;$NAME\u0026#34; || exit 1 update_status \u0026#34;Complete\u0026#34; \u0026#34;Restore finished\u0026#34; kubectl patch databaserestore $NAME --subresource=status --type=merge \\ -p \u0026#34;{\\\u0026#34;status\\\u0026#34;: {\\\u0026#34;lastAppliedBackup\\\u0026#34;: \\\u0026#34;$DESIRED\\\u0026#34;}}\u0026#34; fiCrossplane function calls the same scripts:\ndef compose(request): spec = request.observed.composite.spec status = request.observed.composite.status if spec.backupTimestamp == status.get(\u0026#39;lastAppliedBackup\u0026#39;): return response() # Already applied # Generate Job that runs the same scripts restore_job = { \u0026#34;apiVersion\u0026#34;: \u0026#34;batch/v1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Job\u0026#34;, \u0026#34;metadata\u0026#34;: {\u0026#34;name\u0026#34;: f\u0026#34;restore-{spec.backupTimestamp}\u0026#34;}, \u0026#34;spec\u0026#34;: { \u0026#34;template\u0026#34;: { \u0026#34;spec\u0026#34;: { \u0026#34;containers\u0026#34;: [{ \u0026#34;name\u0026#34;: \u0026#34;restore\u0026#34;, \u0026#34;image\u0026#34;: \u0026#34;your-restore-scripts:v1\u0026#34;, # Same image! \u0026#34;command\u0026#34;: [\u0026#34;/scripts/restore.sh\u0026#34;, spec.backupTimestamp] }] } } } } return response(resources=[restore_job])Step 4: Version Your \u0026ldquo;API\u0026rdquo;#\rAdd annotations to track what\u0026rsquo;s managing it:\napiVersion: ops.example.com/v1alpha1 kind: DatabaseRestore metadata: name: prod-restore annotations: ops.example.com/controller-version: \u0026#34;v1-bash\u0026#34; # or \u0026#34;v2-crossplane\u0026#34; ops.example.com/schema-version: \u0026#34;v1alpha1\u0026#34; spec: backupTimestamp: \u0026#34;2024-12-10T00:00:00Z\u0026#34; database: postgres-prodThis lets you:\nRun both controllers simultaneously during migration Have them ignore resources not meant for them Track which system handled which restore Step 5: Abstract the Controller Detection#\rBoth implementations watch the same resource but check if they should handle it:\nOption 3:\nshould_handle() { local version=$(kubectl get databaserestore $NAME \\ -o jsonpath=\u0026#39;{.metadata.annotations.ops\\.example\\.com/controller-version}\u0026#39;) # Handle if no version set (default) or explicitly v1-bash if [ -z \u0026#34;$version\u0026#34; ] || [ \u0026#34;$version\u0026#34; = \u0026#34;v1-bash\u0026#34; ]; then return 0 fi return 1 }Crossplane function:\ndef should_handle(resource): version = resource.metadata.annotations.get( \u0026#39;ops.example.com/controller-version\u0026#39;, \u0026#39;v2-crossplane\u0026#39; ) return version == \u0026#39;v2-crossplane\u0026#39;Migration Path#\rPhase 1: Start with Option 3#\r# deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: restore-controller-v1 spec: replicas: 1 template: spec: containers: - name: controller image: restore-controller:v1-bash env: - name: CONTROLLER_VERSION value: \u0026#34;v1-bash\u0026#34;Phase 2: Deploy Crossplane (No Changes Yet)#\r# Install Crossplane helm install crossplane --namespace crossplane-system \\ crossplane-stable/crossplane # Your Option 3 controller still handles everythingPhase 3: Create Crossplane XRD (Matches Your CRD)#\rapiVersion: apiextensions.crossplane.io/v1 kind: CompositeResourceDefinition metadata: name: xdatabaserestores.ops.example.com spec: group: ops.example.com names: kind: XDatabaseRestore plural: xdatabaserestores # IMPORTANT: Make it a \u0026#34;claim\u0026#34; for existing CRD claimNames: kind: DatabaseRestore # Same as your existing CRD! plural: databaserestores versions: - name: v1alpha1 # Same version! schema: # Same schema as your CRDPhase 4: Deploy Composition Function (Handles Only New Resources)#\rapiVersion: apiextensions.crossplane.io/v1 kind: Composition metadata: name: database-restore-v2 labels: crossplane.io/xrd: xdatabaserestores.ops.example.com spec: compositeTypeRef: apiVersion: ops.example.com/v1alpha1 kind: XDatabaseRestore mode: Pipeline pipeline: - step: restore functionRef: name: function-restore-v2Now you can test Crossplane with new resources:\napiVersion: ops.example.com/v1alpha1 kind: DatabaseRestore metadata: name: test-restore annotations: ops.example.com/controller-version: \u0026#34;v2-crossplane\u0026#34; # Route to Crossplane spec: backupTimestamp: \u0026#34;2024-12-11T00:00:00Z\u0026#34;Phase 5: Gradual Migration#\r# Migrate one resource at a time kubectl annotate databaserestore prod-restore \\ ops.example.com/controller-version=v2-crossplane # Or migrate all at once kubectl annotate databaserestore --all \\ ops.example.com/controller-version=v2-crossplanePhase 6: Decommission Option 3#\r# Scale down old controller kubectl scale deployment restore-controller-v1 --replicas=0 # Monitor for a week, then delete kubectl delete deployment restore-controller-v1Key Abstraction Points#\rResource schema - Same CRD/spec for both Status contract - Both update .status identically Implementation scripts - Shared container image Controller selection - Annotation-based routing Observability - Both emit same events/metrics File Structure for Clean Separation#\rrestore-system/\r‚îú‚îÄ‚îÄ crd/\r‚îÇ ‚îî‚îÄ‚îÄ databaserestore-crd.yaml # Shared CRD definition\r‚îú‚îÄ‚îÄ option3/\r‚îÇ ‚îú‚îÄ‚îÄ deployment.yaml # Bash controller\r‚îÇ ‚îú‚îÄ‚îÄ controller.sh # Main loop\r‚îÇ ‚îî‚îÄ‚îÄ Dockerfile\r‚îú‚îÄ‚îÄ crossplane/\r‚îÇ ‚îú‚îÄ‚îÄ xrd.yaml # XRD (references same CRD)\r‚îÇ ‚îú‚îÄ‚îÄ composition.yaml\r‚îÇ ‚îî‚îÄ‚îÄ function/\r‚îÇ ‚îú‚îÄ‚îÄ fn.py # Composition function\r‚îÇ ‚îî‚îÄ‚îÄ Dockerfile\r‚îî‚îÄ‚îÄ scripts/ # Shared by both!\r‚îú‚îÄ‚îÄ common.sh\r‚îú‚îÄ‚îÄ pre-restore.sh\r‚îú‚îÄ‚îÄ restore.sh\r‚îî‚îÄ‚îÄ post-restore.shTesting the Abstraction#\rYou can test both simultaneously:\n# Test Option 3 apiVersion: ops.example.com/v1alpha1 kind: DatabaseRestore metadata: name: test-bash annotations: ops.example.com/controller-version: \u0026#34;v1-bash\u0026#34; spec: backupTimestamp: \u0026#34;2024-12-10T00:00:00Z\u0026#34; --- # Test Crossplane (same API!) apiVersion: ops.example.com/v1alpha1 kind: DatabaseRestore metadata: name: test-crossplane annotations: ops.example.com/controller-version: \u0026#34;v2-crossplane\u0026#34; spec: backupTimestamp: \u0026#34;2024-12-10T00:00:00Z\u0026#34;Both use the same spec format, update status the same way, and call the same restore scripts. The only difference is which controller handles the reconciliation.\nThis approach gives you a clean migration path with no big-bang rewrite required. Does this abstraction strategy work for your needs?\n"},{"id":35,"href":"/musings/docs/misc/stage3-outline/","title":"Stage 3: Implementation Patterns - Outline (Crossplane 2.1)","section":"Miscellaneous","content":"Stage 3: Implementation Patterns - Outline (Crossplane 2.1)#\rPurpose#\rDescribe the structural and organizational patterns for each approach using Crossplane 2.1 features. Focus on how manifests are organized, how dependencies are handled, composition functions, and the key architectural decisions that differentiate each approach.\nProposed Outline Structure#\r3.1 Approach 1: Direct Managed Resources#\rManifest Organization File structure and naming conventions Dependency ordering strategy Namespace vs cluster-scoped resources Resource Reference Patterns Crossplane 2.1 reference resolution (matchControllerRef, resolvers) Match labels and selectors for dynamic references External name usage patterns Key Decisions When to use references vs match selectors Managing resource lifecycle and deletion policies Handling parallel vs sequential provisioning 3.2 Approach 2: Composite Resources (XRDs)#\rXRD Design (Crossplane 2.1) XR-only pattern (no Claims in 2.1) API surface design (what parameters to expose) Namespace-scoped XRs and their implications Composition Functions Function pipeline architecture (replacing patches) Using composition functions for resource generation Function chaining and data flow Composition Structure Modern composition mode (Pipeline vs Resources) Composition functions vs traditional patch-and-transform Connection secrets and composition references Key Decisions Traditional patches vs composition functions Function selection and ordering in pipeline Handling composition updates and versioning 3.3 Approach 3a: Opinionated Platform#\rProvider/Platform Selection Upbound Official Provider Families (recommended for 2.1) Pre-built Crossplane Configurations Provider family advantages over monolithic providers Configuration Packages Using Crossplane Configuration packages Dependency management between configurations Package versioning and updates Key Decisions Configuration package granularity Customization vs standardization trade-offs Package dependency management 3.4 Approach 3b: Multi-Cloud Abstraction#\rAbstraction Layer Design Cloud-agnostic XRD definitions Composition selection via environment configs Provider family approach for multi-cloud Composition Selectors Using compositionSelector and compositionRef Environment-based composition selection Label-based routing to cloud-specific compositions Key Decisions XRD API design for portability Composition selection strategy Handling cloud-specific features 3.5 Approach 3c: Fargate-First Architecture#\rFargate-Specific Resources Fargate Profile managed resources Pod Execution Role configuration OIDC provider for pod identities Composition Adaptations Conditional resource rendering (via functions) Fargate profile selectors and namespaces Key Decisions Namespace-based vs label-based Fargate selection Handling workloads not suitable for Fargate IRSA configuration for Fargate pods 3.6 Approach 3d: vCluster-Based Multi-Tenancy#\rvCluster Provider Integration Using provider-helm or provider-kubernetes for vCluster vCluster Helm chart deployment via Crossplane vCluster CRD management Management Cluster Pattern Standard EKS with Crossplane 2.1 Provider configuration for in-cluster resources Resource sizing and quotas Workload vCluster Composition XRD for vCluster provisioning Composition function for vCluster configuration Network and storage configuration Key Decisions vCluster deployment mechanism (Helm vs native) Host cluster provider configuration vCluster isolation and resource limits 3.7 Approach 3e: Kubernetes-Native Tooling#\rTooling Deployment Strategy Using provider-helm for K8s-native tools Using provider-kubernetes for CRD-based tools Composition functions for tool configuration Tool Stack Vault (provider-helm for deployment) Harbor (provider-helm for deployment) cert-manager (provider-kubernetes for CRDs) NGINX Ingress (provider-helm) external-dns (provider-helm) Integration Patterns Tool dependencies and ordering Configuration secrets management AWS integration (S3 for Harbor, Route53 for DNS) Key Decisions Crossplane-managed vs externally-managed tools Tool lifecycle and upgrade management Composition functions for tool configuration 3.8 Approach 3f: ECS Control Plane Alternative#\rECS-Based Crossplane Deployment ECS Task Definition for Crossplane Fargate vs EC2 launch type Persistent storage for Crossplane (EFS or S3) Provider Configuration in ECS AWS credentials via IAM task roles ProviderConfig for ECS-hosted Crossplane Network configuration for ECS tasks State Management etcd as sidecar vs external Backup and restore strategies Key Decisions State persistence approach Crossplane version and upgrade path Observability for ECS-based control plane Crossplane 2.1 Specific Patterns#\rComposition Functions#\rFunction pipeline architecture When to use functions vs patches Available function types (Go templates, patch-and-transform, etc.) Provider Families#\rOfficial Provider Families vs monolithic providers Upjet/Terraform-based providers in 2.1 Provider configuration and credentials Resource Management#\rDeletion policies and management policies Connection secrets in 2.1 ProviderConfig references and inheritance Namespace Scoping#\rXR namespace scoping implications Cross-namespace resource references RBAC considerations for namespace-scoped resources General Patterns Across All Approaches#\rProvider Configuration (2.1 Style)#\rProviderConfig with credentials DeploymentRuntimeConfig for provider settings Provider package installation and configuration Workload Cluster Provisioning#\rTemplate structure for each approach Composition function usage for templating Namespace organization and RBAC Review Questions#\rBefore I flesh out the details:\nComposition Functions: Should I recommend specific composition functions (e.g., function-patch-and-transform, function-go-templating) or keep it generic?\nProvider Selection: For approach 3a (Opinionated), should I recommend Upbound\u0026rsquo;s official provider families and specific Configuration packages?\nLevel of Detail: Should I include conceptual manifest snippets showing key Crossplane 2.1 features (composition functions, match selectors, etc.) or keep it purely descriptive?\nFunction Pipeline Examples: Should I show the structure of composition function pipelines or just describe them conceptually?\nWorkload Templates: Should Stage 3 discuss the workload cluster templates for each approach, or save that entirely for Stage 5 (Reference Implementation)?\n"},{"id":36,"href":"/musings/docs/devplatform/","title":"Development Platforms","section":"Documentation","content":"Development Platforms#\rBrowse all documents in the Development Platforms category.\n"},{"id":37,"href":"/musings/docs/misc/argocd-layered-values/","title":"ArgoCD and Helm Schema Validation","section":"Miscellaneous","content":"ArgoCD and Helm Schema Validation#\rShort answer: Yes, but with important caveats about when and how.\nHow ArgoCD Uses Helm Schema Validation#\rArgoCD uses the native Helm client under the hood, which means:\nDuring sync, ArgoCD runs essentially:\nhelm template \u0026lt;release\u0026gt; \u0026lt;chart\u0026gt; -f values.yaml -f overrides.yaml Helm\u0026rsquo;s schema validation happens as part of template rendering\nIf schema validation fails, the sync fails and ArgoCD reports the error\nSo yes, ArgoCD respects values.schema.json.\nThe Critical Timing Difference#\rStandalone Helm:#\rhelm install myapp ./chart -f values.yaml Schema validation happens before any Kubernetes API calls Fail fast: Bad values = immediate error, nothing deployed ArgoCD:#\r# ArgoCD syncs the application Schema validation happens during the sync phase Might reach DEGRADED state if validation fails Could have partial resources created before failure (depending on sync waves) Where This Matters for Your Layered Values#\rScenario 1: Values in ArgoCD Application Manifest#\rapiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: myapp spec: source: repoURL: https://github.com/org/charts path: charts/myapp helm: valueFiles: - values.yaml - platform.yaml # Your override fileWhat happens:\nArgoCD passes both files to Helm in order Helm merges them: values.yaml + platform.yaml Schema validates the merged result ‚úÖ Works exactly like running helm install -f values.yaml -f platform.yaml Scenario 2: Values in Different Repos/ConfigMaps#\rThis is where it gets tricky with your layered approach.\nPattern A: Values File + ArgoCD Parameters#\rapiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: myapp spec: source: helm: valueFiles: - values.yaml parameters: - name: database.password value: \u0026#34;secret-from-vault\u0026#34;What happens:\nHelm merges values.yaml + parameters Schema validation sees merged result ‚úÖ If database.password is in parameters, validation passes Pattern B: Values in External ConfigMap/Secret#\rapiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: myapp spec: source: helm: valueFiles: - values.yaml - secrets://mysecret/platform-values # External valuesWhat happens:\nArgoCD resolves the secret/configmap Passes both files to Helm ‚úÖ Schema validates merged result ArgoCD-Specific Considerations#\r1. Server-Side vs. Client-Side Rendering#\rArgoCD has two modes:\nClient-Side (Default):#\rspec: source: helm: skipCrds: false Helm runs on the ArgoCD server Schema validation happens on ArgoCD server Same behavior as local helm template Server-Side:#\rspec: source: helm: skipCrds: true syncPolicy: syncOptions: - ServerSideApply=true Kubernetes handles the apply But Helm still renders the template first Schema validation still happens 2. Multi-Source Applications (ArgoCD 2.6+)#\rThis is game-changing for your layered values pattern:\napiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: myapp spec: sources: # Source 1: The Helm chart - repoURL: https://github.com/org/charts path: charts/myapp targetRevision: main helm: valueFiles: - values.yaml - $values/platform-config/production/platform.yaml # Source 2: Platform values repository - repoURL: https://github.com/org/platform-config targetRevision: main ref: valuesWhat happens:\nArgoCD fetches both repos Merges values from both sources Passes merged values to Helm ‚úÖ Schema validates the final merged result This solves the \u0026ldquo;values in different repos\u0026rdquo; problem cleanly.\n3. App of Apps Pattern#\r# Root application apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: platform-apps spec: source: repoURL: https://github.com/org/platform path: applicationsWhere applications/ contains:\n# applications/myapp.yaml apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: myapp spec: source: helm: valueFiles: - ../../base-values/values.yaml - ../../platform-values/production/platform.yamlWhat happens:\nEach child app has its value file references Helm merges them per application ‚úÖ Schema validation works per app When Schema Validation Fails in ArgoCD#\rThe Error Flow:#\rArgoCD detects out of sync Attempts sync Helm renders template (schema validation happens here) Schema validation fails ArgoCD shows error: ComparisonError\rrpc error: code = Unknown desc = Helm template rendering failed: values don\u0026#39;t meet the specifications of the schema(s) in the following chart(s):\rmyapp:\r- database.password: Invalid type. Expected: string, given: null Application status: OutOfSync and Degraded No resources applied (template rendering failed before apply) Key Differences from Standalone Helm#\rAspect Standalone Helm ArgoCD with Helm When validation runs Before any K8s calls During sync phase Failure impact Installation aborted Sync fails, app degraded Retry behavior Manual only Auto-retry if configured Error visibility CLI output ArgoCD UI + notifications Values source Local files only Repos, ConfigMaps, Secrets, Plugins Multi-source values Not supported Supported (v2.6+) Best Practices for ArgoCD + Layered Values#\r1. Use Multi-Source for Clean Separation#\rsources: - repoURL: \u0026lt;chart-repo\u0026gt; helm: valueFiles: - values.yaml # Base - $platform/production.yaml # Platform layer - repoURL: \u0026lt;platform-repo\u0026gt; ref: platform2. Test Locally First#\r# Simulate what ArgoCD will do helm template myapp ./chart \\ -f values.yaml \\ -f ../platform-config/production/platform.yaml3. Use ArgoCD Application Sets for Multiple Environments#\rapiVersion: argoproj.io/v1alpha1 kind: ApplicationSet metadata: name: myapp-environments spec: generators: - list: elements: - env: dev platform: dev-platform.yaml - env: staging platform: staging-platform.yaml - env: production platform: production-platform.yaml template: spec: source: helm: valueFiles: - values.yaml - \u0026#39;{{platform}}\u0026#39;Each environment gets the right platform.yaml, schema validates each separately.\n4. Configure Sync Policies Carefully#\rspec: syncPolicy: automated: prune: true selfHeal: true syncOptions: - CreateNamespace=true - PruneLast=true retry: limit: 5 backoff: duration: 5s factor: 2 maxDuration: 3mIf schema validation fails, ArgoCD will retry based on this config.\nMonitoring Schema Validation in ArgoCD#\rArgoCD Notifications for Schema Failures#\rapiVersion: v1 kind: ConfigMap metadata: name: argocd-notifications-cm data: trigger.on-schema-error: | - when: app.status.operationState.phase == \u0026#39;Error\u0026#39; \u0026amp;\u0026amp; app.status.operationState.message contains \u0026#39;schema\u0026#39; send: [slack-platform-team] template.schema-error: | message: | üö® Schema Validation Failed App: {{.app.metadata.name}} Error: {{.app.status.operationState.message}} This usually means platform.yaml is missing required overrides. Check: https://wiki.company.com/platform-values-guidePrometheus Metrics#\r# Count of apps with schema errors argocd_app_info{sync_status=\u0026#34;OutOfSync\u0026#34;, health_status=\u0026#34;Degraded\u0026#34;} and on(name) argocd_app_sync_total{phase=\u0026#34;Error\u0026#34;} The Bottom Line#\rYes, ArgoCD uses Helm schema validation, and it works exactly like running helm template with your merged values files.\nFor your layered values approach:\n‚úÖ ArgoCD merges values.yaml + platform.yaml before validation ‚úÖ Schema sees the final merged result ‚úÖ Works the same as standalone Helm ‚úÖ Multi-source apps (ArgoCD 2.6+) make this pattern clean ‚ö†Ô∏è Validation failures show in ArgoCD UI, not CLI ‚ö†Ô∏è Set up notifications for schema errors ‚ö†Ô∏è Test locally first to catch errors before committing The schema strategy I showed you (EXAMPLE-13 with sentinel values) works perfectly with ArgoCD because:\nChart packages with sentinel values in values.yaml ArgoCD merges values.yaml + platform.yaml Helm validates merged result Schema catches any remaining sentinel values (= missing overrides) Clear error message in ArgoCD UI No special ArgoCD configuration needed‚Äîit just works.\n"},{"id":38,"href":"/musings/docs/misc/schema-testing/","title":"Creating a Helm Values Schema: Complete Guide","section":"Miscellaneous","content":"Creating a Helm Values Schema: Complete Guide#\rThe values schema (values.schema.json) is a JSON Schema that validates your values.yaml. It catches mistakes before deployment.\nQuick Start: Generate Schema from Existing values.yaml#\rMethod 1: Using helm-schema (Fastest)#\r# Install npm install -g @bitnami/helm-schema # Generate schema helm-schema generate values.yaml \u0026gt; values.schema.json # Or for Go users go install github.com/dadav/helm-schema/cmd/helm-schema@latest helm-schema -input values.yaml -output values.schema.jsonThis gives you 80% of what you need immediately. Then you refine it.\nMethod 2: Manual Creation (Full Control)#\rLet\u0026rsquo;s say your values.yaml looks like this:\n# values.yaml replicaCount: 1 image: repository: nginx tag: \u0026#34;1.21\u0026#34; pullPolicy: IfNotPresent service: type: ClusterIP port: 80 ingress: enabled: false className: nginx hosts: - host: chart-example.local paths: - path: / pathType: Prefix resources: limits: cpu: 100m memory: 128Mi requests: cpu: 100m memory: 128Mi autoscaling: enabled: false minReplicas: 1 maxReplicas: 100 Building the Schema Step-by-Step#\rStep 1: Basic Structure#\r{ \u0026#34;$schema\u0026#34;: \u0026#34;http://json-schema.org/draft-07/schema#\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;My App Helm Chart Values\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { } } Step 2: Simple Properties (Primitives)#\r{ \u0026#34;$schema\u0026#34;: \u0026#34;http://json-schema.org/draft-07/schema#\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;replicaCount\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34;, \u0026#34;minimum\u0026#34;: 1, \u0026#34;maximum\u0026#34;: 100, \u0026#34;description\u0026#34;: \u0026#34;Number of replicas to deploy\u0026#34; } } }What this validates:\n# ‚úÖ Valid helm install my-app . --set replicaCount=3 # ‚ùå Invalid - not an integer helm install my-app . --set replicaCount=1.5 # Error: values don\u0026#39;t meet the specifications of the schema # ‚ùå Invalid - out of range helm install my-app . --set replicaCount=200 # Error: replicaCount must be \u0026lt;= 100 Step 3: Nested Objects#\r{ \u0026#34;properties\u0026#34;: { \u0026#34;image\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;required\u0026#34;: [\u0026#34;repository\u0026#34;, \u0026#34;tag\u0026#34;], \u0026#34;properties\u0026#34;: { \u0026#34;repository\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;minLength\u0026#34;: 1, \u0026#34;description\u0026#34;: \u0026#34;Container image repository\u0026#34; }, \u0026#34;tag\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;pattern\u0026#34;: \u0026#34;^[a-zA-Z0-9._-]+$\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Container image tag\u0026#34; }, \u0026#34;pullPolicy\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;enum\u0026#34;: [\u0026#34;Always\u0026#34;, \u0026#34;IfNotPresent\u0026#34;, \u0026#34;Never\u0026#34;], \u0026#34;description\u0026#34;: \u0026#34;Image pull policy\u0026#34; } } } } }What this validates:\n# ‚úÖ Valid helm install my-app . \\ --set image.repository=myapp \\ --set image.tag=v1.0.0 # ‚ùå Invalid - missing required field helm install my-app . --set image.repository=myapp # Error: image requires property \u0026#34;tag\u0026#34; # ‚ùå Invalid - invalid enum value helm install my-app . --set image.pullPolicy=Sometimes # Error: image.pullPolicy must be one of: Always, IfNotPresent, Never # ‚ùå Invalid - invalid tag format helm install my-app . --set image.tag=\u0026#34;1.0.0 beta\u0026#34; # Error: image.tag must match pattern ^[a-zA-Z0-9._-]+$ Step 4: Arrays#\r{ \u0026#34;properties\u0026#34;: { \u0026#34;ingress\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;hosts\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;array\u0026#34;, \u0026#34;items\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;required\u0026#34;: [\u0026#34;host\u0026#34;], \u0026#34;properties\u0026#34;: { \u0026#34;host\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;hostname\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Hostname for ingress\u0026#34; }, \u0026#34;paths\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;array\u0026#34;, \u0026#34;minItems\u0026#34;: 1, \u0026#34;items\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;required\u0026#34;: [\u0026#34;path\u0026#34;, \u0026#34;pathType\u0026#34;], \u0026#34;properties\u0026#34;: { \u0026#34;path\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;pattern\u0026#34;: \u0026#34;^/\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;URL path (must start with /)\u0026#34; }, \u0026#34;pathType\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;enum\u0026#34;: [\u0026#34;Exact\u0026#34;, \u0026#34;Prefix\u0026#34;, \u0026#34;ImplementationSpecific\u0026#34;] } } } } } } } } } } }What this validates:\n# ‚úÖ Valid helm install my-app . \\ --set \u0026#39;ingress.hosts[0].host=example.com\u0026#39; \\ --set \u0026#39;ingress.hosts[0].paths[0].path=/\u0026#39; \\ --set \u0026#39;ingress.hosts[0].paths[0].pathType=Prefix\u0026#39; # ‚ùå Invalid - path doesn\u0026#39;t start with / helm install my-app . \\ --set \u0026#39;ingress.hosts[0].paths[0].path=api\u0026#39; # Error: path must match pattern ^/ # ‚ùå Invalid - invalid pathType helm install my-app . \\ --set \u0026#39;ingress.hosts[0].paths[0].pathType=Wildcard\u0026#39; # Error: pathType must be one of: Exact, Prefix, ImplementationSpecific Step 5: Conditional Validation (Advanced)#\rProblem: When autoscaling is enabled, minReplicas and maxReplicas should be required.\n{ \u0026#34;properties\u0026#34;: { \u0026#34;autoscaling\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;enabled\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;boolean\u0026#34; }, \u0026#34;minReplicas\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34;, \u0026#34;minimum\u0026#34;: 1 }, \u0026#34;maxReplicas\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34;, \u0026#34;minimum\u0026#34;: 1 } }, \u0026#34;if\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;enabled\u0026#34;: { \u0026#34;const\u0026#34;: true } } }, \u0026#34;then\u0026#34;: { \u0026#34;required\u0026#34;: [\u0026#34;minReplicas\u0026#34;, \u0026#34;maxReplicas\u0026#34;], \u0026#34;properties\u0026#34;: { \u0026#34;maxReplicas\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34;, \u0026#34;minimum\u0026#34;: { \u0026#34;$data\u0026#34;: \u0026#34;1/minReplicas\u0026#34; } } } } } } }What this validates:\n# ‚úÖ Valid - autoscaling disabled, no replicas needed helm install my-app . --set autoscaling.enabled=false # ‚ùå Invalid - enabled but missing minReplicas helm install my-app . \\ --set autoscaling.enabled=true \\ --set autoscaling.maxReplicas=10 # Error: when autoscaling.enabled=true, minReplicas is required # ‚ùå Invalid - maxReplicas \u0026lt; minReplicas helm install my-app . \\ --set autoscaling.enabled=true \\ --set autoscaling.minReplicas=10 \\ --set autoscaling.maxReplicas=5 # Error: maxReplicas must be \u0026gt;= minReplicas Step 6: Cross-Field Validation#\rProblem: When ingress.enabled=true, you must set service.type=ClusterIP\n{ \u0026#34;properties\u0026#34;: { \u0026#34;ingress\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;enabled\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;boolean\u0026#34; } } }, \u0026#34;service\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;type\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;enum\u0026#34;: [\u0026#34;ClusterIP\u0026#34;, \u0026#34;NodePort\u0026#34;, \u0026#34;LoadBalancer\u0026#34;] } } } }, \u0026#34;if\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;ingress\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;enabled\u0026#34;: { \u0026#34;const\u0026#34;: true } } } } }, \u0026#34;then\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;service\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;type\u0026#34;: { \u0026#34;const\u0026#34;: \u0026#34;ClusterIP\u0026#34; } } } } } } Complete Real-World Example#\rHere\u0026rsquo;s a production-ready schema:\n{ \u0026#34;$schema\u0026#34;: \u0026#34;http://json-schema.org/draft-07/schema#\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;My Application Helm Chart Values\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;required\u0026#34;: [\u0026#34;image\u0026#34;], \u0026#34;properties\u0026#34;: { \u0026#34;replicaCount\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34;, \u0026#34;minimum\u0026#34;: 1, \u0026#34;maximum\u0026#34;: 100, \u0026#34;default\u0026#34;: 1, \u0026#34;description\u0026#34;: \u0026#34;Number of pod replicas\u0026#34; }, \u0026#34;image\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;required\u0026#34;: [\u0026#34;repository\u0026#34;, \u0026#34;tag\u0026#34;], \u0026#34;properties\u0026#34;: { \u0026#34;repository\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;minLength\u0026#34;: 1, \u0026#34;description\u0026#34;: \u0026#34;Container image repository\u0026#34;, \u0026#34;examples\u0026#34;: [\u0026#34;nginx\u0026#34;, \u0026#34;myregistry.io/myapp\u0026#34;] }, \u0026#34;tag\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;pattern\u0026#34;: \u0026#34;^[a-zA-Z0-9._-]+$\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Container image tag (no spaces allowed)\u0026#34; }, \u0026#34;pullPolicy\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;enum\u0026#34;: [\u0026#34;Always\u0026#34;, \u0026#34;IfNotPresent\u0026#34;, \u0026#34;Never\u0026#34;], \u0026#34;default\u0026#34;: \u0026#34;IfNotPresent\u0026#34; } }, \u0026#34;additionalProperties\u0026#34;: false }, \u0026#34;service\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;type\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;enum\u0026#34;: [\u0026#34;ClusterIP\u0026#34;, \u0026#34;NodePort\u0026#34;, \u0026#34;LoadBalancer\u0026#34;, \u0026#34;ExternalName\u0026#34;], \u0026#34;default\u0026#34;: \u0026#34;ClusterIP\u0026#34; }, \u0026#34;port\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34;, \u0026#34;minimum\u0026#34;: 1, \u0026#34;maximum\u0026#34;: 65535, \u0026#34;default\u0026#34;: 80 } } }, \u0026#34;ingress\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;enabled\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;boolean\u0026#34;, \u0026#34;default\u0026#34;: false }, \u0026#34;className\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Ingress class name\u0026#34; }, \u0026#34;annotations\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;additionalProperties\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } }, \u0026#34;hosts\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;array\u0026#34;, \u0026#34;minItems\u0026#34;: 1, \u0026#34;items\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;required\u0026#34;: [\u0026#34;host\u0026#34;], \u0026#34;properties\u0026#34;: { \u0026#34;host\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;hostname\u0026#34; }, \u0026#34;paths\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;array\u0026#34;, \u0026#34;minItems\u0026#34;: 1, \u0026#34;items\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;required\u0026#34;: [\u0026#34;path\u0026#34;, \u0026#34;pathType\u0026#34;], \u0026#34;properties\u0026#34;: { \u0026#34;path\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;pattern\u0026#34;: \u0026#34;^/\u0026#34; }, \u0026#34;pathType\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;enum\u0026#34;: [\u0026#34;Exact\u0026#34;, \u0026#34;Prefix\u0026#34;, \u0026#34;ImplementationSpecific\u0026#34;] } } } } } } }, \u0026#34;tls\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;array\u0026#34;, \u0026#34;items\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;required\u0026#34;: [\u0026#34;secretName\u0026#34;], \u0026#34;properties\u0026#34;: { \u0026#34;secretName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;minLength\u0026#34;: 1 }, \u0026#34;hosts\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;array\u0026#34;, \u0026#34;items\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;hostname\u0026#34; } } } } } }, \u0026#34;if\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;enabled\u0026#34;: { \u0026#34;const\u0026#34;: true } } }, \u0026#34;then\u0026#34;: { \u0026#34;required\u0026#34;: [\u0026#34;hosts\u0026#34;] } }, \u0026#34;resources\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;limits\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;cpu\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;pattern\u0026#34;: \u0026#34;^[0-9]+(m|[0-9])*$\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;CPU limit (e.g., 100m, 1, 2)\u0026#34; }, \u0026#34;memory\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;pattern\u0026#34;: \u0026#34;^[0-9]+(Mi|Gi|M|G)$\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Memory limit (e.g., 128Mi, 1Gi)\u0026#34; } } }, \u0026#34;requests\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;cpu\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;pattern\u0026#34;: \u0026#34;^[0-9]+(m|[0-9])*$\u0026#34; }, \u0026#34;memory\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;pattern\u0026#34;: \u0026#34;^[0-9]+(Mi|Gi|M|G)$\u0026#34; } } } } }, \u0026#34;autoscaling\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;enabled\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;boolean\u0026#34;, \u0026#34;default\u0026#34;: false }, \u0026#34;minReplicas\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34;, \u0026#34;minimum\u0026#34;: 1, \u0026#34;maximum\u0026#34;: 100 }, \u0026#34;maxReplicas\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34;, \u0026#34;minimum\u0026#34;: 1, \u0026#34;maximum\u0026#34;: 1000 }, \u0026#34;targetCPUUtilizationPercentage\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34;, \u0026#34;minimum\u0026#34;: 1, \u0026#34;maximum\u0026#34;: 100 } }, \u0026#34;if\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;enabled\u0026#34;: { \u0026#34;const\u0026#34;: true } } }, \u0026#34;then\u0026#34;: { \u0026#34;required\u0026#34;: [\u0026#34;minReplicas\u0026#34;, \u0026#34;maxReplicas\u0026#34;] } }, \u0026#34;nodeSelector\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;additionalProperties\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } }, \u0026#34;tolerations\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;array\u0026#34;, \u0026#34;items\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;key\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;operator\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;enum\u0026#34;: [\u0026#34;Exists\u0026#34;, \u0026#34;Equal\u0026#34;] }, \u0026#34;value\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;effect\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;enum\u0026#34;: [\u0026#34;NoSchedule\u0026#34;, \u0026#34;PreferNoSchedule\u0026#34;, \u0026#34;NoExecute\u0026#34;] } } } }, \u0026#34;affinity\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34; } }, \u0026#34;additionalProperties\u0026#34;: false } Advanced Patterns#\rPattern 1: OneOf (Mutually Exclusive Options)#\rUse case: Either use existingSecret OR provide username/password, but not both.\n{ \u0026#34;properties\u0026#34;: { \u0026#34;database\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;oneOf\u0026#34;: [ { \u0026#34;required\u0026#34;: [\u0026#34;existingSecret\u0026#34;], \u0026#34;properties\u0026#34;: { \u0026#34;existingSecret\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;minLength\u0026#34;: 1 } } }, { \u0026#34;required\u0026#34;: [\u0026#34;username\u0026#34;, \u0026#34;password\u0026#34;], \u0026#34;properties\u0026#34;: { \u0026#34;username\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;minLength\u0026#34;: 1 }, \u0026#34;password\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;minLength\u0026#34;: 8 } } } ] } } }Pattern 2: AnyOf (At Least One Required)#\rUse case: Must specify at least one of: httpGet, exec, or tcpSocket for liveness probe.\n{ \u0026#34;properties\u0026#34;: { \u0026#34;livenessProbe\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;anyOf\u0026#34;: [ { \u0026#34;required\u0026#34;: [\u0026#34;httpGet\u0026#34;] }, { \u0026#34;required\u0026#34;: [\u0026#34;exec\u0026#34;] }, { \u0026#34;required\u0026#34;: [\u0026#34;tcpSocket\u0026#34;] } ], \u0026#34;properties\u0026#34;: { \u0026#34;httpGet\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;required\u0026#34;: [\u0026#34;path\u0026#34;, \u0026#34;port\u0026#34;], \u0026#34;properties\u0026#34;: { \u0026#34;path\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;port\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34; } } }, \u0026#34;exec\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;required\u0026#34;: [\u0026#34;command\u0026#34;], \u0026#34;properties\u0026#34;: { \u0026#34;command\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;array\u0026#34;, \u0026#34;items\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } } } }, \u0026#34;tcpSocket\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;required\u0026#34;: [\u0026#34;port\u0026#34;], \u0026#34;properties\u0026#34;: { \u0026#34;port\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34; } } } } } } }Pattern 3: AllOf (Multiple Constraints)#\rUse case: Production deployments must have both resource limits AND pod disruption budget.\n{ \u0026#34;properties\u0026#34;: { \u0026#34;environment\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;enum\u0026#34;: [\u0026#34;dev\u0026#34;, \u0026#34;staging\u0026#34;, \u0026#34;production\u0026#34;] } }, \u0026#34;allOf\u0026#34;: [ { \u0026#34;if\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;environment\u0026#34;: { \u0026#34;const\u0026#34;: \u0026#34;production\u0026#34; } } }, \u0026#34;then\u0026#34;: { \u0026#34;required\u0026#34;: [\u0026#34;resources\u0026#34;], \u0026#34;properties\u0026#34;: { \u0026#34;resources\u0026#34;: { \u0026#34;required\u0026#34;: [\u0026#34;limits\u0026#34;, \u0026#34;requests\u0026#34;] }, \u0026#34;podDisruptionBudget\u0026#34;: { \u0026#34;required\u0026#34;: [\u0026#34;enabled\u0026#34;], \u0026#34;properties\u0026#34;: { \u0026#34;enabled\u0026#34;: { \u0026#34;const\u0026#34;: true } } } } } } ] } Testing Your Schema#\rManual Testing#\r# Test with valid values helm install my-app . --dry-run # Test with invalid values helm install my-app . \\ --set replicaCount=1000 \\ --dry-run # Expected: Error about replicaCount max value # Test missing required fields helm install my-app . \\ --set image.repository=nginx \\ --dry-run # Missing image.tag # Expected: Error about required fieldAutomated Testing#\r# Create test values files cat \u0026gt; test-values/valid.yaml \u0026lt;\u0026lt;EOF image: repository: nginx tag: \u0026#34;1.21\u0026#34; replicaCount: 3 EOF cat \u0026gt; test-values/invalid-replicas.yaml \u0026lt;\u0026lt;EOF image: repository: nginx tag: \u0026#34;1.21\u0026#34; replicaCount: 200 # Exceeds maximum EOF cat \u0026gt; test-values/invalid-missing-tag.yaml \u0026lt;\u0026lt;EOF image: repository: nginx # Missing required tag field EOFTest script:\n#!/bin/bash set -e echo \u0026#34;Testing valid values...\u0026#34; helm template my-app . -f test-values/valid.yaml \u0026gt; /dev/null echo \u0026#34;‚úì Valid values passed\u0026#34; echo \u0026#34;Testing invalid replica count...\u0026#34; if helm template my-app . -f test-values/invalid-replicas.yaml 2\u0026gt;\u0026amp;1 | grep -q \u0026#34;values don\u0026#39;t meet\u0026#34;; then echo \u0026#34;‚úì Invalid replicas correctly rejected\u0026#34; else echo \u0026#34;‚úó Invalid replicas should have been rejected\u0026#34; exit 1 fi echo \u0026#34;Testing missing required field...\u0026#34; if helm template my-app . -f test-values/invalid-missing-tag.yaml 2\u0026gt;\u0026amp;1 | grep -q \u0026#34;required\u0026#34;; then echo \u0026#34;‚úì Missing tag correctly rejected\u0026#34; else echo \u0026#34;‚úó Missing tag should have been rejected\u0026#34; exit 1 fi echo \u0026#34;All schema tests passed!\u0026#34; Common Gotchas#\r1. JSON vs YAML Types#\r// ‚ùå WRONG - This is YAML null, not JSON null { \u0026#34;default\u0026#34;: null } // ‚úÖ CORRECT - Omit the default or use a valid value { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34; }2. Pattern Anchoring#\r// ‚ùå WRONG - Matches \u0026#34;abc123def\u0026#34; { \u0026#34;pattern\u0026#34;: \u0026#34;[0-9]+\u0026#34; } // ‚úÖ CORRECT - Only matches pure numbers { \u0026#34;pattern\u0026#34;: \u0026#34;^[0-9]+$\u0026#34; }3. Resource Quantity Validation#\r// ‚ùå WRONG - \u0026#34;500\u0026#34; is valid, \u0026#34;500m\u0026#34; is not { \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34; } // ‚úÖ CORRECT - Accepts both \u0026#34;500m\u0026#34; and \u0026#34;1\u0026#34; { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;pattern\u0026#34;: \u0026#34;^[0-9]+(m|[0-9])*$\u0026#34; }4. Empty Arrays#\r// ‚ùå Allows empty array { \u0026#34;type\u0026#34;: \u0026#34;array\u0026#34; } // ‚úÖ Requires at least one item { \u0026#34;type\u0026#34;: \u0026#34;array\u0026#34;, \u0026#34;minItems\u0026#34;: 1 } Quick Reference: Common Validations#\r{ \u0026#34;properties\u0026#34;: { // String with length constraints \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;minLength\u0026#34;: 1, \u0026#34;maxLength\u0026#34;: 63 }, // Email format \u0026#34;email\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;email\u0026#34; }, // Hostname format \u0026#34;host\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;hostname\u0026#34; }, // Integer with range \u0026#34;port\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34;, \u0026#34;minimum\u0026#34;: 1, \u0026#34;maximum\u0026#34;: 65535 }, // Enum (limited choices) \u0026#34;logLevel\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;enum\u0026#34;: [\u0026#34;debug\u0026#34;, \u0026#34;info\u0026#34;, \u0026#34;warn\u0026#34;, \u0026#34;error\u0026#34;] }, // Boolean \u0026#34;enabled\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;boolean\u0026#34;, \u0026#34;default\u0026#34;: false }, // URL \u0026#34;webhookUrl\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;uri\u0026#34;, \u0026#34;pattern\u0026#34;: \u0026#34;^https://\u0026#34; }, // Kubernetes resource quantity \u0026#34;memory\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;pattern\u0026#34;: \u0026#34;^[0-9]+(Mi|Gi|M|G|Ki|K|Ti|T|Pi|P|Ei|E)$\u0026#34; }, // Duration \u0026#34;timeout\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;pattern\u0026#34;: \u0026#34;^[0-9]+(s|m|h)$\u0026#34; } } } Your Action Plan#\rToday: Generate initial schema with helm-schema This week: Add validation for top 5 most critical values This month: Add conditional validation and cross-field rules Ongoing: Update schema whenever you change values.yaml Start simple, iterate based on real bugs you find!\n"},{"id":39,"href":"/musings/docs/misc/stage1/","title":"Crossplane EKS Management Cluster - Approach Overview \u0026 Comparison","section":"Miscellaneous","content":"Crossplane EKS Management Cluster - Approach Overview \u0026amp; Comparison#\rApproach 1: Direct Managed Resources#\rThis approach uses individual Crossplane Managed Resources that map directly to AWS APIs. Each AWS resource (VPC, Subnet, InternetGateway, RouteTable, SecurityGroup, IAM Role, EKS Cluster, NodeGroup, etc.) is defined as a separate Kubernetes manifest.\nCharacteristics#\rOne-to-one mapping between Crossplane resources and AWS resources All resource properties are explicitly defined Dependencies managed via Kubernetes references or external-name annotations No abstraction layer between user and AWS API Pros#\rMaximum transparency - what you see is what you get Direct debugging - AWS console matches manifests exactly No \u0026ldquo;magic\u0026rdquo; - all configuration is explicit Easier for teams already familiar with AWS APIs Fine-grained control over every resource property Cons#\rHighly verbose - a single EKS cluster requires 15-20+ separate resource definitions Repetitive - similar patterns repeated for each cluster Error-prone - manual management of dependencies and references No guardrails - easy to create insecure or non-compliant configurations Difficult to maintain consistency across multiple clusters Changes require updating many individual resources Approach 2: Composite Resources (XRDs)#\rThis approach creates custom Kubernetes APIs (Composite Resource Definitions) that abstract away the underlying AWS resources. Users interact with high-level resources like EKSCluster which internally create all necessary AWS infrastructure.\nCharacteristics#\rCustom CRDs define the user-facing API Compositions define how XRDs map to underlying Managed Resources Users provide minimal parameters; defaults and best practices are encoded Platform team maintains the abstractions Pros#\rSimplified interface - users provide 5-10 parameters instead of hundreds Enforced best practices - security, networking, compliance built into compositions Reusability - same composition used for all clusters Consistency - all clusters follow the same patterns Easier evolution - change composition once, affects all future clusters Clear separation between platform and application teams Cons#\rAdditional abstraction layer to understand and maintain Learning curve for XRD/Composition concepts Debugging requires understanding both the XRD and underlying resources Less flexibility for edge cases - may need escape hatches Initial development overhead to create good abstractions Versioning and migration complexity when changing compositions Approach 3: Alternative Architectural Patterns#\rThis approach explores fundamentally different architectures enabled by various Crossplane providers and integrations. Each sub-approach represents a distinct philosophy or architectural pattern.\n3a: Opinionated Platform Provider#\rPhilosophy: Encode organizational best practices and compliance requirements into pre-built, opinionated abstractions.\nCharacteristics:\nPre-configured compositions with sensible defaults Built-in security and compliance patterns Reduced decision fatigue - most choices already made Often includes observability and policy enforcement Pros:\nFastest time to value - minimal configuration needed Best practices built-in by default Reduced surface area for configuration errors Good for teams new to cloud infrastructure Cons:\nLess flexibility - harder to deviate from opinions May not match your organization\u0026rsquo;s specific requirements Potential vendor lock-in to specific patterns Learning curve to understand what\u0026rsquo;s happening under the hood 3b: Multi-Cloud Abstraction Provider#\rPhilosophy: Abstract away cloud-specific details to enable portability across AWS, GCP, Azure, etc.\nCharacteristics:\nCloud-agnostic API definitions Provider-specific implementations behind common interface Focus on portable primitives (compute, storage, networking) Trade specificity for portability Pros:\nTheoretical cloud portability Simplified mental model - one API for multiple clouds Easier multi-cloud strategy Reduces cloud-specific knowledge requirements Cons:\nLowest common denominator problem - can\u0026rsquo;t use cloud-specific features Abstraction leaks when cloud differences emerge Debugging complexity across multiple provider implementations Portability often theoretical rather than practical Performance and feature gaps compared to native services 3c: Fargate-First Architecture#\rPhilosophy: Eliminate node management entirely using serverless compute (AWS Fargate).\nCharacteristics:\nManagement cluster runs on Fargate (no EC2 nodes) Workload clusters are Fargate profiles Focus on ephemeral, short-lived workload clusters Pay-per-pod pricing model Pros:\nZero node management overhead Automatic scaling and patching Perfect for ephemeral workload clusters Simplified security model (no SSH, no node access) Cost optimization for bursty workloads Cons:\nHigher per-pod costs for steady-state workloads Fargate limitations (no DaemonSets, limited instance types, startup latency) Less control over underlying compute Not all Kubernetes features supported Debugging constraints 3d: vCluster-Based Multi-Tenancy#\rPhilosophy: Use virtual clusters (vClusters) to maximize density and resource sharing while maintaining isolation.\nCharacteristics:\nSingle \u0026ldquo;host\u0026rdquo; EKS cluster runs the management plane Workload \u0026ldquo;clusters\u0026rdquo; are actually vClusters (virtual clusters) Much lighter weight than full EKS clusters Shared underlying infrastructure with logical isolation Pros:\nDramatically reduced cost - one EKS cluster supports many workload \u0026ldquo;clusters\u0026rdquo; Faster provisioning - vClusters spin up in seconds Perfect for development/testing environments Strong multi-tenancy isolation Easier upgrades - upgrade host cluster, vClusters inherit Cons:\nNot true cluster isolation - shares kernel and some resources Complexity in networking and service mesh integration Resource contention possible between vClusters Less suitable for strict compliance requirements Learning curve for operational model 3e: Kubernetes-Native Tooling Architecture#\rPhilosophy: Minimize AWS-specific dependencies by using cloud-agnostic Kubernetes ecosystem tools.\nCharacteristics:\nVault instead of AWS Secrets Manager Harbor instead of ECR cert-manager instead of ACM NGINX Ingress instead of AWS Load Balancer Controller External-DNS for route53 (minimal AWS coupling) Pros:\nCloud portability - easier to migrate to different cloud Consistent tooling across environments Rich Kubernetes ecosystem features Often better integration with GitOps workflows Skills transfer across clouds Cons:\nAdditional operational burden - you manage these services More infrastructure to maintain and secure Potentially higher resource costs (running Harbor, Vault, etc.) May sacrifice AWS-native integrations and optimizations Increased complexity in initial setup 3f: ECS as Control Plane Alternative#\rPhilosophy: Use ECS for the management cluster control plane, Crossplane manages both ECS tasks and EKS workload clusters.\nCharacteristics:\nManagement cluster components run as ECS tasks Crossplane controller runs in ECS Provisions EKS clusters as workload clusters Hybrid container orchestration model Pros:\nPotentially lower management cluster costs Simpler management plane (no Kubernetes overhead for management) Familiar to teams already using ECS Fine-grained IAM controls at task level Cons:\nUnusual architecture - less community support Crossplane primarily designed for Kubernetes Complexity in managing ECS-based control plane Limited tooling compared to Kubernetes-native approach Mixing orchestrators increases cognitive load Summary Comparison Table#\rCriteria Direct MRs XRDs Opinionated Multi-Cloud Fargate vCluster K8s-Native ECS Control Plane Complexity Low (simple) Medium Low Medium-High Medium Medium-High High High Verbosity Very High Low Very Low Low Medium Low Medium Medium AWS Coupling High High High Low Very High Medium Low Very High Portability None None None High None Medium High None Operational Overhead Low Low Very Low Medium Very Low Medium Very High High Cost (relative) Baseline Baseline Baseline Higher Variable* Much Lower Higher Lower Time to Production Slow Medium Fast Slow Medium Medium Very Slow Slow Flexibility Maximum High Low Medium Low Medium High Medium Team Skill Required AWS APIs Crossplane + AWS Minimal Abstraction concepts AWS Fargate vCluster + K8s K8s ecosystem ECS + Crossplane Best For Learning, debugging Production platforms Quick starts Multi-cloud orgs Serverless workloads Dev/test density Cloud agnostic ECS-heavy orgs *Fargate costs are lower for bursty workloads, higher for steady-state\nKey Architectural Differences#\rControl vs Flexibility Spectrum#\rMost Control: Direct MRs Balanced: XRDs, K8s-Native Most Opinionated: Opinionated, Fargate Infrastructure Philosophy#\rTraditional Infrastructure: Direct MRs, XRDs, Opinionated Serverless/Minimal Ops: Fargate, ECS Control Plane Density/Efficiency: vCluster Portability: Multi-Cloud, K8s-Native Cluster Model#\rFull EKS Clusters: Direct MRs, XRDs, Opinionated, Multi-Cloud, K8s-Native Fargate Profiles: Fargate Virtual Clusters: vCluster Hybrid: ECS Control Plane "},{"id":40,"href":"/musings/docs/misc/executive-summary/","title":"DevOps Domain Model: Executive Summary","section":"Miscellaneous","content":"DevOps Domain Model: Executive Summary#\rWhat You\u0026rsquo;re Building#\rA unified domain model overlaid on distributed DevOps infrastructure (Git, Docker, Kubernetes, CI/CD, logs) that enables:\nComplete traceability - From any running pod back to the exact git commit Consistency verification - Automated checks that reality matches rules Audit trail - Complete history of what was deployed, when, and by whom Incident response - Instantly correlate production issues to code changes Why This Approach Is Perfect for Your Use Case#\r‚úÖ No centralized monitor needed - Correlation via metadata, not data aggregation ‚úÖ Non-invasive - Git, Docker, K8s remain independent ‚úÖ Eventual consistency acceptable - Read-mostly, verification after-the-fact ‚úÖ Scales naturally - Each system scales independently ‚úÖ Incremental adoption - Start with one pipeline, expand gradually\nThe Three-Layer Architecture#\r‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\r‚îÇ Layer 1: METADATA INJECTION ‚îÇ\r‚îÇ - Git tags with build/deploy info ‚îÇ\r‚îÇ - Docker labels with commit/build metadata ‚îÇ\r‚îÇ - K8s annotations with full traceability chain ‚îÇ\r‚îÇ - Application logs with correlation IDs ‚îÇ\r‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\r‚Üì\r‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\r‚îÇ Layer 2: CORRELATION ENGINE ‚îÇ\r‚îÇ - Lightweight collectors extract metadata ‚îÇ\r‚îÇ - Correlation logic links objects across systems ‚îÇ\r‚îÇ - Build complete traceability chains ‚îÇ\r‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\r‚Üì\r‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\r‚îÇ Layer 3: CONSISTENCY VERIFICATION ‚îÇ\r‚îÇ - Define business rules (invariants) ‚îÇ\r‚îÇ - Check distributed state against rules ‚îÇ\r‚îÇ - Report violations, generate alerts ‚îÇ\r‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îòCore Domain Model#\r7 Key Entity Types:\nCommit - Source code changes (Git) Build - CI/CD pipeline execution ContainerImage - Built artifacts (Docker Registry) Deployment - Deployment events (K8s) Pod - Running instances (K8s Runtime) LogEntry - Application logs Metric - Observability data Primary Correlation Key: commitHash (SHA-256) Secondary Keys: buildId, imageDigest, deploymentId\nImplementation Roadmap#\rPhase 1: Foundation (1-2 weeks)#\rAdapt domain model to your context Choose correlation identifiers Document metadata schema Phase 2: Metadata Injection (3-4 weeks)#\rUpdate one CI/CD pipeline Add Docker labels Add K8s annotations Test end-to-end traceability Phase 3: Correlation (5-6 weeks)#\rBuild lightweight collectors Implement correlation logic Create traceability queries Phase 4: Consistency Checks (7-8 weeks)#\rDefine business rules Implement verification Generate violation reports Quick Win (1 day)#\r# In CI/CD: Add commit hash to Docker image docker build --label \u0026#34;git.commit=$GIT_COMMIT\u0026#34; -t myapp . # In deployment: Add to K8s kubectl annotate deployment myapp git.commit=\u0026#34;$GIT_COMMIT\u0026#34; # Verify correlation kubectl get deployment myapp -o jsonpath=\u0026#39;{.metadata.annotations.git\\.commit}\u0026#39;Example Business Rules#\rTraceability Rule: Every production deployment must be traceable to a git commit Security Rule: All container images must have passed security scan Approval Rule: Production deployments must reference a change request Consistency Rule: Pod count must match deployment replica count Registry Rule: All pods must use approved container registry Branch Rule: Production deployments must come from protected branches Freshness Rule: No deployments older than 90 days without rebuild Success Metrics#\rMetric Target Purpose Traceability Coverage 95%+ Can trace deployment to code Consistency Score 95%+ State matches rules Time to Trace \u0026lt;5 min From incident to root commit Audit Completeness 100% Every deployment documented Tools \u0026amp; Technologies#\rMinimal Stack:\nPython scripts for collectors JSON files for data storage Standard APIs (Git, Docker, K8s) Basic correlation logic Enhanced Stack (Optional):\nOpenTelemetry for distributed tracing Prometheus/Grafana for metrics Loki for log aggregation Harbor for container registry ArgoCD for GitOps Backstage for developer portal Key Files Delivered#\rdomain-model.png - Visual domain model diagram DEVOPS-DOMAIN-MODEL-GUIDE.md - Complete implementation guide CI-CD-TEMPLATES.md - GitHub Actions, GitLab CI, K8s templates Collector examples - Python code for Git, Docker, K8s Correlation engine - Logic for linking objects Consistency checker - Business rule validation Next Steps#\rReview the domain model diagram Adapt the model to your specific infrastructure Choose correlation identifiers (commit hash is primary) Implement metadata injection in one CI/CD pipeline (proof of concept) Build simple collector to verify correlation works Define 2-3 critical business rules Expand to more pipelines and systems Critical Success Factors#\r‚úÖ Start small - One pipeline, one rule, prove it works ‚úÖ Document standards - Clear metadata schema everyone follows ‚úÖ Automate injection - Metadata added by CI/CD, not manually ‚úÖ Make it visible - Reports, dashboards, alerts ‚úÖ Iterate quickly - Add rules as you discover gaps ‚úÖ Don\u0026rsquo;t overengineer - JSON files and Python scripts go far\nWhy This Works#\rThis is exactly the right pattern for your use case because:\nDistributed reality is inherent - You can\u0026rsquo;t centralize Git/Docker/K8s Prevention isn\u0026rsquo;t goal - You want visibility, not enforcement (initially) Eventual consistency is fine - Minutes of lag is acceptable Post-hoc analysis has value - Understanding what happened matters Incremental adoption possible - Each team can adopt at their pace Unlike transactional business systems where verification-after-the-fact creates problems, in DevOps:\n‚úÖ Deployments are discrete events (not continuous updates) ‚úÖ History is immutable (git commits don\u0026rsquo;t change) ‚úÖ Remediation is straightforward (redeploy or rollback) ‚úÖ Gaps are acceptable initially (95% coverage is huge value) This is a proven pattern used at scale by organizations like Google (with Borg metadata), Netflix (with Spinnaker), and many others.\nYou\u0026rsquo;re on exactly the right track! üéØ\n"},{"id":41,"href":"/musings/docs/misc/devops-domain-model-guide/","title":"DevOps Domain Model: Implementation Guide","section":"Miscellaneous","content":"DevOps Domain Model: Implementation Guide#\rOverview#\rThis guide shows how to impose a unified domain model on distributed DevOps infrastructure (Git, Docker, Kubernetes, CI/CD, logs) for consolidated visibility and consistency checking.\nWhy This Approach Works for DevOps#\r‚úÖ Distributed by nature - Git, Docker, K8s should remain independent ‚úÖ Read-mostly - We\u0026rsquo;re correlating, not enforcing ‚úÖ Eventual consistency acceptable - Minutes of lag is fine ‚úÖ Post-hoc analysis valuable - Understanding deployment history is critical ‚úÖ No overarching system needed - Correlation via metadata\nStep 1: Define Correlation Identifiers#\rThese are your \u0026ldquo;foreign keys\u0026rdquo; across systems:\nPrimary Correlation Keys#\rIdentifier Format Used In Purpose commitHash sha256:abc123... Git, CI, Docker metadata, K8s annotations Primary traceability key imageDigest sha256:def456... Docker registry, K8s pod specs Container identity buildId build-2024-001 CI system, Docker labels, K8s annotations Build traceability deploymentId deploy-prod-2024-001 K8s deployment, logs, metrics Deployment tracking version/tag v1.2.3 or main-abc123 Git tags, Docker tags, K8s labels Human-readable reference Secondary Correlation Keys#\rIdentifier Purpose Example buildNumber CI pipeline tracking #1234 prNumber Pull request linking PR-456 releaseVersion Semantic versioning v2.1.0 environmentName Deployment target production, staging Step 2: Metadata Injection Strategy#\rGit (Source of Truth)#\rAlready has:\n‚úÖ commitHash ‚úÖ author, timestamp, message ‚úÖ branch name Add via git tags:\n# Tag commit with build info git tag -a \u0026#34;build-${BUILD_ID}\u0026#34; -m \u0026#34;Build: ${BUILD_NUMBER}, CI: ${CI_JOB_URL}\u0026#34; # Tag with deployment info git tag -a \u0026#34;deploy-prod-${DEPLOY_ID}\u0026#34; -m \u0026#34;Deployed to prod at $(date)\u0026#34;Add via commit messages (structured):\nfeat: Add user authentication\rbuild-id: build-2024-001\rjira: PROJ-123\rreviewed-by: alice@example.comDocker Images (Container Layer)#\rInject metadata as OCI labels:\n# In Dockerfile or at build time LABEL org.opencontainers.image.source=\u0026#34;https://github.com/myorg/myrepo\u0026#34; LABEL org.opencontainers.image.revision=\u0026#34;${GIT_COMMIT}\u0026#34; LABEL org.opencontainers.image.version=\u0026#34;${VERSION}\u0026#34; LABEL org.opencontainers.image.created=\u0026#34;${BUILD_DATE}\u0026#34; # Custom domain labels LABEL com.myorg.build.id=\u0026#34;${BUILD_ID}\u0026#34; LABEL com.myorg.build.number=\u0026#34;${BUILD_NUMBER}\u0026#34; LABEL com.myorg.build.url=\u0026#34;${CI_JOB_URL}\u0026#34; LABEL com.myorg.build.branch=\u0026#34;${GIT_BRANCH}\u0026#34; LABEL com.myorg.test.passed=\u0026#34;${TEST_STATUS}\u0026#34; LABEL com.myorg.security.scan=\u0026#34;${SCAN_ID}\u0026#34;At build time:\ndocker build \\ --label \u0026#34;org.opencontainers.image.revision=${GIT_COMMIT}\u0026#34; \\ --label \u0026#34;com.myorg.build.id=${BUILD_ID}\u0026#34; \\ --label \u0026#34;com.myorg.build.timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)\u0026#34; \\ -t myapp:${VERSION} .Query labels:\ndocker inspect myapp:latest --format=\u0026#39;{{json .Config.Labels}}\u0026#39;Kubernetes (Runtime Layer)#\rInject metadata as annotations and labels:\napiVersion: apps/v1 kind: Deployment metadata: name: myapp namespace: production labels: app: myapp version: v1.2.3 environment: production annotations: # Source traceability git.commit: \u0026#34;abc123def456\u0026#34; git.branch: \u0026#34;main\u0026#34; git.repo: \u0026#34;https://github.com/myorg/myrepo\u0026#34; # Build traceability build.id: \u0026#34;build-2024-001\u0026#34; build.number: \u0026#34;1234\u0026#34; build.url: \u0026#34;https://ci.example.com/job/1234\u0026#34; build.timestamp: \u0026#34;2024-01-15T10:30:00Z\u0026#34; # Deployment traceability deployment.id: \u0026#34;deploy-prod-2024-001\u0026#34; deployment.timestamp: \u0026#34;2024-01-15T14:00:00Z\u0026#34; deployment.operator: \u0026#34;alice@example.com\u0026#34; deployment.change-request: \u0026#34;CHG-5678\u0026#34; # Container traceability image.digest: \u0026#34;sha256:def456...\u0026#34; image.scan.id: \u0026#34;scan-2024-001\u0026#34; image.scan.status: \u0026#34;passed\u0026#34; spec: template: metadata: labels: app: myapp version: v1.2.3 annotations: # Propagate to pods git.commit: \u0026#34;abc123def456\u0026#34; deployment.id: \u0026#34;deploy-prod-2024-001\u0026#34; spec: containers: - name: myapp image: myregistry.io/myapp@sha256:def456... # Use digest! env: - name: GIT_COMMIT value: \u0026#34;abc123def456\u0026#34; - name: BUILD_ID value: \u0026#34;build-2024-001\u0026#34; - name: DEPLOYMENT_ID value: \u0026#34;deploy-prod-2024-001\u0026#34;Add to pods at runtime:\nkubectl annotate deployment myapp \\ git.commit=\u0026#34;${GIT_COMMIT}\u0026#34; \\ deployment.id=\u0026#34;${DEPLOY_ID}\u0026#34; \\ --overwriteApplication Logs (Observability Layer)#\rStructure logs with correlation fields:\n{ \u0026#34;timestamp\u0026#34;: \u0026#34;2024-01-15T14:05:23Z\u0026#34;, \u0026#34;level\u0026#34;: \u0026#34;INFO\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;User authentication successful\u0026#34;, \u0026#34;correlation\u0026#34;: { \u0026#34;git_commit\u0026#34;: \u0026#34;abc123def456\u0026#34;, \u0026#34;build_id\u0026#34;: \u0026#34;build-2024-001\u0026#34;, \u0026#34;deployment_id\u0026#34;: \u0026#34;deploy-prod-2024-001\u0026#34;, \u0026#34;pod_name\u0026#34;: \u0026#34;myapp-7d8f9-xyz\u0026#34;, \u0026#34;container_id\u0026#34;: \u0026#34;docker://123abc\u0026#34;, \u0026#34;node_name\u0026#34;: \u0026#34;node-01\u0026#34; }, \u0026#34;context\u0026#34;: { \u0026#34;user_id\u0026#34;: \u0026#34;user-123\u0026#34;, \u0026#34;trace_id\u0026#34;: \u0026#34;trace-abc-def\u0026#34;, \u0026#34;span_id\u0026#34;: \u0026#34;span-123\u0026#34; } }In application code:\nimport os import logging # Read correlation IDs from environment GIT_COMMIT = os.getenv(\u0026#39;GIT_COMMIT\u0026#39;, \u0026#39;unknown\u0026#39;) BUILD_ID = os.getenv(\u0026#39;BUILD_ID\u0026#39;, \u0026#39;unknown\u0026#39;) DEPLOYMENT_ID = os.getenv(\u0026#39;DEPLOYMENT_ID\u0026#39;, \u0026#39;unknown\u0026#39;) POD_NAME = os.getenv(\u0026#39;HOSTNAME\u0026#39;, \u0026#39;unknown\u0026#39;) # Configure structured logging logging.basicConfig( format=\u0026#39;%(message)s\u0026#39;, level=logging.INFO ) class CorrelationLogger: def info(self, message, **context): log_entry = { \u0026#34;timestamp\u0026#34;: datetime.utcnow().isoformat(), \u0026#34;level\u0026#34;: \u0026#34;INFO\u0026#34;, \u0026#34;message\u0026#34;: message, \u0026#34;correlation\u0026#34;: { \u0026#34;git_commit\u0026#34;: GIT_COMMIT, \u0026#34;build_id\u0026#34;: BUILD_ID, \u0026#34;deployment_id\u0026#34;: DEPLOYMENT_ID, \u0026#34;pod_name\u0026#34;: POD_NAME, }, \u0026#34;context\u0026#34;: context } logging.info(json.dumps(log_entry)) logger = CorrelationLogger() logger.info(\u0026#34;Request processed\u0026#34;, user_id=\u0026#34;123\u0026#34;, duration_ms=45) Step 3: Data Collection Strategy#\rYou need to extract metadata from each system without building a monolithic monitor.\nCollection Architecture#\r‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\r‚îÇ Git API ‚îÇ ‚Üí Collector ‚Üí JSON files\r‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\r‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\r‚îÇ Docker API ‚îÇ ‚Üí Collector ‚Üí JSON files\r‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\r‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\r‚îÇ K8s API ‚îÇ ‚Üí Collector ‚Üí JSON files\r‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\r‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\r‚îÇ Logs API ‚îÇ ‚Üí Collector ‚Üí JSON files\r‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\r‚Üì\r[Correlation Engine]\r‚Üì\r[Consistency Checker]\r‚Üì\r[Reports / Dashboards]Simple Collector Pattern#\rEach collector:\nQueries its source system API Extracts objects and metadata Writes to standardized JSON format Stores with timestamp Example: Git Collector\n# git_collector.py import git import json from datetime import datetime def collect_git_metadata(repo_path, output_dir): repo = git.Repo(repo_path) commits = [] for commit in repo.iter_commits(max_count=100): commits.append({ \u0026#34;type\u0026#34;: \u0026#34;Commit\u0026#34;, \u0026#34;commitHash\u0026#34;: commit.hexsha, \u0026#34;shortHash\u0026#34;: commit.hexsha[:7], \u0026#34;author\u0026#34;: str(commit.author), \u0026#34;timestamp\u0026#34;: commit.committed_datetime.isoformat(), \u0026#34;message\u0026#34;: commit.message.strip(), \u0026#34;branch\u0026#34;: repo.active_branch.name, \u0026#34;tags\u0026#34;: [tag.name for tag in repo.tags if tag.commit == commit], \u0026#34;parents\u0026#34;: [p.hexsha for p in commit.parents] }) output = { \u0026#34;collectedAt\u0026#34;: datetime.utcnow().isoformat(), \u0026#34;source\u0026#34;: \u0026#34;git\u0026#34;, \u0026#34;repository\u0026#34;: repo.remotes.origin.url, \u0026#34;commits\u0026#34;: commits } with open(f\u0026#34;{output_dir}/git-{datetime.now().strftime(\u0026#39;%Y%m%d-%H%M%S\u0026#39;)}.json\u0026#34;, \u0026#39;w\u0026#39;) as f: json.dump(output, f, indent=2) if __name__ == \u0026#34;__main__\u0026#34;: collect_git_metadata(\u0026#34;/path/to/repo\u0026#34;, \u0026#34;./data/git\u0026#34;)Example: Docker Collector\n# docker_collector.py import docker import json from datetime import datetime def collect_docker_metadata(registry_url, image_names, output_dir): client = docker.from_env() images = [] for image_name in image_names: try: image = client.images.get(image_name) images.append({ \u0026#34;type\u0026#34;: \u0026#34;ContainerImage\u0026#34;, \u0026#34;imageId\u0026#34;: image.id, \u0026#34;tags\u0026#34;: image.tags, \u0026#34;digest\u0026#34;: image.attrs.get(\u0026#39;RepoDigests\u0026#39;, [\u0026#39;unknown\u0026#39;])[0], \u0026#34;created\u0026#34;: image.attrs[\u0026#39;Created\u0026#39;], \u0026#34;size\u0026#34;: image.attrs[\u0026#39;Size\u0026#39;], \u0026#34;labels\u0026#34;: image.labels, \u0026#34;metadata\u0026#34;: { \u0026#34;gitCommit\u0026#34;: image.labels.get(\u0026#39;org.opencontainers.image.revision\u0026#39;), \u0026#34;buildId\u0026#34;: image.labels.get(\u0026#39;com.myorg.build.id\u0026#39;), \u0026#34;buildNumber\u0026#34;: image.labels.get(\u0026#39;com.myorg.build.number\u0026#39;), \u0026#34;version\u0026#34;: image.labels.get(\u0026#39;org.opencontainers.image.version\u0026#39;), } }) except docker.errors.ImageNotFound: continue output = { \u0026#34;collectedAt\u0026#34;: datetime.utcnow().isoformat(), \u0026#34;source\u0026#34;: \u0026#34;docker\u0026#34;, \u0026#34;registry\u0026#34;: registry_url, \u0026#34;images\u0026#34;: images } with open(f\u0026#34;{output_dir}/docker-{datetime.now().strftime(\u0026#39;%Y%m%d-%H%M%S\u0026#39;)}.json\u0026#34;, \u0026#39;w\u0026#39;) as f: json.dump(output, f, indent=2) if __name__ == \u0026#34;__main__\u0026#34;: collect_docker_metadata( \u0026#34;myregistry.io\u0026#34;, [\u0026#34;myapp:latest\u0026#34;, \u0026#34;myapp:v1.2.3\u0026#34;], \u0026#34;./data/docker\u0026#34; )Example: Kubernetes Collector\n# k8s_collector.py from kubernetes import client, config import json from datetime import datetime def collect_k8s_metadata(namespace, output_dir): config.load_kube_config() apps_v1 = client.AppsV1Api() core_v1 = client.CoreV1Api() deployments = [] for deploy in apps_v1.list_namespaced_deployment(namespace).items: deployments.append({ \u0026#34;type\u0026#34;: \u0026#34;Deployment\u0026#34;, \u0026#34;deploymentId\u0026#34;: f\u0026#34;{namespace}-{deploy.metadata.name}\u0026#34;, \u0026#34;name\u0026#34;: deploy.metadata.name, \u0026#34;namespace\u0026#34;: deploy.metadata.namespace, \u0026#34;labels\u0026#34;: deploy.metadata.labels, \u0026#34;annotations\u0026#34;: deploy.metadata.annotations, \u0026#34;replicas\u0026#34;: deploy.spec.replicas, \u0026#34;image\u0026#34;: deploy.spec.template.spec.containers[0].image, \u0026#34;createdAt\u0026#34;: deploy.metadata.creation_timestamp.isoformat(), \u0026#34;metadata\u0026#34;: { \u0026#34;gitCommit\u0026#34;: deploy.metadata.annotations.get(\u0026#39;git.commit\u0026#39;) if deploy.metadata.annotations else None, \u0026#34;buildId\u0026#34;: deploy.metadata.annotations.get(\u0026#39;build.id\u0026#39;) if deploy.metadata.annotations else None, \u0026#34;deploymentId\u0026#34;: deploy.metadata.annotations.get(\u0026#39;deployment.id\u0026#39;) if deploy.metadata.annotations else None, } }) pods = [] for pod in core_v1.list_namespaced_pod(namespace).items: pods.append({ \u0026#34;type\u0026#34;: \u0026#34;Pod\u0026#34;, \u0026#34;podId\u0026#34;: pod.metadata.name, \u0026#34;namespace\u0026#34;: pod.metadata.namespace, \u0026#34;nodeName\u0026#34;: pod.spec.node_name, \u0026#34;status\u0026#34;: pod.status.phase, \u0026#34;startedAt\u0026#34;: pod.status.start_time.isoformat() if pod.status.start_time else None, \u0026#34;labels\u0026#34;: pod.metadata.labels, \u0026#34;annotations\u0026#34;: pod.metadata.annotations, \u0026#34;containers\u0026#34;: [ { \u0026#34;name\u0026#34;: c.name, \u0026#34;image\u0026#34;: c.image, \u0026#34;imageId\u0026#34;: pod.status.container_statuses[i].image_id if pod.status.container_statuses else None, \u0026#34;restartCount\u0026#34;: pod.status.container_statuses[i].restart_count if pod.status.container_statuses else 0, } for i, c in enumerate(pod.spec.containers) ], \u0026#34;metadata\u0026#34;: { \u0026#34;gitCommit\u0026#34;: pod.metadata.annotations.get(\u0026#39;git.commit\u0026#39;) if pod.metadata.annotations else None, \u0026#34;deploymentId\u0026#34;: pod.metadata.annotations.get(\u0026#39;deployment.id\u0026#39;) if pod.metadata.annotations else None, } }) output = { \u0026#34;collectedAt\u0026#34;: datetime.utcnow().isoformat(), \u0026#34;source\u0026#34;: \u0026#34;kubernetes\u0026#34;, \u0026#34;cluster\u0026#34;: \u0026#34;production\u0026#34;, \u0026#34;namespace\u0026#34;: namespace, \u0026#34;deployments\u0026#34;: deployments, \u0026#34;pods\u0026#34;: pods } with open(f\u0026#34;{output_dir}/k8s-{datetime.now().strftime(\u0026#39;%Y%m%d-%H%M%S\u0026#39;)}.json\u0026#34;, \u0026#39;w\u0026#39;) as f: json.dump(output, f, indent=2) if __name__ == \u0026#34;__main__\u0026#34;: collect_k8s_metadata(\u0026#34;production\u0026#34;, \u0026#34;./data/k8s\u0026#34;) Step 4: Correlation Engine#\rNow build correlation logic to link objects across systems:\n# correlation_engine.py import json import glob from collections import defaultdict class CorrelationEngine: def __init__(self, data_dir): self.data_dir = data_dir self.commits = {} self.builds = {} self.images = {} self.deployments = {} self.pods = {} def load_all_data(self): \u0026#34;\u0026#34;\u0026#34;Load all collected JSON files\u0026#34;\u0026#34;\u0026#34; # Load Git data for file in glob.glob(f\u0026#34;{self.data_dir}/git/*.json\u0026#34;): with open(file) as f: data = json.load(f) for commit in data.get(\u0026#39;commits\u0026#39;, []): self.commits[commit[\u0026#39;commitHash\u0026#39;]] = commit # Load Docker data for file in glob.glob(f\u0026#34;{self.data_dir}/docker/*.json\u0026#34;): with open(file) as f: data = json.load(f) for image in data.get(\u0026#39;images\u0026#39;, []): digest = image.get(\u0026#39;digest\u0026#39;, \u0026#39;unknown\u0026#39;) self.images[digest] = image # Load K8s data for file in glob.glob(f\u0026#34;{self.data_dir}/k8s/*.json\u0026#34;): with open(file) as f: data = json.load(f) for deploy in data.get(\u0026#39;deployments\u0026#39;, []): self.deployments[deploy[\u0026#39;deploymentId\u0026#39;]] = deploy for pod in data.get(\u0026#39;pods\u0026#39;, []): self.pods[pod[\u0026#39;podId\u0026#39;]] = pod def correlate_deployment_to_commit(self, deployment_id): \u0026#34;\u0026#34;\u0026#34;Find the git commit for a deployment\u0026#34;\u0026#34;\u0026#34; deployment = self.deployments.get(deployment_id) if not deployment: return None # Try annotation first commit_hash = deployment.get(\u0026#39;metadata\u0026#39;, {}).get(\u0026#39;gitCommit\u0026#39;) if commit_hash and commit_hash in self.commits: return self.commits[commit_hash] # Try image labels image = deployment.get(\u0026#39;image\u0026#39;) for img_digest, img_data in self.images.items(): if image in img_data.get(\u0026#39;tags\u0026#39;, []): commit_hash = img_data.get(\u0026#39;metadata\u0026#39;, {}).get(\u0026#39;gitCommit\u0026#39;) if commit_hash and commit_hash in self.commits: return self.commits[commit_hash] return None def correlate_pod_to_commit(self, pod_id): \u0026#34;\u0026#34;\u0026#34;Find the git commit for a running pod\u0026#34;\u0026#34;\u0026#34; pod = self.pods.get(pod_id) if not pod: return None # Try annotation commit_hash = pod.get(\u0026#39;metadata\u0026#39;, {}).get(\u0026#39;gitCommit\u0026#39;) if commit_hash: return self.commits.get(commit_hash) # Try image digest for container in pod.get(\u0026#39;containers\u0026#39;, []): image_id = container.get(\u0026#39;imageId\u0026#39;, \u0026#39;\u0026#39;) # Extract digest from image_id (format: docker-pullable://repo@sha256:...) if \u0026#39;@sha256:\u0026#39; in image_id: digest = \u0026#39;sha256:\u0026#39; + image_id.split(\u0026#39;@sha256:\u0026#39;)[1].split(\u0026#39;@\u0026#39;)[0] image = self.images.get(digest) if image: commit_hash = image.get(\u0026#39;metadata\u0026#39;, {}).get(\u0026#39;gitCommit\u0026#39;) if commit_hash: return self.commits.get(commit_hash) return None def build_traceability_chain(self, starting_point, starting_type): \u0026#34;\u0026#34;\u0026#34;Build complete traceability from any starting point\u0026#34;\u0026#34;\u0026#34; chain = { \u0026#34;commit\u0026#34;: None, \u0026#34;build\u0026#34;: None, \u0026#34;image\u0026#34;: None, \u0026#34;deployment\u0026#34;: None, \u0026#34;pods\u0026#34;: [] } if starting_type == \u0026#34;commit\u0026#34;: commit_hash = starting_point chain[\u0026#34;commit\u0026#34;] = self.commits.get(commit_hash) # Find images with this commit for digest, image in self.images.items(): if image.get(\u0026#39;metadata\u0026#39;, {}).get(\u0026#39;gitCommit\u0026#39;) == commit_hash: chain[\u0026#34;image\u0026#34;] = image # Find deployments with this image for deploy_id, deploy in self.deployments.items(): if digest in deploy.get(\u0026#39;image\u0026#39;, \u0026#39;\u0026#39;): chain[\u0026#34;deployment\u0026#34;] = deploy # Find pods from this deployment deploy_name = deploy.get(\u0026#39;name\u0026#39;) for pod_id, pod in self.pods.items(): if pod.get(\u0026#39;labels\u0026#39;, {}).get(\u0026#39;app\u0026#39;) == deploy_name: chain[\u0026#34;pods\u0026#34;].append(pod) break # Similar logic for other starting_types... return chain def generate_report(self): \u0026#34;\u0026#34;\u0026#34;Generate correlation report\u0026#34;\u0026#34;\u0026#34; report = { \u0026#34;summary\u0026#34;: { \u0026#34;total_commits\u0026#34;: len(self.commits), \u0026#34;total_images\u0026#34;: len(self.images), \u0026#34;total_deployments\u0026#34;: len(self.deployments), \u0026#34;total_pods\u0026#34;: len(self.pods), }, \u0026#34;correlations\u0026#34;: [], \u0026#34;orphans\u0026#34;: { \u0026#34;images_without_commit\u0026#34;: [], \u0026#34;deployments_without_commit\u0026#34;: [], \u0026#34;pods_without_correlation\u0026#34;: [] } } # Check for orphaned images for digest, image in self.images.items(): commit = image.get(\u0026#39;metadata\u0026#39;, {}).get(\u0026#39;gitCommit\u0026#39;) if not commit or commit not in self.commits: report[\u0026#34;orphans\u0026#34;][\u0026#34;images_without_commit\u0026#34;].append({ \u0026#34;digest\u0026#34;: digest, \u0026#34;tags\u0026#34;: image.get(\u0026#39;tags\u0026#39;, []) }) # Check for orphaned deployments for deploy_id, deploy in self.deployments.items(): commit = self.correlate_deployment_to_commit(deploy_id) if not commit: report[\u0026#34;orphans\u0026#34;][\u0026#34;deployments_without_commit\u0026#34;].append({ \u0026#34;deploymentId\u0026#34;: deploy_id, \u0026#34;name\u0026#34;: deploy.get(\u0026#39;name\u0026#39;), \u0026#34;image\u0026#34;: deploy.get(\u0026#39;image\u0026#39;) }) return report if __name__ == \u0026#34;__main__\u0026#34;: engine = CorrelationEngine(\u0026#34;./data\u0026#34;) engine.load_all_data() report = engine.generate_report() print(json.dumps(report, indent=2)) Step 5: Consistency Checks#\rDefine business rules and check them:\n# consistency_checker.py from correlation_engine import CorrelationEngine class ConsistencyChecker: def __init__(self, engine): self.engine = engine self.violations = [] def check_all(self): \u0026#34;\u0026#34;\u0026#34;Run all consistency checks\u0026#34;\u0026#34;\u0026#34; self.check_all_deployments_have_commit() self.check_all_images_scanned() self.check_all_pods_from_known_images() self.check_deployment_matches_pods() return self.violations def check_all_deployments_have_commit(self): \u0026#34;\u0026#34;\u0026#34;Rule: Every deployment must be traceable to a git commit\u0026#34;\u0026#34;\u0026#34; for deploy_id, deploy in self.engine.deployments.items(): commit = self.engine.correlate_deployment_to_commit(deploy_id) if not commit: self.violations.append({ \u0026#34;rule\u0026#34;: \u0026#34;deployment_has_commit\u0026#34;, \u0026#34;severity\u0026#34;: \u0026#34;HIGH\u0026#34;, \u0026#34;deploymentId\u0026#34;: deploy_id, \u0026#34;deployment\u0026#34;: deploy.get(\u0026#39;name\u0026#39;), \u0026#34;message\u0026#34;: f\u0026#34;Deployment {deploy.get(\u0026#39;name\u0026#39;)} has no traceable git commit\u0026#34; }) def check_all_images_scanned(self): \u0026#34;\u0026#34;\u0026#34;Rule: All images must have security scan\u0026#34;\u0026#34;\u0026#34; for digest, image in self.engine.images.items(): scan_status = image.get(\u0026#39;labels\u0026#39;, {}).get(\u0026#39;com.myorg.security.scan\u0026#39;) if not scan_status: self.violations.append({ \u0026#34;rule\u0026#34;: \u0026#34;image_must_be_scanned\u0026#34;, \u0026#34;severity\u0026#34;: \u0026#34;CRITICAL\u0026#34;, \u0026#34;imageDigest\u0026#34;: digest, \u0026#34;tags\u0026#34;: image.get(\u0026#39;tags\u0026#39;, []), \u0026#34;message\u0026#34;: f\u0026#34;Image {image.get(\u0026#39;tags\u0026#39;, [\u0026#39;unknown\u0026#39;])[0]} has no security scan\u0026#34; }) def check_all_pods_from_known_images(self): \u0026#34;\u0026#34;\u0026#34;Rule: All running pods must use images from our registry\u0026#34;\u0026#34;\u0026#34; for pod_id, pod in self.engine.pods.items(): for container in pod.get(\u0026#39;containers\u0026#39;, []): image = container.get(\u0026#39;image\u0026#39;, \u0026#39;\u0026#39;) if not image.startswith(\u0026#39;myregistry.io/\u0026#39;): self.violations.append({ \u0026#34;rule\u0026#34;: \u0026#34;pods_use_approved_registry\u0026#34;, \u0026#34;severity\u0026#34;: \u0026#34;MEDIUM\u0026#34;, \u0026#34;podId\u0026#34;: pod_id, \u0026#34;image\u0026#34;: image, \u0026#34;message\u0026#34;: f\u0026#34;Pod {pod_id} uses image from unapproved registry: {image}\u0026#34; }) def check_deployment_matches_pods(self): \u0026#34;\u0026#34;\u0026#34;Rule: Pod count should match deployment replica count\u0026#34;\u0026#34;\u0026#34; for deploy_id, deploy in self.engine.deployments.items(): expected_replicas = deploy.get(\u0026#39;replicas\u0026#39;, 1) deploy_name = deploy.get(\u0026#39;name\u0026#39;) # Count running pods for this deployment running_pods = [ pod for pod in self.engine.pods.values() if pod.get(\u0026#39;labels\u0026#39;, {}).get(\u0026#39;app\u0026#39;) == deploy_name and pod.get(\u0026#39;status\u0026#39;) == \u0026#39;Running\u0026#39; ] if len(running_pods) != expected_replicas: self.violations.append({ \u0026#34;rule\u0026#34;: \u0026#34;deployment_replica_count\u0026#34;, \u0026#34;severity\u0026#34;: \u0026#34;MEDIUM\u0026#34;, \u0026#34;deploymentId\u0026#34;: deploy_id, \u0026#34;expected\u0026#34;: expected_replicas, \u0026#34;actual\u0026#34;: len(running_pods), \u0026#34;message\u0026#34;: f\u0026#34;Deployment {deploy_name} expects {expected_replicas} replicas but has {len(running_pods)}\u0026#34; }) if __name__ == \u0026#34;__main__\u0026#34;: engine = CorrelationEngine(\u0026#34;./data\u0026#34;) engine.load_all_data() checker = ConsistencyChecker(engine) violations = checker.check_all() print(f\u0026#34;Found {len(violations)} violations:\u0026#34;) for v in violations: print(f\u0026#34; [{v[\u0026#39;severity\u0026#39;]}] {v[\u0026#39;rule\u0026#39;]}: {v[\u0026#39;message\u0026#39;]}\u0026#34;) Step 6: Practical Implementation Roadmap#\rPhase 1: Foundation (Week 1-2)#\rDefine your domain model (adapt the one provided) Choose correlation identifiers Document metadata schema Create simple collectors for Git, Docker, K8s Phase 2: Metadata Injection (Week 3-4)#\rUpdate CI/CD pipelines to add labels to Docker images Update K8s deployment templates with annotations Instrument application logging with correlation IDs Test end-to-end traceability Phase 3: Correlation (Week 5-6)#\rBuild correlation engine Test correlation logic Create initial reports Phase 4: Consistency Checking (Week 7-8)#\rDefine business rules Implement consistency checks Create violation reports Set up alerting Phase 5: Visualization (Week 9-10)#\rBuild dashboards Create traceability diagrams Automate report generation Quick Win: Minimum Viable Implementation#\rStart here for immediate value:\n# Step 1: Add to CI/CD pipeline docker build \\ --label \u0026#34;git.commit=$GIT_COMMIT\u0026#34; \\ --label \u0026#34;build.id=$CI_JOB_ID\u0026#34; \\ -t myapp:$VERSION . # Step 2: Add to K8s deployment kubectl annotate deployment myapp \\ git.commit=\u0026#34;$GIT_COMMIT\u0026#34; \\ build.id=\u0026#34;$CI_JOB_ID\u0026#34; # Step 3: Query correlation DEPLOY_COMMIT=$(kubectl get deployment myapp -o jsonpath=\u0026#39;{.metadata.annotations.git\\.commit}\u0026#39;) IMAGE=$(kubectl get deployment myapp -o jsonpath=\u0026#39;{.spec.template.spec.containers[0].image}\u0026#39;) IMAGE_COMMIT=$(docker inspect $IMAGE --format=\u0026#39;{{.Config.Labels.git\\.commit}}\u0026#39;) if [ \u0026#34;$DEPLOY_COMMIT\u0026#34; == \u0026#34;$IMAGE_COMMIT\u0026#34; ]; then echo \u0026#34;‚úì Deployment matches image\u0026#34; else echo \u0026#34;‚úó MISMATCH: Deploy=$DEPLOY_COMMIT, Image=$IMAGE_COMMIT\u0026#34; fiThis gives you basic traceability in one day.\nTools That Can Help#\rYou don\u0026rsquo;t need to build everything from scratch:\nTool Purpose Use Case OpenTelemetry Distributed tracing Correlation across services Prometheus + Grafana Metrics + visualization Runtime monitoring Loki Log aggregation Centralized logs with labels Jaeger Distributed tracing Request flow tracking Kubernetes Dashboard K8s visualization Deployment state Harbor Container registry Image scanning, metadata ArgoCD GitOps Git-to-K8s traceability Backstage Developer portal Service catalog Many of these already support metadata propagation!\nSuccess Metrics#\rHow do you know this is working?\nTraceability Coverage\n% of deployments traceable to commit: Target 95%+ % of images with build metadata: Target 100% % of pods with correlation IDs: Target 90%+ Consistency Compliance\n% of deployments passing all checks: Target 95%+ Mean time to detect violations: Target \u0026lt;5 minutes % of violations with known remediation: Target 80%+ Operational Impact\nTime to trace production issue to code: Target \u0026lt;5 minutes % of incidents with complete audit trail: Target 95%+ Developer satisfaction with traceability: Survey Next Steps#\rReview and adapt the domain model Choose your correlation identifiers Implement metadata injection in one CI pipeline (proof of concept) Build a simple collector for one system Create one correlation query Expand gradually This is exactly the right approach for your scenario! Start small, prove value, then expand.\n"},{"id":42,"href":"/musings/docs/misc/getting-started-concise/","title":"Getting Started Concise","section":"Miscellaneous","content":"Here\u0026rsquo;s a concise strategy:\n1. Schema Validation First (Quickest Win)\nRun helm template to generate manifests Validate with kubeconform or kubeval against Kubernetes schemas Automate in CI/CD 2. Unit Tests (Template Logic)\nUse helm-unittest plugin Start with critical templates (deployments, services, configmaps) Test: default values render correctly, conditional logic works, value overrides produce expected output Focus on areas with complex templating (if/else, loops, merges) 3. Integration Tests (Deployment Reality)\nUse kind or k3s for lightweight test clusters Deploy with different values files (dev, prod scenarios) Verify: pods start successfully, services are reachable, inter-component communication works Consider helm test hooks for basic smoke tests Tools: ct (chart-testing) for install/upgrade tests 4. Prioritization\nStart with schema validation (catches most obvious errors) Add unit tests for templates that have caused issues in production Add integration tests for critical deployment scenarios Expand coverage incrementally 5. CI/CD Integration Schema validation ‚Üí Unit tests ‚Üí Integration tests (on PRs/commits)\n"},{"id":43,"href":"/musings/docs/misc/ci-cd-templates/","title":"GitHub Actions Example","section":"Miscellaneous","content":"CI/CD Pipeline Template with Metadata Injection#\rGitHub Actions Example#\rname: Build and Deploy on: push: branches: [main] env: REGISTRY: myregistry.io IMAGE_NAME: myapp jobs: build: runs-on: ubuntu-latest outputs: image_digest: ${{ steps.build.outputs.digest }} build_id: ${{ steps.metadata.outputs.build_id }} steps: - name: Checkout code uses: actions/checkout@v3 with: fetch-depth: 0 # Full history for traceability - name: Generate build metadata id: metadata run: | echo \u0026#34;build_id=build-$(date +%Y%m%d)-${{ github.run_number }}\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT echo \u0026#34;build_timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT echo \u0026#34;git_commit=${{ github.sha }}\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT echo \u0026#34;git_short_commit=${GITHUB_SHA:0:7}\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT echo \u0026#34;git_branch=${GITHUB_REF##*/}\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT - name: Build Docker image with metadata id: build run: | docker build \\ --label \u0026#34;org.opencontainers.image.source=${{ github.repositoryUrl }}\u0026#34; \\ --label \u0026#34;org.opencontainers.image.revision=${{ steps.metadata.outputs.git_commit }}\u0026#34; \\ --label \u0026#34;org.opencontainers.image.version=${{ github.ref_name }}\u0026#34; \\ --label \u0026#34;org.opencontainers.image.created=${{ steps.metadata.outputs.build_timestamp }}\u0026#34; \\ --label \u0026#34;com.myorg.build.id=${{ steps.metadata.outputs.build_id }}\u0026#34; \\ --label \u0026#34;com.myorg.build.number=${{ github.run_number }}\u0026#34; \\ --label \u0026#34;com.myorg.build.url=${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\u0026#34; \\ --label \u0026#34;com.myorg.build.branch=${{ steps.metadata.outputs.git_branch }}\u0026#34; \\ --label \u0026#34;com.myorg.git.commit.short=${{ steps.metadata.outputs.git_short_commit }}\u0026#34; \\ --label \u0026#34;com.myorg.git.author=${{ github.actor }}\u0026#34; \\ -t ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ steps.metadata.outputs.git_short_commit }} \\ -t ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest \\ . # Get image digest IMAGE_ID=$(docker images -q ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest) DIGEST=$(docker inspect $IMAGE_ID --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; | cut -d\u0026#39;@\u0026#39; -f2) echo \u0026#34;digest=$DIGEST\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT - name: Run tests run: | docker run --rm ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest npm test - name: Security scan id: scan run: | # Example with Trivy docker run --rm -v /var/run/docker.sock:/var/run/docker.sock \\ aquasec/trivy image --severity HIGH,CRITICAL \\ ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest SCAN_ID=\u0026#34;scan-$(date +%Y%m%d%H%M%S)\u0026#34; echo \u0026#34;scan_id=$SCAN_ID\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT # Add scan result as label (in real world, push updated image) echo \u0026#34;Security scan completed: $SCAN_ID\u0026#34; - name: Tag image with scan metadata run: | # Re-tag with scan info (or use registry API to update labels) docker tag \\ ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest \\ ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:scanned-${{ steps.metadata.outputs.git_short_commit }} - name: Push to registry run: | echo \u0026#34;${{ secrets.REGISTRY_PASSWORD }}\u0026#34; | docker login ${{ env.REGISTRY }} -u ${{ secrets.REGISTRY_USERNAME }} --password-stdin docker push ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ steps.metadata.outputs.git_short_commit }} docker push ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest - name: Create git tag for build run: | git config user.name \u0026#34;GitHub Actions\u0026#34; git config user.email \u0026#34;actions@github.com\u0026#34; git tag -a \u0026#34;${{ steps.metadata.outputs.build_id }}\u0026#34; \\ -m \u0026#34;Build: ${{ steps.metadata.outputs.build_id }}\u0026#34; \\ -m \u0026#34;Run: ${{ github.run_number }}\u0026#34; \\ -m \u0026#34;Commit: ${{ steps.metadata.outputs.git_commit }}\u0026#34; \\ -m \u0026#34;Image: ${{ steps.build.outputs.digest }}\u0026#34; git push origin \u0026#34;${{ steps.metadata.outputs.build_id }}\u0026#34; deploy: needs: build runs-on: ubuntu-latest environment: production steps: - name: Checkout K8s manifests uses: actions/checkout@v3 - name: Generate deployment metadata id: deploy_metadata run: | echo \u0026#34;deployment_id=deploy-prod-$(date +%Y%m%d%H%M%S)\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT echo \u0026#34;deployment_timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT - name: Update K8s deployment with metadata run: | # Update deployment YAML with annotations kubectl set image deployment/myapp \\ myapp=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}@${{ needs.build.outputs.image_digest }} \\ --record kubectl annotate deployment/myapp \\ git.commit=\u0026#34;${{ github.sha }}\u0026#34; \\ git.branch=\u0026#34;${GITHUB_REF##*/}\u0026#34; \\ git.repo=\u0026#34;${{ github.repositoryUrl }}\u0026#34; \\ build.id=\u0026#34;${{ needs.build.outputs.build_id }}\u0026#34; \\ build.number=\u0026#34;${{ github.run_number }}\u0026#34; \\ build.url=\u0026#34;${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\u0026#34; \\ build.timestamp=\u0026#34;$(date -u +%Y-%m-%dT%H:%M:%SZ)\u0026#34; \\ deployment.id=\u0026#34;${{ steps.deploy_metadata.outputs.deployment_id }}\u0026#34; \\ deployment.timestamp=\u0026#34;${{ steps.deploy_metadata.outputs.deployment_timestamp }}\u0026#34; \\ deployment.operator=\u0026#34;${{ github.actor }}\u0026#34; \\ deployment.run-id=\u0026#34;${{ github.run_id }}\u0026#34; \\ image.digest=\u0026#34;${{ needs.build.outputs.image_digest }}\u0026#34; \\ --overwrite - name: Wait for rollout run: | kubectl rollout status deployment/myapp --timeout=5m - name: Create deployment tag in git run: | git config user.name \u0026#34;GitHub Actions\u0026#34; git config user.email \u0026#34;actions@github.com\u0026#34; git tag -a \u0026#34;${{ steps.deploy_metadata.outputs.deployment_id }}\u0026#34; \\ -m \u0026#34;Deployment to production\u0026#34; \\ -m \u0026#34;Timestamp: ${{ steps.deploy_metadata.outputs.deployment_timestamp }}\u0026#34; \\ -m \u0026#34;Operator: ${{ github.actor }}\u0026#34; \\ -m \u0026#34;Build: ${{ needs.build.outputs.build_id }}\u0026#34; \\ -m \u0026#34;Image: ${{ needs.build.outputs.image_digest }}\u0026#34; git push origin \u0026#34;${{ steps.deploy_metadata.outputs.deployment_id }}\u0026#34; - name: Create deployment record run: | # Write deployment record for correlation cat \u0026gt; deployment-record.json \u0026lt;\u0026lt; EOF { \u0026#34;deploymentId\u0026#34;: \u0026#34;${{ steps.deploy_metadata.outputs.deployment_id }}\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;${{ steps.deploy_metadata.outputs.deployment_timestamp }}\u0026#34;, \u0026#34;environment\u0026#34;: \u0026#34;production\u0026#34;, \u0026#34;gitCommit\u0026#34;: \u0026#34;${{ github.sha }}\u0026#34;, \u0026#34;buildId\u0026#34;: \u0026#34;${{ needs.build.outputs.build_id }}\u0026#34;, \u0026#34;imageDigest\u0026#34;: \u0026#34;${{ needs.build.outputs.image_digest }}\u0026#34;, \u0026#34;operator\u0026#34;: \u0026#34;${{ github.actor }}\u0026#34;, \u0026#34;runId\u0026#34;: \u0026#34;${{ github.run_id }}\u0026#34;, \u0026#34;runUrl\u0026#34;: \u0026#34;${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\u0026#34; } EOF # Store in artifact or S3/database cat deployment-record.jsonGitLab CI Example#\r# .gitlab-ci.yml variables: REGISTRY: myregistry.io IMAGE_NAME: myapp DOCKER_DRIVER: overlay2 stages: - build - test - scan - deploy before_script: - export BUILD_ID=\u0026#34;build-$(date +%Y%m%d)-${CI_PIPELINE_ID}\u0026#34; - export BUILD_TIMESTAMP=$(date -u +%Y-%m-%dT%H:%M:%SZ) - export GIT_SHORT_COMMIT=${CI_COMMIT_SHORT_SHA} build: stage: build script: - | docker build \\ --label \u0026#34;org.opencontainers.image.source=${CI_PROJECT_URL}\u0026#34; \\ --label \u0026#34;org.opencontainers.image.revision=${CI_COMMIT_SHA}\u0026#34; \\ --label \u0026#34;org.opencontainers.image.version=${CI_COMMIT_REF_NAME}\u0026#34; \\ --label \u0026#34;org.opencontainers.image.created=${BUILD_TIMESTAMP}\u0026#34; \\ --label \u0026#34;com.myorg.build.id=${BUILD_ID}\u0026#34; \\ --label \u0026#34;com.myorg.build.number=${CI_PIPELINE_ID}\u0026#34; \\ --label \u0026#34;com.myorg.build.url=${CI_PIPELINE_URL}\u0026#34; \\ --label \u0026#34;com.myorg.build.branch=${CI_COMMIT_REF_NAME}\u0026#34; \\ --label \u0026#34;com.myorg.git.commit.short=${GIT_SHORT_COMMIT}\u0026#34; \\ --label \u0026#34;com.myorg.git.author=${GITLAB_USER_NAME}\u0026#34; \\ -t ${REGISTRY}/${IMAGE_NAME}:${GIT_SHORT_COMMIT} \\ -t ${REGISTRY}/${IMAGE_NAME}:latest \\ . - docker push ${REGISTRY}/${IMAGE_NAME}:${GIT_SHORT_COMMIT} - docker push ${REGISTRY}/${IMAGE_NAME}:latest # Get digest and save for later stages - IMAGE_DIGEST=$(docker inspect ${REGISTRY}/${IMAGE_NAME}:latest --format=\u0026#39;{{index .RepoDigests 0}}\u0026#39; | cut -d\u0026#39;@\u0026#39; -f2) - echo \u0026#34;IMAGE_DIGEST=${IMAGE_DIGEST}\u0026#34; \u0026gt; build.env - echo \u0026#34;BUILD_ID=${BUILD_ID}\u0026#34; \u0026gt;\u0026gt; build.env artifacts: reports: dotenv: build.env deploy: stage: deploy environment: name: production dependencies: - build script: - export DEPLOYMENT_ID=\u0026#34;deploy-prod-$(date +%Y%m%d%H%M%S)\u0026#34; - export DEPLOYMENT_TIMESTAMP=$(date -u +%Y-%m-%dT%H:%M:%SZ) - | kubectl set image deployment/myapp \\ myapp=${REGISTRY}/${IMAGE_NAME}@${IMAGE_DIGEST} kubectl annotate deployment/myapp \\ git.commit=\u0026#34;${CI_COMMIT_SHA}\u0026#34; \\ git.branch=\u0026#34;${CI_COMMIT_REF_NAME}\u0026#34; \\ git.repo=\u0026#34;${CI_PROJECT_URL}\u0026#34; \\ build.id=\u0026#34;${BUILD_ID}\u0026#34; \\ build.number=\u0026#34;${CI_PIPELINE_ID}\u0026#34; \\ build.url=\u0026#34;${CI_PIPELINE_URL}\u0026#34; \\ deployment.id=\u0026#34;${DEPLOYMENT_ID}\u0026#34; \\ deployment.timestamp=\u0026#34;${DEPLOYMENT_TIMESTAMP}\u0026#34; \\ deployment.operator=\u0026#34;${GITLAB_USER_NAME}\u0026#34; \\ image.digest=\u0026#34;${IMAGE_DIGEST}\u0026#34; \\ --overwriteKubernetes Deployment Template#\r# k8s/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: myapp namespace: production labels: app: myapp version: \u0026#34;1.0.0\u0026#34; environment: production team: platform annotations: # These will be populated by CI/CD git.commit: \u0026#34;placeholder\u0026#34; git.branch: \u0026#34;placeholder\u0026#34; git.repo: \u0026#34;https://github.com/myorg/myrepo\u0026#34; build.id: \u0026#34;placeholder\u0026#34; build.number: \u0026#34;placeholder\u0026#34; build.url: \u0026#34;placeholder\u0026#34; deployment.id: \u0026#34;placeholder\u0026#34; deployment.timestamp: \u0026#34;placeholder\u0026#34; deployment.operator: \u0026#34;placeholder\u0026#34; image.digest: \u0026#34;placeholder\u0026#34; # Documentation docs.url: \u0026#34;https://docs.myorg.com/myapp\u0026#34; oncall.team: \u0026#34;platform-team\u0026#34; oncall.slack: \u0026#34;#platform-oncall\u0026#34; spec: replicas: 3 selector: matchLabels: app: myapp template: metadata: labels: app: myapp version: \u0026#34;1.0.0\u0026#34; environment: production annotations: # Propagate to pods git.commit: \u0026#34;placeholder\u0026#34; deployment.id: \u0026#34;placeholder\u0026#34; prometheus.io/scrape: \u0026#34;true\u0026#34; prometheus.io/port: \u0026#34;9090\u0026#34; spec: containers: - name: myapp image: myregistry.io/myapp:latest # Will be overridden by digest imagePullPolicy: Always # Inject correlation IDs as environment variables env: - name: GIT_COMMIT value: \u0026#34;placeholder\u0026#34; # Set by CI/CD - name: BUILD_ID value: \u0026#34;placeholder\u0026#34; - name: DEPLOYMENT_ID value: \u0026#34;placeholder\u0026#34; - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP - name: NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName ports: - containerPort: 8080 name: http - containerPort: 9090 name: metrics resources: requests: memory: \u0026#34;256Mi\u0026#34; cpu: \u0026#34;100m\u0026#34; limits: memory: \u0026#34;512Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; livenessProbe: httpGet: path: /health port: 8080 initialDelaySeconds: 30 periodSeconds: 10 readinessProbe: httpGet: path: /ready port: 8080 initialDelaySeconds: 5 periodSeconds: 5Dockerfile with Metadata#\r# Dockerfile FROM node:18-alpine AS builder # Build args for metadata ARG GIT_COMMIT=unknown ARG BUILD_ID=unknown ARG BUILD_NUMBER=unknown ARG BUILD_TIMESTAMP=unknown ARG BUILD_URL=unknown WORKDIR /app COPY package*.json ./ RUN npm ci --only=production COPY . . RUN npm run build # Production image FROM node:18-alpine # Inject metadata as OCI labels LABEL org.opencontainers.image.source=\u0026#34;https://github.com/myorg/myrepo\u0026#34; LABEL org.opencontainers.image.revision=\u0026#34;${GIT_COMMIT}\u0026#34; LABEL org.opencontainers.image.created=\u0026#34;${BUILD_TIMESTAMP}\u0026#34; LABEL org.opencontainers.image.version=\u0026#34;1.0.0\u0026#34; LABEL org.opencontainers.image.title=\u0026#34;MyApp\u0026#34; LABEL org.opencontainers.image.description=\u0026#34;My application\u0026#34; # Custom labels for our domain model LABEL com.myorg.build.id=\u0026#34;${BUILD_ID}\u0026#34; LABEL com.myorg.build.number=\u0026#34;${BUILD_NUMBER}\u0026#34; LABEL com.myorg.build.url=\u0026#34;${BUILD_URL}\u0026#34; LABEL com.myorg.build.timestamp=\u0026#34;${BUILD_TIMESTAMP}\u0026#34; LABEL com.myorg.git.commit=\u0026#34;${GIT_COMMIT}\u0026#34; LABEL com.myorg.team=\u0026#34;platform\u0026#34; LABEL com.myorg.oncall=\u0026#34;platform-team\u0026#34; # Also available as environment variables in running container ENV GIT_COMMIT=${GIT_COMMIT} \\ BUILD_ID=${BUILD_ID} \\ BUILD_NUMBER=${BUILD_NUMBER} WORKDIR /app COPY --from=builder /app/node_modules ./node_modules COPY --from=builder /app/dist ./dist COPY package.json ./ EXPOSE 8080 9090 CMD [\u0026#34;node\u0026#34;, \u0026#34;dist/index.js\u0026#34;]These templates provide complete traceability from code to runtime!\n"},{"id":44,"href":"/musings/docs/misc/k8s-mcp-servers/","title":"K8s Mcp Servers","section":"Miscellaneous","content":"I\u0026rsquo;ll search for information about open source Kubernetes MCP servers for you.Based on my research, here\u0026rsquo;s what I found about open source Kubernetes MCP servers:\nAvailable Open Source Kubernetes MCP Servers#\rSeveral open source options exist:\n1. containers/kubernetes-mcp-server (Red Hat/Podman team)\nNative Go implementation that interacts directly with the Kubernetes API No external dependencies (no kubectl, Helm, Node.js, or Python required) Supports both Kubernetes and OpenShift Available as native binaries for Linux, macOS, Windows, plus npm/Python packages and container images Features: CRUD operations on all resources, pod management, Helm support, KubeVirt VMs 2. Flux159/mcp-server-kubernetes\nTypeScript/Node.js-based implementation Uses kubectl and Helm under the hood Available as npm package Non-destructive mode support with secrets masking Includes troubleshooting prompts and workflows 3. feiskyer/mcp-kubernetes-server\nPython-based implementation Available on PyPI Also relies on kubectl and Helm commands 4. Amazon EKS MCP Server\nAWS-specific, now offered as fully managed service (preview) Open source version available for local installation EKS-optimized with CloudWatch integration 5. Stacklok\u0026rsquo;s Kubernetes MCP Server (via ToolHive)\nGo-based implementation Focused on enterprise security with RBAC, network policies Deployed via Kubernetes Operator Maturity \u0026amp; Production Readiness#\rDeveloper Preview / Early Stage: Most of these servers are in developer preview or early development stages. Red Hat explicitly describes their implementation as a \u0026ldquo;developer preview,\u0026rdquo; and AWS\u0026rsquo;s fully managed version is also in \u0026ldquo;preview.\u0026rdquo;\nProduction Considerations:\ncontainers/kubernetes-mcp-server: Most mature in terms of architecture (native Go, no external dependencies, high performance). Red Hat recommends running with dedicated service accounts and read-only mode as safeguards AWS EKS MCP Server: The fully managed version addresses production concerns with cloud hosting, but still in preview Stacklok/ToolHive: Explicitly positioned for production with security-first design, though still emerging Security Posture: All implementations recommend:\nRunning with least-privilege service accounts Using read-only or non-destructive modes initially RBAC-controlled access Secrets masking for sensitive data Functionality Provided#\rCommon capabilities across servers:\nResource Management:\nList, get, create, update, delete any Kubernetes resources Support for custom resources Multi-cluster support (via kubeconfig) Pod Operations:\nView logs Execute commands in pods Port forwarding Resource usage metrics Cleanup of problematic pods Application Deployment:\nHelm chart installation/upgrade YAML manifest application Manifest generation from natural language Troubleshooting:\nSystematic diagnostic workflows Natural language queries about cluster state Error analysis and remediation suggestions Additional Features:\nVirtualMachine management (containers/kubernetes-mcp-server with KubeVirt) CloudWatch integration (AWS EKS) Network policy enforcement (ToolHive) Bottom Line#\rWhile there are several solid open source options, none are considered production-grade yet. They\u0026rsquo;re in the \u0026ldquo;safe to experiment with, use caution in production\u0026rdquo; stage. The containers/kubernetes-mcp-server appears most technically mature due to its native implementation, but all vendors recommend starting with read-only access and gradually expanding permissions as teams gain confidence.\nHere are the GitHub links for the Kubernetes MCP server projects:\nMain Open Source Projects#\r1. containers/kubernetes-mcp-server (Red Hat/Podman team)\nhttps://github.com/containers/kubernetes-mcp-server 2. Flux159/mcp-server-kubernetes\nhttps://github.com/Flux159/mcp-server-kubernetes 3. feiskyer/mcp-kubernetes-server\nhttps://github.com/feiskyer/mcp-kubernetes-server 4. Stacklok ToolHive (MCP server deployment framework)\nhttps://github.com/stacklok/mcp-go (the Kubernetes MCP server implementation) ToolHive repository (for deployment/management) 5. k8s-mcp-server (lightweight Go implementation)\nRepository mentioned in Kubernetes discussion forums but specific link would need to be searched 6. Amazon EKS MCP Server\nThe open source version should be available on AWS\u0026rsquo;s GitHub, though the search results primarily discussed the managed service Related Projects#\rkmcp (Kubernetes MCP Controller for deploying MCP servers)\nPart of the kagent.dev ecosystem for deploying MCP servers on Kubernetes https://kagent.dev/docs/kmcp/ The most actively developed and documented appear to be the containers/kubernetes-mcp-server and Flux159/mcp-server-kubernetes projects based on the search results.\n"},{"id":45,"href":"/musings/docs/misc/non-helm-schema-validation/","title":"Non Helm Schema Validation","section":"Miscellaneous","content":"Yes, there are several tools that can validate Kubernetes manifests independently of Helm:\nkubeval - Validates Kubernetes configuration files against the expected schema. Works with individual files or directories. It\u0026rsquo;s standalone and doesn\u0026rsquo;t require a cluster.\nkubeconform - A faster, more modern alternative to kubeval. Supports newer Kubernetes versions and CRDs. Can validate multiple files and supports schema customization.\nkube-score - Goes beyond basic schema validation to perform static analysis and best practice checks. It validates syntax but also flags potential issues like missing resource limits, security contexts, etc.\nkubectl \u0026ndash;dry-run - While technically part of kubectl, you can use kubectl apply --dry-run=client -f manifest.yaml or kubectl apply --dry-run=server -f manifest.yaml to validate without applying. The server option validates against your cluster\u0026rsquo;s actual API.\nDatree - A policy enforcement tool that includes schema validation plus custom policy checks. Can run in CI/CD pipelines.\nPluto - Specifically checks for deprecated API versions, which is useful when upgrading Kubernetes versions.\nAll of these work with both single manifest files and merged/multi-document YAML files. kubeconform is generally recommended as the most actively maintained option with the best performance and features.\n"},{"id":46,"href":"/musings/docs/misc/overview/","title":"Overview","section":"Miscellaneous","content":"Perfect - I now have a clear picture of your situation. Let me provide a high-level conceptual approach to release management for your platform.\nCore Strategy: Progressive Delivery with Blast Radius Containment#\rThe fundamental principle is to create progressive gates that increase confidence before platform changes reach all developers, while instrumenting the platform to detect problems early.\n1. Multi-Stage Platform Environments#\rCreate separate instances of your platform infrastructure that mirror the promotion path:\nPlatform Dev: Unstable, rapid iteration environment for platform team Platform Staging: Stable environment that mirrors production configuration Platform Production: Where all developers work Key concept: These aren\u0026rsquo;t just namespace separations - they\u0026rsquo;re separate instances of ArgoCD, Keycloak, Vault, Jenkins, etc. This gives you true isolation to test platform changes without any risk to developers.\nThe platform staging environment should ideally include:\nA subset of real application workloads (synthetic or volunteered \u0026ldquo;canary\u0026rdquo; teams) Automated tests that validate the full developer workflow end-to-end Configuration that matches production as closely as possible 2. Gitops Promotion Flow#\rStructure your Git repositories to enforce promotion gates:\nConceptual flow:\nPlatform changes merged to main ‚Üí auto-deploy to Platform Dev Manual/automated promotion ‚Üí Platform Staging (via PR or tag) Soak period + validation gates ‚Üí Platform Production (via PR or tag) Critical principle: Production platform configs should be in a protected branch/path that requires explicit promotion, not automatic sync from every merge.\n3. Validation \u0026amp; Testing Gates#\rBefore promoting platform changes from Staging ‚Üí Production:\nAutomated validation:\nHealth checks on all platform components (ArgoCD can sync, Vault is unsealed, Keycloak auth works) End-to-end workflow tests: simulate a developer deploying an app from Git ‚Üí running in cluster Integration tests: verify Jenkins can trigger builds, apps can authenticate with Keycloak, secrets sync from Vault Regression suite: ensure existing applications continue to function Manual validation:\nSoak time (e.g., 24-48 hours in staging with no issues) Platform team review of logs/metrics Optional: Canary developer team validates their actual workflows in staging 4. Observability \u0026amp; Blast Radius Detection#\rYou need leading indicators that tell you when a platform change has broken something:\nPlatform health metrics (not just logs):\nArgoCD sync success/failure rates across all applications Application deployment success rates (track pods failing to start) Authentication success/failure rates (Keycloak) Secret access patterns (Vault) Build success rates (Jenkins) Developer impact signals:\nTime-to-deploy metrics (are deployments taking longer after a platform change?) Error rate spikes in platform components Support requests or incident reports Concept: Establish baselines in staging, then monitor for deviations in production after changes. Set up alerts that fire if metrics degrade post-deployment.\n5. Change Management Practices#\rCategorize changes by risk:\nLow risk: Configuration tweaks, resource limit adjustments, minor version updates Medium risk: Feature flag changes, new platform capabilities, dependency updates High risk: Major version upgrades (ArgoCD, Kubernetes), authentication changes, core GitOps workflow modifications Apply different gates based on risk:\nLow risk: Automated promotion after passing staging validation Medium risk: Extended soak period, manual approval High risk: Maintenance window, canary deployment, detailed runbooks, extra communication 6. Handling the \u0026ldquo;ArgoCD Paradox\u0026rdquo;#\rYour biggest fear - ArgoCD itself breaking - requires special handling:\nConceptual approaches:\nOut-of-band recovery mechanism: Have a way to manage ArgoCD that doesn\u0026rsquo;t depend on ArgoCD (e.g., Helm charts, Terraform, or manual kubectl) ArgoCD self-management: If ArgoCD manages itself, ensure it\u0026rsquo;s in a separate, privileged namespace with extra safeguards Backup sync mechanism: Keep documented procedures for manual recovery of critical platform components Progressive rollout: Update ArgoCD in staging first, verify for extended period before production 7. Communication \u0026amp; Coordination#\rDeveloper notification strategy:\nAdvance notice of platform maintenance windows Clear communication about what\u0026rsquo;s changing and potential impacts Status page or notification channel for platform health Rollback communication if issues detected Change windows:\nSchedule platform changes during low-activity periods Batch related changes together to minimize disruption frequency Maintain a change calendar visible to developers Key Mindset Shifts#\rPlatform as a product: Treat your platform with the same release rigor as a product team treats their application Fail in staging, not production: The staging environment exists to absorb all the risk and unknowns Observability first: You can\u0026rsquo;t manage what you can\u0026rsquo;t measure - invest in metrics before making risky changes Gradual confidence building: Each gate should increase confidence that the change is safe What This Gives You#\rUnknown ‚Üí Known blast radius: Testing in staging reveals issues before they hit developers Rollback becomes prevention: Most issues caught before production, not after Confidence in changes: Data-driven promotion decisions rather than \u0026ldquo;hope and pray\u0026rdquo; Reduced MTTR: When issues do occur, you have metrics to quickly identify impact and cause Does this conceptual framework align with what you\u0026rsquo;re looking for? Would you like me to dive deeper into any particular aspect?\n"},{"id":47,"href":"/musings/docs/misc/prompt-guide/","title":"Production Readiness Deep-Dive Prompt Guide","section":"Miscellaneous","content":"Production Readiness Deep-Dive Prompt Guide#\rHow to Use This Guide#\rThis guide contains a collection of prompts designed to generate detailed, in-depth information on specific production readiness topics. The prompts are structured to work with AI assistants to produce comprehensive technical guidance.\nContents#\rThis prompt contains usage instructions explaining the pattern, one priming prompt to establish context for AI conversations, and 20 topic-specific prompts.\nThe prompts cover:\nMulti-replica testing Data durability Observability Runbooks Blue/green deployment Load testing Chaos engineering Service mesh Resource management Monitoring/alerting Secrets management Configuration management Persistent storage Database connections Health checks Graceful shutdown Logging strategy Incident response Pre-production validation Capacity planning Usage Pattern#\rStart a new conversation with your AI assistant for each topic area you want to explore Copy and paste the Priming Prompt (below) to establish context Copy and paste one or more Topic-Specific Prompts to get detailed guidance on that area Continue the conversation by asking follow-up questions or requesting clarifications Repeat with additional topic prompts in the same conversation if they\u0026rsquo;re related Why This Structure?#\rThe priming prompt establishes who you are (experienced traditional ops professional), what you\u0026rsquo;re doing (moving to Kubernetes), and what kind of information you need (actionable, vendor-neutral guidance). This context allows the topic-specific prompts to be concise while still generating high-quality, relevant responses.\nTips for Best Results#\rBe specific in follow-ups: If a response is too high-level, ask for concrete examples or step-by-step procedures Request checklists: Ask for evaluation checklists or readiness assessments for each topic Ask for scenarios: Request specific failure scenarios or edge cases to consider Combine related topics: You can use multiple topic prompts in one conversation if they\u0026rsquo;re closely related Adapt to your context: Mention your specific technologies (database type, programming languages) to get more targeted advice Priming Prompt#\rCopy and paste this prompt at the start of each new conversation to establish context:\nI am an experienced technical architect with a background in traditional Unix and PC server deployments. I\u0026#39;m responsible for taking a microservices application that was developed and tested using docker-compose with single replicas of each service, and deploying it to production on a managed Kubernetes service in the cloud.\rMy goals are to:\r- Ensure the application scales with load\r- Make it robust under non-ideal conditions - Achieve data durability (resilient to application and hardware failures)\r- Provide predictable, manageable operations for our Tier 2 operations team\rOur operations model has three tiers:\r- Tier 1: End users and their supervisors handle basic questions\r- Tier 2: Operations platform personnel monitor systems and handle initial issue remediation\r- Tier 3: My development and DevOps team handles complex issues and code changes\rI want to minimize Tier 3 escalations by empowering Tier 2 with proper tools, documentation, runbooks, and observability.\rWe are using:\r- Managed Kubernetes service (cloud provider)\r- Cloud-managed databases (not databases running in Kubernetes)\r- A platform-provided CI/CD pipeline with dev/staging/production environments\r- Blue/green deployment strategy\r- Service mesh (for traffic management and observability)\rPlease provide guidance that is:\r- Vendor-neutral but acknowledges managed services context\r- Actionable and practical, not just theoretical\r- Focused on operational excellence and Tier 2 enablement\r- Appropriate for someone experienced with traditional ops but new to distributed systems\rI understand distributed systems concepts but need help translating them into concrete practices for my production environment. Topic-Specific Prompts#\r1. Multi-Replica State Discovery and Testing#\rProvide detailed guidance on testing whether my microservices are truly stateless and can run as multiple replicas. Our services have only been tested with single replicas in docker-compose.\rCover:\r- Specific test scenarios that reveal hidden state dependencies\r- How to design load tests that expose race conditions and concurrency issues\r- What patterns to look for in code that indicate state problems (sessions, file I/O, caches, singletons)\r- A testing progression plan (how many replicas to test with, for how long)\r- Tools and techniques for discovering inter-pod dependencies\r- How to validate that session management, file uploads, and caching work correctly with multiple replicas\r- Common pitfalls when moving from 1 to N replicas\rInclude a checklist of specific tests to perform and questions to ask about the application architecture.2. Data Durability and Backup Strategies#\rProvide comprehensive guidance on protecting data durability in a Kubernetes environment where we use cloud-managed databases and may need persistent volumes for application data.\rCover:\r- How to categorize data by criticality and choose appropriate protection strategies\r- Backup strategies for cloud-managed databases (frequency, retention, testing)\r- RPO and RTO definition and how to meet different requirements\r- Persistent volume backup approaches for application data\r- Object storage strategies for unstructured data (uploads, generated files)\r- Disaster recovery planning and testing procedures\r- How to validate that backups actually work (restore testing)\r- Monitoring backup health and alerting on failures\r- Documentation requirements for recovery procedures\rInclude a checklist for assessing current data protection posture and identifying gaps.3. Observability Architecture for Operations#\rProvide detailed guidance on building an observability system that enables Tier 2 operations personnel to diagnose and resolve issues without developer expertise.\rCover:\r- What metrics to collect and why (the \u0026#34;golden signals\u0026#34; and beyond)\r- Structured logging design for distributed systems (correlation IDs, log levels, JSON formatting)\r- How to implement distributed tracing and when it\u0026#39;s valuable\r- Designing Grafana dashboards that answer operational questions\r- What makes a dashboard useful vs. overwhelming for non-developers\r- Alert design philosophy (what to alert on, how to write actionable alerts)\r- Log aggregation and search strategies for troubleshooting\r- How to make metrics, logs, and traces correlate for effective investigation\r- Retention policies for metrics and logs\rInclude a checklist of observability capabilities needed and dashboard design principles.4. Runbook Development for Non-Developer Operations#\rProvide detailed guidance on creating effective runbooks that enable Tier 2 operations personnel to resolve issues independently.\rCover:\r- The anatomy of an effective runbook (structure, required sections)\r- How to write diagnosis steps that non-developers can follow\r- How to write resolution procedures that are clear and safe\r- When to escalate vs. when to remediate\r- How to identify what runbooks you need (common failure patterns)\r- Maintaining and updating runbooks based on incidents\r- Linking runbooks to alerts and dashboards\r- Examples of good vs. bad runbook content\r- How to test runbooks with the operations team\r- Creating an operations handbook as an overview document\rInclude templates for common runbook types and a checklist of runbook quality criteria.5. Blue/Green Deployment Implementation#\rProvide detailed guidance on implementing blue/green deployments for zero-downtime releases in Kubernetes.\rCover:\r- How blue/green deployment works conceptually\r- Why blue/green is valuable for mission-critical applications\r- Implementation in Kubernetes (Services, labels, selectors)\r- Managing database schema changes with blue/green (backward compatibility)\r- Health check design for pre-traffic validation\r- Rollback procedures and when to use them\r- Monitoring during blue/green switches\r- Communication and coordination process\r- Handling stateful services in blue/green\r- Cost implications of running two environments\r- Integration with CI/CD pipelines\rInclude a checklist of blue/green readiness and a decision framework for when to rollback.6. Load Testing Strategy and Execution#\rProvide detailed guidance on load testing a distributed microservices application before production deployment.\rCover:\r- Designing realistic load test scenarios (user patterns, traffic mix)\r- How to establish performance baselines\r- Progressive load testing (baseline ‚Üí expected ‚Üí peak ‚Üí breaking point)\r- What metrics to measure during load testing\r- How long to run sustained load tests and why\r- Identifying bottlenecks and resource constraints\r- Testing auto-scaling behavior under load\r- Load testing tools and approaches for Kubernetes\r- Interpreting load test results and identifying problems\r- When load testing indicates production readiness vs. more work needed\rInclude a checklist of load testing scenarios to execute and success criteria.7. Chaos Engineering for Resilience Validation#\rProvide detailed guidance on chaos engineering practices to validate system resilience before and after production deployment.\rCover:\r- What chaos engineering is and why it\u0026#39;s valuable\r- Starting with chaos testing in non-production environments\r- Types of failures to inject (pod kills, node failures, network issues, resource exhaustion, dependency failures)\r- How to design chaos experiments (hypothesis, execution, observation)\r- Tools for chaos testing in Kubernetes\r- Measuring system resilience (recovery time, user impact)\r- Building confidence to run chaos tests in production\r- What to do when chaos tests reveal critical weaknesses\r- Creating a chaos testing program (frequency, scope, evolution)\r- Documenting chaos test results and improvements\rInclude a progression plan for chaos testing and a checklist of failure scenarios to validate.8. Service Mesh Implementation and Configuration#\rProvide detailed guidance on implementing and configuring a service mesh for traffic management, resilience, and observability.\rCover:\r- What a service mesh is and what problems it solves\r- When you need a service mesh vs. when it\u0026#39;s overkill\r- How service mesh works (sidecars, control plane, data plane)\r- Key capabilities: traffic routing, retries, circuit breaking, timeouts, load balancing\r- Observability benefits (distributed tracing, service-to-service metrics)\r- Security features (mutual TLS, service identity)\r- What application changes are needed to work with a service mesh\r- Configuration patterns (timeouts, retry policies, circuit breakers)\r- Operations implications (new debugging surface, configuration management)\r- How to use service mesh observability tools for troubleshooting\r- Training operations team on service mesh concepts\rInclude a checklist of service mesh readiness and configuration review points.9. Kubernetes Resource Management and Sizing#\rProvide detailed guidance on setting CPU and memory requests/limits for Kubernetes pods and planning cluster capacity.\rCover:\r- Understanding requests vs. limits and how they affect scheduling\r- How to determine appropriate resource values (load testing, monitoring)\r- Consequences of under-provisioning (OOMKilled, throttling) and over-provisioning (waste, cost)\r- Horizontal Pod Autoscaler (HPA) configuration and testing\r- Vertical Pod Autoscaler (VPA) use cases\r- Setting minimum and maximum replica counts\r- Quality of Service (QoS) classes and their implications\r- Node capacity planning and headroom\r- Resource quota patterns for teams/namespaces\r- Monitoring resource utilization vs. requests/limits\rInclude a checklist for resource sizing and a process for tuning based on production metrics.10. Monitoring and Alerting Design#\rProvide detailed guidance on designing monitoring and alerting that enables operations to detect and respond to issues effectively.\rCover:\r- What to monitor (application, infrastructure, business metrics)\r- Choosing the right metrics (signal vs. noise)\r- Alert design principles (actionable, timely, contextual)\r- Alert severity levels and when to use them\r- Threshold tuning to minimize false positives\r- Alert fatigue and how to avoid it\r- Notification channels and escalation paths\r- Alert messages: what to include, how to phrase them\r- Linking alerts to runbooks and dashboards\r- Testing alerting (triggering test alerts, validating delivery)\r- Alert review and refinement process\rInclude a checklist of alerting best practices and an alert quality rubric.11. Secrets Management Strategy#\rProvide detailed guidance on managing secrets (passwords, API keys, certificates) in Kubernetes securely.\rCover:\r- Kubernetes Secrets vs. external secret management systems\r- Encryption at rest for Kubernetes Secrets\r- RBAC for limiting secret access\r- Secret rotation strategies and automation\r- Mounting secrets as environment variables vs. volume mounts\r- Integration with external secret managers (Vault, cloud secret services)\r- Secrets in CI/CD pipelines (how to handle securely)\r- Audit logging for secret access\r- What to do when secrets are compromised\r- Documentation of secret locations without exposing values\r- Operations team secret access policies\rInclude a checklist of secret security practices and a secret rotation procedure template.12. Configuration Management Across Environments#\rProvide detailed guidance on managing application configuration across dev, staging, and production environments in Kubernetes.\rCover:\r- ConfigMaps for non-sensitive configuration\r- Structuring configuration (environment variables, files, both)\r- Configuration per environment (dev, staging, production)\r- Immutable configuration strategy (deploy new version vs. update in place)\r- Configuration validation before deployment\r- Version control for configuration\r- How applications should consume configuration\r- Handling configuration changes (do they require restarts?)\r- Configuration documentation for operations\r- Troubleshooting configuration issues\rInclude a checklist of configuration management best practices.13. Persistent Storage Strategy#\rProvide detailed guidance on using persistent volumes in Kubernetes for application data that must survive pod restarts.\rCover:\r- When to use persistent volumes vs. object storage vs. external databases\r- StatefulSets vs. Deployments for stateful services\r- Storage classes and performance characteristics\r- Provisioning persistent volumes (static vs. dynamic)\r- Volume expansion and capacity planning\r- Backup strategies for persistent volumes (snapshots, replication)\r- Volume access modes (ReadWriteOnce, ReadWriteMany)\r- Monitoring volume usage and alerting on capacity\r- Handling volume failures and data recovery\r- Persistent volume lifecycle (creation, attachment, deletion)\rInclude a checklist of persistent storage considerations and decision criteria.14. Database Connection Management#\rProvide detailed guidance on managing database connections from Kubernetes pods to avoid exhaustion and performance issues.\rCover:\r- Why connection pooling matters in multi-replica environments\r- Connection pool sizing (per pod, total across all pods)\r- Connection pool configuration parameters (min, max, timeout, idle)\r- Using connection pooling proxies (PgBouncer, ProxySQL)\r- Monitoring database connections (active, idle, refused)\r- What happens when connection limits are reached\r- Database connection limits and how to adjust them\r- Graceful handling of connection failures and retries\r- Connection management during scaling events (pods starting/stopping)\r- Load testing database connection behavior\rInclude a checklist of connection management configuration and a troubleshooting guide for connection issues.15. Health Checks and Readiness Probes#\rProvide detailed guidance on implementing health checks, liveness probes, and readiness probes in Kubernetes.\rCover:\r- Difference between liveness, readiness, and startup probes\r- What each probe type should check\r- How Kubernetes uses probe results (traffic routing, restarts)\r- Implementing health check endpoints in applications\r- Probe configuration parameters (delay, timeout, period, threshold)\r- Health check dependencies (database, external APIs)\r- Avoiding cascading failures through health checks\r- Testing health checks (simulating failures)\r- Monitoring health check success rates\r- Health checks during deployments and scaling\rInclude a checklist of health check implementation requirements and common mistakes to avoid.16. Graceful Shutdown and Pod Termination#\rProvide detailed guidance on implementing graceful shutdown in applications to avoid disrupting requests during pod termination.\rCover:\r- Kubernetes pod termination lifecycle (SIGTERM, grace period, SIGKILL)\r- Why graceful shutdown matters (avoiding failed requests during deployments)\r- Implementing shutdown handlers in applications\r- Draining in-flight requests before shutdown\r- Handling long-running operations during shutdown\r- PreStop hooks and when to use them\r- Grace period configuration (how long is enough?)\r- Connection draining from load balancers\r- Testing graceful shutdown\r- Monitoring shutdown behavior\rInclude a checklist of graceful shutdown implementation steps.17. Logging Strategy for Distributed Systems#\rProvide detailed guidance on implementing logging for distributed microservices that enables effective troubleshooting.\rCover:\r- Structured logging (JSON format, consistent fields)\r- Log levels and when to use each (ERROR, WARN, INFO, DEBUG)\r- Correlation IDs for tracing requests across services\r- What to include in every log entry (timestamp, service, pod, correlation ID)\r- Log aggregation and centralization\r- Log retention and storage costs\r- Searching and filtering logs efficiently\r- Avoiding log noise (what not to log)\r- Performance implications of logging\r- Log security (avoiding sensitive data in logs)\r- Operations team log analysis training\rInclude a checklist of logging implementation requirements and log entry design guidelines.18. Incident Response and Post-Mortems#\rProvide detailed guidance on incident response processes and conducting effective post-mortem reviews.\rCover:\r- Incident severity levels and classification\r- Incident response roles (incident commander, communications, technical)\r- Communication during incidents (who to notify, how often)\r- Incident timeline tracking and documentation\r- When to escalate and to whom\r- Blameless post-mortem philosophy\r- Conducting post-mortem meetings effectively\r- Post-mortem documentation (what happened, why, how to prevent)\r- Tracking action items from post-mortems\r- Sharing lessons learned across teams\r- Building incident response muscle through practice\rInclude an incident response checklist and a post-mortem template.19. Pre-Production Validation Gates#\rProvide detailed guidance on establishing validation gates that code must pass before production deployment.\rCover:\r- Types of validation gates (automated tests, manual checks, approvals)\r- Test coverage requirements for production readiness\r- Performance testing gates (load test pass criteria)\r- Security scanning and vulnerability thresholds\r- Configuration validation\r- Health check and smoke test validation\r- Backup and restore testing requirements\r- Runbook validation with operations team\r- Staging environment validation duration\r- Deployment checklist and sign-off process\rInclude a comprehensive pre-production checklist and gate criteria.20. Capacity Planning and Growth Management#\rProvide detailed guidance on capacity planning to ensure the system can handle growth without unexpected failures.\rCover:\r- Establishing current capacity baseline\r- Projecting future growth (users, data, traffic)\r- Resource headroom philosophy (how much spare capacity?)\r- Monitoring capacity utilization trends\r- Triggers for capacity expansion (automated and manual)\r- Scaling dimensions (replicas, nodes, storage, database)\r- Cost implications of over-provisioning vs. under-provisioning\r- Capacity testing (validating system at projected future load)\r- Database capacity planning (storage, connections, query throughput)\r- Network capacity considerations\r- Quarterly capacity review process\rInclude a capacity planning worksheet and monitoring checklist. Advanced Usage#\rCombining Prompts#\rYou can combine related prompts in a single conversation. For example, after using the priming prompt:\nUse the \u0026ldquo;Multi-Replica State Discovery\u0026rdquo; prompt Follow with the \u0026ldquo;Load Testing Strategy\u0026rdquo; prompt Then use the \u0026ldquo;Chaos Engineering\u0026rdquo; prompt This creates a comprehensive testing and validation conversation.\nRequesting Specific Formats#\rWhen using topic prompts, you can request specific output formats:\n\u0026ldquo;Provide this as a step-by-step implementation guide\u0026rdquo; \u0026ldquo;Create this as a decision tree for troubleshooting\u0026rdquo; \u0026ldquo;Format this as a training document for operations personnel\u0026rdquo; \u0026ldquo;Give me this as a table comparing approaches\u0026rdquo; \u0026ldquo;Provide example configurations or code samples\u0026rdquo; Following Up for Depth#\rAfter receiving a response to a topic prompt, dig deeper:\n\u0026ldquo;Can you provide a specific example of [concept] in a [your context] environment?\u0026rdquo; \u0026ldquo;What are the most common mistakes teams make with [topic]?\u0026rdquo; \u0026ldquo;How would this approach differ if we were using [specific technology]?\u0026rdquo; \u0026ldquo;Can you walk through a specific failure scenario and how to diagnose it?\u0026rdquo; \u0026ldquo;What metrics or KPIs should we track for [topic]?\u0026rdquo; Adapting to Your Technology Stack#\rAfter the priming prompt, mention your specific technologies:\n\u0026ldquo;We\u0026rsquo;re using PostgreSQL as our database\u0026rdquo; \u0026ldquo;Our services are written in Java with Spring Boot\u0026rdquo; \u0026ldquo;We\u0026rsquo;re on AWS EKS for Kubernetes\u0026rdquo; \u0026ldquo;We use Istio as our service mesh\u0026rdquo; This allows topic prompts to generate more specific, actionable guidance.\nPrompt Maintenance#\rThese prompts are designed to be relatively timeless, focusing on concepts and patterns rather than specific tool versions. However, you may want to:\nAdd technology-specific variants for your environment Create prompts for topics specific to your application domain Evolve prompts based on what information proves most useful Share and refine prompts across your team Consider the prompts as starting points that can be refined and customized for your organization\u0026rsquo;s specific needs and context.\n"},{"id":48,"href":"/musings/docs/misc/production-readiness-overview/","title":"Production Readiness Guide: From Docker Compose to Kubernetes","section":"Miscellaneous","content":"Production Readiness Guide: From Docker Compose to Kubernetes#\rIntroduction#\rThis guide bridges the gap between traditional single-server deployments and cloud-native Kubernetes environments. If you\u0026rsquo;ve spent years deploying applications on Unix servers or PC hardware, where each service runs as a single process on a known machine, the shift to distributed systems represents both an operational paradigm change and an architectural evolution.\nThe fundamental difference is deceptively simple: instead of one copy of your service running on one machine, you now have N copies running across M nodes, managed by an orchestrator you don\u0026rsquo;t directly control. This change ripples through every aspect of operations‚Äîfrom how you think about state and storage, to how you diagnose failures, to what your operators need to know to keep systems running.\nThis guide focuses on the operational and architectural considerations that matter when your application must scale, survive failures, and be supported by operations personnel who may not be developers. We\u0026rsquo;ll explore what changes when you move from docker-compose with single replicas to Kubernetes with multiple replicas, and what you need to build to ensure your Tier 2 operations team can handle issues without constantly escalating to your development team.\ngraph LR\rA[Traditional:\u0026lt;br/\u0026gt;1 Service\u0026lt;br/\u0026gt;1 Server] --\u0026gt; B[Kubernetes:\u0026lt;br/\u0026gt;N Replicas\u0026lt;br/\u0026gt;M Nodes]\rB --\u0026gt; C[New Challenges]\rC --\u0026gt; D[State Management]\rC --\u0026gt; E[Observability]\rC --\u0026gt; F[Operations]1. Understanding the Replica Reality: Is Your Application Really Stateless?#\rWhen you\u0026rsquo;ve only ever run one copy of each service, you\u0026rsquo;ve never truly tested whether your application can handle multiple concurrent replicas. Hidden assumptions about uniqueness, file system access, and in-memory state can surface immediately when you scale to multiple pods.\nThe Single-Replica Trap#\rApplications developed and tested with single replicas often contain patterns that break under distribution:\nIn-memory caching that assumes all requests hit the same process. User sessions stored in application memory work fine with one replica but fail randomly when load-balanced across multiple pods.\nFile system operations that treat local disk as shared storage. If one replica writes files that another expects to read, you\u0026rsquo;ll encounter intermittent failures based on which pod handles which request.\nSequential processing assumptions where developers expect operations to complete in order because there\u0026rsquo;s only one processor. With multiple replicas, race conditions emerge.\nSingleton patterns and global state that work perfectly when there\u0026rsquo;s truly only one instance, but create conflicts when replicated.\nDiscovery Through Testing#\rBefore you go to production, you must validate true statelessness:\nRun multiple replicas in staging - Deploy at least 3 replicas of each service and run your test suite. Many race conditions only appear with 3+ replicas.\nSession affinity testing - Disable sticky sessions and verify users can complete transactions even when each request hits a different replica.\nFile system audit - Identify every filesystem write. Ask: does this need to be persistent? Shared? Should it use object storage instead?\nLoad testing with replica variation - Test with 2 replicas, then 5, then 10. Watch for errors that correlate with replica count changes.\nChaos testing - Kill random pods during load tests. Does the application recover gracefully?\nCommon Patterns That Need Attention#\rgraph TD\rA[Request] --\u0026gt; B{Load Balancer}\rB --\u0026gt; C[Pod 1\u0026lt;br/\u0026gt;Session in Memory]\rB --\u0026gt; D[Pod 2\u0026lt;br/\u0026gt;Different Session]\rB --\u0026gt; E[Pod 3\u0026lt;br/\u0026gt;Different Session]\rC -.User sees\u0026lt;br/\u0026gt;inconsistent state.-\u0026gt; F[Problem]\rD -.-\u0026gt; F\rE -.-\u0026gt; FSessions: Move to distributed session stores (Redis, managed cache services) or use stateless JWT tokens.\nUploads and generated files: Use object storage (S3, GCS, Azure Blob) instead of local filesystem. Every replica must access the same storage.\nBackground jobs: Ensure job queues prevent duplicate processing. Multiple replicas shouldn\u0026rsquo;t pick up the same work item.\nDatabase connections: Connection pools per replica can overwhelm databases. Use connection pooling proxies or managed database connection management.\nCaching: Implement distributed caching or accept cache inconsistency. Document which caches are per-pod vs. shared.\nTesting Strategy#\rCreate a test plan specifically for multi-replica behavior:\nSmoke tests with replica scaling - Deploy, scale to N replicas, run tests, scale to N+5, run tests again Concurrent user simulation - Load testing tools that hit different replicas per request Data consistency validation - Verify that operations produce identical results regardless of which replica processes them Failure injection - Kill pods, restart pods, network partition pods during active transactions The goal is to discover state-related issues before production deployment, not during a critical outage.\n2. Data Durability: Protecting What Matters#\rIn distributed systems, durability requires conscious architecture. The ephemeral nature of containers means nothing stored in a pod\u0026rsquo;s filesystem survives that pod\u0026rsquo;s termination. Understanding what needs to persist, where it lives, and how it\u0026rsquo;s protected becomes critical.\nThe Persistence Hierarchy#\rNot all data is equal. Categorize your data by criticality:\nMission-critical data that must survive any failure: user transactions, financial records, compliance data. This belongs in managed databases with automated backups, point-in-time recovery, and high availability configurations.\nImportant application state that\u0026rsquo;s expensive to regenerate: cached computations, processed datasets, user uploads. Store in persistent volumes with backup strategies or object storage with versioning.\nTemporary working data that can be safely lost: in-progress calculations, request logs not yet shipped to log aggregation. Ephemeral storage is fine.\nDerived data that can be rebuilt: materialized views, aggregated reports, search indexes. Consider whether the rebuild cost justifies backup complexity.\nDatabase Strategy for Kubernetes#\rSince you\u0026rsquo;re using cloud-managed databases external to Kubernetes, your durability strategy simplifies significantly:\nLeverage managed services: Use RDS, Cloud SQL, Azure Database services. These provide automated backups, replication, and point-in-time recovery without you building the infrastructure.\nDefine RPO and RTO: Recovery Point Objective (how much data can you afford to lose?) and Recovery Time Objective (how quickly must you recover?) drive your backup frequency and testing requirements.\nTest restores regularly: Backups you\u0026rsquo;ve never restored are just hope, not disaster recovery. Schedule quarterly restore drills to non-production environments.\nMonitor backup completion: Treat backup failures as critical alerts. A month of failed backups means you\u0026rsquo;re no longer protected.\ngraph LR\rA[Kubernetes Pods] --\u0026gt; B[Managed Database]\rB --\u0026gt; C[Automated Backups]\rB --\u0026gt; D[Read Replicas]\rC --\u0026gt; E[Point-in-Time\u0026lt;br/\u0026gt;Recovery]\rD --\u0026gt; F[High Availability]Persistent Volumes for Application Data#\rWhen applications need persistent storage beyond databases:\nStatefulSets vs. Deployments - Use StatefulSets when pods need stable, persistent storage. Each pod gets its own persistent volume that follows it through restarts.\nStorage classes and performance - Understand the performance characteristics of your cloud provider\u0026rsquo;s storage options. Not all persistent volumes are equally fast.\nBackup strategies for volumes - Cloud providers offer volume snapshots. Implement scheduled snapshots for important application volumes, but recognize these are point-in-time and may not capture in-flight transactions.\nVolume expansion planning - Start with adequate space and monitor usage. Some storage classes support online expansion; others require downtime.\nObject Storage for Unstructured Data#\rUser uploads, generated reports, and files belong in object storage, not in pods or persistent volumes:\nVersioning and lifecycle policies - Enable versioning to protect against accidental deletion. Implement lifecycle rules to archive or delete old versions.\nCross-region replication - For disaster recovery, replicate critical buckets to another region.\nAccess patterns and costs - Frequent access vs. archive storage have different performance and cost profiles. Design accordingly.\nDisaster Recovery Planning#\rBeyond backups, you need tested procedures:\nDocument recovery procedures - Step-by-step instructions for restoring from backup to a functioning system. Include database restores, volume restores, and configuration restoration.\nPractice disaster scenarios - At least annually, perform a full disaster recovery test. Can you rebuild the entire system from backups in a reasonable timeframe?\nCommunication plans - Who needs to know about data loss incidents? What\u0026rsquo;s the escalation path? Document this before you need it.\n3. Observability: Enabling Tier 2 Operations#\rYour operations team needs to diagnose and resolve issues without understanding your codebase. This requires thoughtfully designed observability‚Äîmetrics, logs, and dashboards that surface problems and their probable causes.\nThe Three Pillars for Operations#\rMetrics answer \u0026ldquo;what is happening?\u0026rdquo; - Request rates, error rates, resource utilization, queue depths. These should be aggregated across all replicas.\nLogs answer \u0026ldquo;why did this happen?\u0026rdquo; - Structured logs with correlation IDs that let operators trace a request across multiple services and replicas.\nTraces answer \u0026ldquo;where is the bottleneck?\u0026rdquo; - Distributed tracing shows request flow through your microservices, highlighting slow or failing components.\nFor Tier 2 operations, metrics and logs are most immediately actionable. Traces are powerful for Tier 3 deep dives.\nDesigning Metrics That Matter#\rAvoid metric sprawl. Focus on metrics that indicate health and enable diagnosis:\nGolden signals for each service:\nRequest rate (requests per second) Error rate (percentage of requests failing) Latency (response time distribution) Saturation (resource utilization - CPU, memory, connections) Infrastructure metrics:\nPod restart counts (indicates crashes) Node resource pressure (CPU, memory, disk) Persistent volume usage Network errors and retransmissions Application-specific metrics:\nBusiness transaction rates (orders processed, reports generated) Queue depths and processing lag External dependency health (database, third-party APIs) Cache hit rates graph TD\rA[Application] --\u0026gt; B[Metrics Export]\rB --\u0026gt; C[Prometheus/Cloud Monitoring]\rC --\u0026gt; D[Grafana Dashboards]\rD --\u0026gt; E[Tier 2 Operators]\rC --\u0026gt; F[Alert Manager]\rF --\u0026gt; EStructured Logging Strategy#\rLogs are only useful if operators can find relevant information quickly:\nLog levels with purpose:\nERROR: Something failed that requires attention WARN: Something unexpected but handled INFO: Normal business operations (transactions, state changes) DEBUG: Detailed information for troubleshooting (disabled in production by default) Structured format: Use JSON logging with consistent fields:\nTimestamp with timezone Service name and version Pod/replica identifier Correlation ID (traces request across services) User/session identifier (when applicable) Message and context data Correlation is critical: When a request flows through Service A ‚Üí Service B ‚Üí Service C, the correlation ID should flow through all three. This lets operators grep logs across services to understand a single transaction\u0026rsquo;s path.\nAvoid log noise: Don\u0026rsquo;t log every request at INFO level in high-traffic services. Log errors, significant state changes, and business events. Trace-level detail should require explicit enablement.\nGrafana Dashboards for Operators#\rDesign dashboards that answer common questions without requiring code knowledge:\nService Health Dashboard (one per service):\nRequest rate over time Error rate and error types Response time percentiles (p50, p95, p99) Active replica count Pod restart frequency Recent error log samples Infrastructure Health Dashboard:\nNode CPU and memory utilization Pod resource requests vs. limits vs. actual usage Persistent volume capacity and usage Network throughput and error rates Business Metrics Dashboard:\nTransaction volumes by type Processing queue depths SLA compliance (if applicable) External dependency status Alert Status Dashboard:\nCurrently firing alerts Alert history and trends Alert fatigue metrics (alerts that fire but are ignored) Each dashboard should have annotations explaining what metrics mean and what values suggest problems. For example: \u0026ldquo;Error rate above 1% indicates service degradation‚Äîcheck error logs.\u0026rdquo;\nAlert Design Philosophy#\rAlerts should be actionable. If an alert fires but operators can\u0026rsquo;t do anything about it, it\u0026rsquo;s noise:\nAlert on symptoms, not causes: Alert when users are impacted (high error rate), not on low-level metrics unless they reliably predict impact.\nProvide context: Alert messages should include:\nWhat is wrong (service X error rate is 5%) Why it matters (users cannot complete transactions) Initial investigation steps (check pod logs for errors, verify database connectivity) Runbook link Avoid alert fatigue: Too many alerts train operators to ignore them. Set thresholds carefully. Use warning vs. critical severity appropriately.\nAlert on trends: Don\u0026rsquo;t just alert when disk is 95% full‚Äîalert when disk usage is growing at a rate that will fill it within 24 hours. This provides time to act before crisis.\n4. Runbooks: Empowering Non-Developer Operations#\rRunbooks transform tribal knowledge into executable procedures. Well-written runbooks let Tier 2 operators resolve issues independently, reserving Tier 3 escalation for truly novel problems.\nAnatomy of an Effective Runbook#\rA good runbook has consistent structure:\nTitle and summary: What problem does this address? Brief symptom description.\nSymptoms and detection: How do operators know this is happening? What alerts fire? What do dashboards show?\nImpact assessment: Is this affecting users? Partially or completely? Which features are degraded?\nDiagnosis steps: How to confirm the root cause. Include commands to run, logs to check, metrics to examine.\nResolution steps: Numbered, sequential actions to resolve the issue. Each step should be precise enough for someone unfamiliar with the system.\nEscalation criteria: When should this be escalated to Tier 3? What information should be collected before escalating?\nPrevention: How can this be prevented in the future? Is this a symptom of needed architectural changes?\nRunbook Example 1: Pod Memory Thrashing#\rProblem: Pods repeatedly killed for OOM (Out Of Memory) and immediately rescheduled, creating a restart loop that degrades service.\nSymptoms:\nHigh pod restart count for specific service (visible in Kubernetes dashboard) Alert: \u0026ldquo;ServiceX pod restart rate exceeds threshold\u0026rdquo; Grafana shows intermittent service availability Pod events show \u0026ldquo;OOMKilled\u0026rdquo; status Impact: Service degraded or unavailable. Requests may timeout or return errors. Users experience intermittent failures.\nDiagnosis:\nCheck pod restart count:\nkubectl get pods -n production | grep service-nameLook for high RESTARTS count.\nExamine pod events:\nkubectl describe pod \u0026lt;pod-name\u0026gt; -n productionLook for \u0026ldquo;OOMKilled\u0026rdquo; in events section.\nCheck memory usage vs. limits:\nkubectl top pod \u0026lt;pod-name\u0026gt; -n productionCompare memory usage against configured limits (visible in pod spec).\nReview memory trends in Grafana: Navigate to Service Health dashboard, examine memory usage over past hour. Is memory climbing steadily or spiking?\nRoot Cause Determination:\nIf memory usage approaches limit before killing: Memory limit too low for workload If memory climbs continuously until kill: Potential memory leak If memory spikes suddenly: Workload burst (e.g., large report generation) Resolution:\nFor memory limit too low (most common):\nCheck current memory request and limit:\nkubectl get deployment \u0026lt;service-name\u0026gt; -n production -o yaml | grep -A 5 resources Increase memory limit (requires deployment update‚ÄîTier 3 action):\nDocument current memory request/limit values Document observed memory usage pattern Escalate to Tier 3 with this data Temporary mitigation: If service is completely down, reduce replica count to zero, then scale back to 1:\nkubectl scale deployment \u0026lt;service-name\u0026gt; -n production --replicas=0\rkubectl scale deployment \u0026lt;service-name\u0026gt; -n production --replicas=1This stops the restart loop temporarily. Alert Tier 3 immediately.\nFor suspected memory leak:\nCollect heap dump (if application supports it) - Tier 3 action Document memory growth rate Escalate to Tier 3 with timeline and metrics For workload burst:\nVerify if spike correlates with known activity (report generation schedule, batch job) If predictable, note timing for capacity planning discussion with Tier 3 If unpredictable, escalate for investigation Escalation Criteria:\nUnable to determine root cause within 15 minutes Resolution requires deployment configuration changes Suspected application code issue (memory leak) Problem persists after temporary mitigation Prevention: Regular capacity review. Memory limits should be set with 30% headroom above normal peak usage.\nRunbook Example 2: Financial Reporting Service Timeout#\rProblem: Financial report generation requests timeout after 60 seconds, preventing users from accessing monthly reports.\nSymptoms:\nAlert: \u0026ldquo;ReportService response time p99 exceeds 60s\u0026rdquo; User reports: \u0026ldquo;I can\u0026rsquo;t download this month\u0026rsquo;s financial report\u0026rdquo; Grafana ReportService dashboard shows high latency for report generation endpoint Error logs show \u0026ldquo;Request timeout after 60000ms\u0026rdquo; Impact: Users cannot generate or download financial reports. Critical for month-end closing processes.\nDiagnosis:\nVerify the specific report type failing:\nCheck error logs for report parameters: kubectl logs -n production deployment/report-service --tail=100 | grep \u0026#34;timeout\u0026#34; Note report period, account range, or other parameters in timeout messages Check database performance:\nNavigate to Database Health dashboard in Grafana Look for slow query alerts or high connection counts Check if database CPU/memory is saturated Examine report service resource usage:\nCheck Report Service dashboard for CPU/memory saturation Compare current resource usage to baseline Review recent changes:\nCheck deployment history: Has report service been updated recently? Check data volume: Is this month\u0026rsquo;s data significantly larger than previous months? Check concurrent report requests: Are many users requesting reports simultaneously? Root Cause Determination:\nDatabase slow query: Database dashboard shows high query time, low throughput Insufficient compute resources: Report service CPU at 100% Data volume spike: This month\u0026rsquo;s transaction volume 5x normal Concurrent load: 20+ simultaneous report requests (normal is \u0026lt;5) Resolution:\nFor database slow query:\nIdentify slow queries in database logs (if accessible) Check for missing indexes or lock contention Escalate to Tier 3 if query optimization needed Temporary: Ask users to stagger report generation rather than requesting all at once For insufficient compute resources:\nScale report service replicas to increase capacity:\nkubectl scale deployment report-service -n production --replicas=5(Current replica count is typically 2; scaling to 5 provides additional capacity)\nMonitor for 5 minutes: Check if report completion rate improves\nIf improved, maintain higher replica count and notify Tier 3 for permanent capacity adjustment\nIf not improved, scale back and escalate\nFor data volume spike:\nVerify transaction count for current period is anomalous Confirm with business stakeholders if data volume is expected (e.g., acquisition, new customer onboarding) Escalate to Tier 3 for query optimization and capacity planning Temporary: Offer users ability to generate reports for sub-ranges (weekly instead of monthly) if report service supports it For concurrent load:\nCheck if report requests correlate with month-end schedule Implement request queuing if supported (check service documentation) Communicate with users about staggering requests Escalate to Tier 3 for capacity planning if this is a recurring monthly pattern Escalation Criteria:\nRoot cause unclear after 20 minutes of investigation Database performance issues persist Scaling replicas doesn\u0026rsquo;t improve performance Problem occurs outside month-end period (suggests underlying issue) Users report critical business impact (month-end closing deadline) Prevention:\nCapacity testing before month-end with expected data volumes Query performance monitoring and optimization Implement report request queuing to smooth load Consider pre-generating common reports asynchronously Runbook Maintenance#\rRunbooks are living documents:\nUpdate after each incident: If operators discover new diagnostic steps or resolution approaches, incorporate them immediately.\nValidate quarterly: Test runbooks against non-production environments. Can a junior operator follow them successfully?\nTrack escalation patterns: If a runbook consistently leads to Tier 3 escalation, either the runbook is incomplete or the issue requires architectural changes.\nLink to monitoring: Every runbook should be linked from relevant alerts and dashboard annotations.\n5. Deployment Patterns: Blue/Green for Zero Downtime#\rTraditional deployments often involve downtime windows: stop the service, deploy new code, start the service. In a mission-critical environment supporting many downstream systems, downtime is not acceptable. Blue/green deployment provides a path to zero-downtime releases.\nWhat is Blue/Green Deployment?#\rBlue/green maintains two complete production environments: blue (current production) and green (new version). At any time, one environment is live and serving traffic. The other is idle or staging the next release.\ngraph LR\rA[Load Balancer] --\u0026gt; B[Blue Environment\u0026lt;br/\u0026gt;v1.5 - Active]\rA -.Future.-\u0026gt; C[Green Environment\u0026lt;br/\u0026gt;v1.6 - Staged]\rB --\u0026gt; D[Database]\rC -.-\u0026gt; DDeployment process:\nBlue is serving production traffic at v1.5 Deploy v1.6 to green environment Test green environment thoroughly (smoke tests, health checks) Switch traffic from blue to green Monitor green for issues If successful, blue becomes the standby environment for next release If problems, switch traffic back to blue (instant rollback) Why Blue/Green for Your Context?#\rMission-critical dependency: Downstream systems rely on your service. Any downtime cascades to those systems.\nInstant rollback: If the new version has issues, reverting is a traffic switch, not a redeployment. Rollback takes seconds, not minutes.\nPre-production validation: The green environment is running the real production configuration. You can validate thoroughly before traffic hits it.\nConfidence in releases: Operators know that any release can be undone instantly, reducing anxiety around deployments.\nImplementation in Kubernetes#\rKubernetes supports blue/green through Services and label selectors:\nLabel environments: Pods are labeled with version (e.g., version: blue or version: green)\nService selector: The Kubernetes Service that fronts your application selects which version receives traffic:\nselector: app: financial-service version: blueSwitching traffic: Update the Service selector to point to green:\nselector: app: financial-service version: greenTools and automation: Your CI/CD platform likely provides blue/green capabilities. Work with platform team to understand the mechanism.\nConsiderations and Challenges#\rDatabase migrations: Both blue and green connect to the same database. Database schema changes must be backward-compatible with the previous version. Deploy schema changes separately, before application code changes.\nStateful services: Blue/green works best for stateless services. If services maintain state, switching traffic can be complex. This is another reason to validate true statelessness.\nResource cost: You\u0026rsquo;re running two full environments. In cloud Kubernetes, this means paying for double the resources during deployment. Accepting this cost is part of achieving zero downtime.\nHealth check design: Green must pass comprehensive health checks before traffic switches. Design health endpoints that verify all dependencies.\nRollback Decision Criteria#\rDefine clear criteria for when to rollback:\nAutomated rollback triggers:\nError rate exceeds threshold (e.g., 5% of requests failing) Response time degradation (e.g., p95 latency doubles) Health check failures Manual rollback criteria:\nCustomer reports of data inconsistency Critical functionality not working (even if error rate looks normal) Unexpected behavior in downstream systems Rollback window: Define how long you monitor before declaring success. Typical: 30-60 minutes of green running with production traffic before decommissioning blue.\nCommunication and Coordination#\rBlue/green requires coordination:\nPre-deployment: Notify operations team of deployment window. Ensure monitoring is active and operators are available.\nDuring deployment: Real-time monitoring of metrics and logs as traffic switches to green.\nPost-deployment: Clear \u0026ldquo;all clear\u0026rdquo; or \u0026ldquo;rollback initiated\u0026rdquo; communication.\nRunbook for rollback: Operators should have a runbook for executing rollback, including who has authority to make the call.\n6. Service Mesh: Traffic Management and Observability#\rA service mesh is infrastructure layer that handles service-to-service communication, providing features like traffic routing, load balancing, retries, circuit breaking, and observability without requiring changes to application code. For a distributed microservices architecture, service mesh addresses cross-cutting concerns that would otherwise need to be implemented in every service.\nWhat Problem Does Service Mesh Solve?#\rIn a microservices architecture, services communicate frequently. Without a service mesh, each service must implement:\nRetry logic: When Service A calls Service B and gets a transient failure, should it retry? How many times? With what backoff?\nCircuit breaking: If Service B is consistently failing, Service A should stop calling it temporarily to avoid cascading failures.\nLoad balancing: When Service A calls Service B, which of the 5 replicas of Service B should receive the request?\nObservability: How do you track request flow from Service A ‚Üí Service B ‚Üí Service C? How do you measure latency at each hop?\nSecurity: How do you ensure traffic between services is encrypted? How do you verify service identity?\nImplementing these in application code is repetitive, error-prone, and creates coupling. Service mesh moves these concerns to infrastructure.\ngraph TD\rA[Service A] --\u0026gt; B[Service Mesh Proxy]\rB --\u0026gt; C[Service B Proxy]\rC --\u0026gt; D[Service B]\rB --\u0026gt; E[Metrics \u0026amp; Traces]\rC --\u0026gt; ECore Service Mesh Capabilities#\rTraffic management: Fine-grained control over routing. Send 95% of traffic to stable version, 5% to new version (canary). Route requests from specific users to a test version. Implement timeouts and retries consistently.\nResilience patterns: Circuit breakers prevent cascading failures. Retries handle transient errors. Timeouts prevent resource exhaustion from slow services.\nObservability: Automatic distributed tracing. Service-to-service latency metrics. Request success and failure rates between services. All without instrumentation code.\nSecurity: Mutual TLS between services. Service identity and authorization. Traffic encryption without application code changes.\nWhen You Need Service Mesh#\rService mesh adds complexity‚Äîcontainers, configuration, learning curve. Consider service mesh when:\nYou have many microservices (more than 5-10). The communication complexity justifies the infrastructure complexity.\nInter-service communication is complex: Services call each other frequently. Failure in one service impacts many others.\nObservability is difficult: You struggle to understand request flow or identify bottlenecks across services.\nYou need traffic control: Canary deployments, A/B testing, or gradual rollouts are business requirements.\nSecurity requirements are stringent: Mutual TLS and service identity verification are mandatory.\nFor simpler architectures (2-3 services with straightforward communication), service mesh may be overkill. Evaluate against your complexity and requirements.\nIntegration Considerations#\rIntroducing service mesh requires application awareness:\nHealth checks: Services must expose health endpoints. The mesh uses these to determine if a service replica is healthy.\nGraceful shutdown: When a pod terminates, it should finish in-flight requests before exiting. The mesh uses this to drain traffic during rollouts.\nConnection handling: Services should handle connection resets gracefully. The mesh may close connections as part of traffic management.\nConfiguration: While the mesh handles traffic routing, someone must configure the policies‚Äîtimeout values, retry counts, circuit breaker thresholds.\nService Mesh and Your Operations Team#\rService mesh provides powerful observability tools for Tier 2:\nService topology visualization: Graphical view of which services call which others, with latency and error rates on each connection.\nRequest tracing: Click on a slow request and see its entire path through the system, with timing at each service.\nTraffic shifting: During incidents, redirect traffic away from problematic service replicas without redeploying.\nHowever, service mesh also requires new knowledge:\nNew debugging surface: When issues arise, is it application code, service mesh configuration, or mesh infrastructure? Operators need training to distinguish.\nConfiguration complexity: Misconfigured service mesh policies can cause outages. Change management for mesh configuration is critical.\nRunbooks updated: Troubleshooting steps must account for the mesh. \u0026ldquo;Check service logs\u0026rdquo; becomes \u0026ldquo;Check service logs and mesh proxy logs.\u0026rdquo;\n7. Secrets and Configuration Management#\rApplications need configuration (database URLs, feature flags) and secrets (database passwords, API keys). In traditional deployments, these lived in config files on known servers. In Kubernetes, where pods are ephemeral and scheduled unpredictably, configuration must be externalized and secrets must be protected.\nConfiguration vs. Secrets#\rConfiguration: Non-sensitive data that varies between environments (dev/staging/production). Examples: service URLs, timeout values, feature flags, replica counts.\nSecrets: Sensitive data that must be protected. Examples: database passwords, API keys, TLS certificates, encryption keys.\nSeparate these conceptually and technically. Configuration can be visible; secrets must be encrypted and access-controlled.\nKubernetes ConfigMaps for Configuration#\rConfigMaps store configuration data as key-value pairs or files, accessible to pods:\nEnvironment variables: Inject configuration as environment variables into containers.\nVolume mounts: Mount configuration files into containers at specific paths.\nUpdates: Changing a ConfigMap doesn\u0026rsquo;t automatically restart pods. Decide if configuration changes require redeployment or if applications watch for changes.\nPer-environment: Maintain separate ConfigMaps for dev, staging, production. Your CI/CD pipeline should deploy the correct ConfigMap for each environment.\nKubernetes Secrets for Sensitive Data#\rKubernetes Secrets store sensitive data, with some additional protections:\nBase64 encoding: Secrets are base64-encoded (not encrypted by default). This prevents casual viewing but is not security.\nEncryption at rest: Configure Kubernetes to encrypt Secrets at rest in etcd. Work with your platform team to verify this is enabled.\nAccess control: Use RBAC to limit which services can read which Secrets. Not every service needs access to every secret.\nRotation: Secrets should be rotated periodically (database passwords, API keys). Plan for how applications will pick up new secrets‚Äîtypically requires pod restart.\nExternal Secrets Management#\rMany organizations use external secret management systems (HashiCorp Vault, AWS Secrets Manager, Azure Key Vault, Google Secret Manager) integrated with Kubernetes:\nBenefits:\nCentralized secret management across multiple clusters Audit logging of secret access Automated rotation Fine-grained access policies Integration: External secret operators sync secrets from external systems into Kubernetes Secrets, keeping applications unchanged while improving secret security.\nIf your platform team provides external secrets integration, use it. If not, Kubernetes Secrets are acceptable with proper encryption and access control.\nConfiguration Management Strategy#\rImmutable configuration: Treat configuration as immutable within an environment. Don\u0026rsquo;t change configuration in running production‚Äîdeploy a new version with updated configuration. This ensures configuration changes are tracked and reproducible.\nVersion control: All ConfigMaps and Secret definitions (values redacted) should be in version control alongside application code. This provides history and rollback capability.\nValidation: Validate configuration before deployment. Missing or malformed configuration should cause deployment failure in staging, not production.\nDocumentation: Document what each configuration value means and acceptable ranges. Operators may need to adjust values during incidents (e.g., increase timeouts during slow database performance).\nSecrets in Operations Runbooks#\rRunbooks should never include secrets. Instead:\nReference secret locations: \u0026ldquo;Database password is in Secret db-credentials key password\u0026rdquo;\nDescribe retrieval process: \u0026ldquo;To view current database connection string: kubectl get secret db-credentials -n production -o yaml | grep connection-string\u0026rdquo;\nEscalation for secret access: If Tier 2 operators need to view secrets during troubleshooting, document the approval and audit process.\n8. Pre-Production Validation and Testing#\rBefore a distributed system goes to production, validation must go beyond \u0026ldquo;the tests pass.\u0026rdquo; You must validate behavior under failure, load, and chaos‚Äîconditions that don\u0026rsquo;t occur in development.\nTesting Pyramid for Distributed Systems#\rUnit tests: Verify individual component behavior. These remain important but don\u0026rsquo;t validate distributed behavior.\nIntegration tests: Verify service-to-service communication. Test that Service A correctly calls Service B with expected contracts.\nEnd-to-end tests: Verify complete user workflows across all services. Critical for catching cross-service issues.\nLoad tests: Verify performance and stability under expected and peak load. Identify bottlenecks and resource exhaustion.\nChaos tests: Verify resilience under failure conditions. Kill pods, introduce network latency, fill disks‚Äîdoes the system recover?\nLoad Testing Strategy#\rLoad testing answers: \u0026ldquo;Can this system handle production traffic?\u0026rdquo;\nRealistic traffic patterns: Don\u0026rsquo;t just hammer one endpoint. Simulate actual user behavior‚Äîbrowsing, transactions, reports‚Äîin realistic proportions.\nBaseline performance: Establish performance baseline in staging with low load. Then increase load to identify degradation points.\nSustained load: Run at expected production load for hours, not minutes. Memory leaks and resource exhaustion take time to manifest.\nPeak load: Test at 2x-3x expected peak traffic. This provides margin for unexpected traffic spikes and validates auto-scaling.\nBreaking point: Continue increasing load until the system fails. Understanding failure modes is valuable‚Äîdoes it degrade gracefully or crash catastrophically?\nMetrics under load:\nResponse time percentiles (p50, p95, p99) Error rates Resource utilization (CPU, memory, network, disk) Database query performance Queue depths if using asynchronous processing Chaos Engineering#\rChaos testing validates that your system handles failures gracefully:\nPod failures: Kill random pods during traffic. Does the system continue serving requests? Do other replicas take over?\nNode failures: Drain or terminate entire nodes. Does Kubernetes reschedule pods? Does traffic route around failed nodes?\nNetwork issues: Introduce latency between services. Introduce packet loss. Does the system timeout gracefully? Do retries work?\nResource exhaustion: Limit CPU or memory for specific services. Does the system degrade gracefully or cascade fail?\nDependency failures: Make external dependencies (database, third-party APIs) slow or unavailable. Does the circuit breaker trigger? Are errors handled?\ngraph TD\rA[Steady State] --\u0026gt; B[Introduce Failure]\rB --\u0026gt; C{System Recovers?}\rC --\u0026gt;|Yes| D[Identify Weakness\u0026lt;br/\u0026gt;Improve Resilience]\rC --\u0026gt;|No| E[Identify Critical\u0026lt;br/\u0026gt;Gap - Fix Required]\rD --\u0026gt; A\rE --\u0026gt; F[Fix Implementation]\rF --\u0026gt; AStart small: Don\u0026rsquo;t run chaos tests in production initially. Validate resilience in staging first. Once confident, consider controlled production chaos testing.\nTools: Chaos Mesh, Litmus, or cloud provider chaos tools. Your platform team may have preferred tools.\nValidation Checklist Before Production#\rBefore declaring production-ready:\nAll services pass health checks under load Multi-replica testing completed (3+ replicas of each service) Load testing at 2x expected peak traffic completed successfully Chaos testing identifies no critical failures Database connection pooling validated (no connection exhaustion) Monitoring and alerting tested (trigger test alerts, verify delivery) Runbooks tested by operations team on staging environment Backup and restore tested successfully Blue/green deployment tested and rolled back successfully Log aggregation verified (can find logs for specific requests) All Tier 2 training completed 9. Additional Considerations#\rCapacity Planning and Auto-Scaling#\rKubernetes can automatically scale replicas based on metrics, but this requires planning:\nHorizontal Pod Autoscaler (HPA): Scales replicas based on CPU, memory, or custom metrics. Configure target utilization (e.g., \u0026ldquo;scale up when average CPU exceeds 70%\u0026rdquo;).\nVertical Pod Autoscaler (VPA): Adjusts CPU and memory requests/limits. Useful for right-sizing resources.\nTesting auto-scaling: Validate that scaling triggers at expected thresholds. Verify that new replicas become healthy before receiving traffic. Test scale-down to ensure it doesn\u0026rsquo;t disrupt active requests.\nLimits on scaling: Set minimum and maximum replica counts. Unlimited scaling can exhaust cluster resources or cause cost surprises.\nMulti-Environment Strategy#\rYour platform provides dev/staging/production environments:\nEnvironment parity: Keep environments as similar as possible. Differences create bugs that only appear in production.\nData in staging: Use realistic data volumes in staging. Small datasets don\u0026rsquo;t reveal performance issues.\nPromotion process: Code and containers promoted through environments, not rebuilt. A container tested in staging is the same container deployed to production.\nConfiguration separation: Only configuration differs between environments. Use separate ConfigMaps and Secrets per environment.\nDocumentation for Sustainability#\rBeyond runbooks, maintain:\nArchitecture documentation: Service dependencies, data flow, external integrations. Updated as architecture evolves.\nOperations handbook: Overview of the system for new operators. How to navigate dashboards, where to find logs, who to escalate to.\nChange log: Document all production changes‚Äîdeployments, configuration updates, infrastructure changes. Invaluable during incident investigation.\nIncident post-mortems: After incidents, document what happened, root cause, and prevention steps. Share learnings across teams.\nContinuous Improvement#\rProduction operations is iterative:\nMonitor escalation patterns: If Tier 2 consistently escalates specific issues, improve runbooks or add automation.\nReduce toil: Repetitive manual tasks are candidates for automation. Scripted remediation reduces time to recovery.\nReview and update: Quarterly review of runbooks, dashboards, and alerts. Remove obsolete items, add new patterns.\nTraining and knowledge sharing: Regular training sessions for operators. Shadowing during incidents builds expertise.\nProduction Readiness Checklist#\rUse this checklist to evaluate your readiness for production deployment. Each item represents a category of concern. If you answer \u0026ldquo;no\u0026rdquo; or \u0026ldquo;unsure\u0026rdquo; to any item, investigate further using the relevant section of this guide.\nApplication Architecture and Testing#\rServices have been tested running with 3+ replicas simultaneously Session management does not rely on in-memory storage in individual pods File uploads and generated files use object storage, not local filesystem Load testing completed at 2x expected peak traffic for sustained period (4+ hours) Chaos testing performed (pod kills, network latency, resource constraints) No singleton patterns or global state that assume single instance Database connection pooling validated‚Äîno connection exhaustion under load Race conditions and concurrency issues investigated and resolved End-to-end tests pass consistently with multiple replicas Data Durability and Backup#\rAll critical data stored in managed databases with automated backups enabled Recovery Point Objective (RPO) and Recovery Time Objective (RTO) defined Database restore procedure documented and tested within last 3 months Backup monitoring and alerting configured (alerts on backup failure) Persistent volumes identified and backup strategy defined Object storage configured with versioning for critical data Disaster recovery plan documented with step-by-step procedures Cross-region replication enabled for critical data (if required by RPO/RTO) Observability and Monitoring#\rMetrics exported for all services (request rate, error rate, latency, saturation) Structured logging implemented with correlation IDs across services Log aggregation configured‚Äîcan search logs across all pods and services Grafana dashboards created for service health, infrastructure health, and business metrics Dashboard annotations explain metrics and alert thresholds Alerts configured for critical symptoms (error rate, latency, resource exhaustion) Alert messages include context and link to relevant runbooks Distributed tracing available (if using service mesh or APM tooling) All metrics and logs retained for at least 30 days for investigation Operations and Runbooks#\rRunbooks created for common failure scenarios (pod OOM, service timeouts, database connectivity) Runbooks tested by operations team in staging environment Each runbook includes symptoms, diagnosis steps, resolution steps, and escalation criteria Runbooks linked from relevant alerts and dashboard annotations Operations team trained on runbook usage and troubleshooting procedures Escalation path documented (Tier 1 ‚Üí Tier 2 ‚Üí Tier 3) Communication plan for incidents defined (who to notify, how to escalate) Operations handbook created with system overview and navigation guide Deployment and Release#\rBlue/green deployment strategy implemented and tested Rollback procedure documented and tested successfully Health check endpoints defined and validated for all services Database migrations are backward-compatible with previous application version Deployment automation includes pre-deployment validation checks Post-deployment monitoring period defined (typically 30-60 minutes) Rollback decision criteria clearly defined (error rate thresholds, etc.) Deployment communication process established (notify operations before deployment) Service Mesh (if applicable)#\rService mesh deployed and configured across all services Traffic management policies defined (timeouts, retries, circuit breakers) Service-to-service communication encrypted (mutual TLS) Operations team trained on service mesh observability tools Runbooks updated to include service mesh proxy logs in troubleshooting Service mesh configuration change management process defined Secrets and Configuration#\rSecrets stored in Kubernetes Secrets or external secrets manager (not in code) Kubernetes Secrets encrypted at rest (verify with platform team) RBAC configured to limit secret access to required services only ConfigMaps defined for each environment (dev/staging/production) Configuration validated before deployment (prevents malformed config in production) All configuration and secret definitions (values redacted) in version control Secret rotation process defined and tested Runbooks reference secret locations, never include actual secrets Pre-Production Validation#\rMulti-replica testing completed in staging (3+ replicas per service) Load testing at peak traffic completed successfully Sustained load testing (4+ hours) shows no memory leaks or resource exhaustion Chaos testing identifies no critical failures Auto-scaling tested and validated (if configured) All monitoring and alerting tested‚Äîcan trigger and receive test alerts Backup and restore tested successfully in non-production environment Blue/green deployment and rollback tested end-to-end Security scanning completed (container vulnerabilities, dependency audits) Capacity and Resource Planning#\rCPU and memory requests/limits defined for all services based on load testing Persistent volume sizes defined with growth projections Horizontal Pod Autoscaler configured with tested thresholds (if using auto-scaling) Minimum and maximum replica counts defined Node capacity sufficient for expected replica counts with headroom Database capacity sufficient for expected data growth over 6-12 months Documentation and Knowledge Transfer#\rArchitecture documentation current and accessible Service dependencies mapped and documented Operations handbook created with system overview All runbooks accessible to operations team (wiki, documentation portal) Training completed for Tier 2 operations team Incident response process documented Post-incident review process defined Contact information for escalations documented and current Compliance and Security (coordinate with platform teams)#\rNetwork policies reviewed and approved by platform security team RBAC permissions reviewed and approved Container security scanning completed Secrets access audited and minimized to required services Compliance requirements reviewed (data residency, retention, etc.) Vulnerability remediation process defined Conclusion#\rMoving from traditional single-server deployments to distributed Kubernetes environments requires rethinking assumptions about state, persistence, observability, and operations. The shift from one replica to N replicas across M nodes is not just a scaling change‚Äîit\u0026rsquo;s an architectural and operational transformation.\nThis guide has covered the critical areas you must address to deploy a production-ready distributed system: validating true statelessness, protecting data durability, enabling operational observability, empowering Tier 2 with effective runbooks, implementing zero-downtime deployments, understanding service mesh value, managing secrets properly, and validating thoroughly before production.\nUse the checklist as your roadmap. Each unchecked item represents risk. Some risks are acceptable in early production; others are not. Prioritize based on your mission-critical requirements and user impact.\nRemember that production operations is iterative. You will discover gaps, refine runbooks, adjust alerts, and improve observability based on real-world experience. Build feedback loops‚Äîfrom incidents to post-mortems to improvements‚Äîand your operational maturity will grow continuously.\nThe detailed prompts in the companion guide provide deep-dive resources for each topic. Use them to expand your knowledge in specific areas as you progress through your production readiness journey.\n"},{"id":49,"href":"/musings/docs/misc/job-plus-configmap-annotation/","title":"Read current state AND capture resourceVersion","section":"Miscellaneous","content":"Good choice - Option 3 is pragmatic and you can iterate quickly. Let me expand on it.\nHow Option 3 Works in Practice#\rThe basic idea is you commit a ConfigMap (or annotation on an existing resource) that represents \u0026ldquo;desired state\u0026rdquo;. A controller mechanism (CronJob, or better - a simple Deployment with a watch loop) continuously checks if desiredState != appliedState and executes the restore workflow when they differ.\nDetailed Look at the Cons#\r1. Less robust than operator pattern#\rThe reconciliation logic lives in shell scripts or simple programs rather than a proper controller framework. This means:\nNo automatic retry with exponential backoff Less structured error handling No queuing of work items Harder to ensure exactly-once semantics 2. CronJob timing issues#\rIf using CronJob:\nMight run every 5 minutes but change happened 4 minutes ago = 4 minute delay Or runs every minute = wasteful, checking state 1,439 times when nothing changed No immediate reaction to git changes 3. State management is manual#\rYou need to explicitly:\nRead current state from cluster Compare with desired state Update \u0026ldquo;last applied\u0026rdquo; after success Handle partial failures (what if ingress disabled but restore failed?) 4. No built-in reconciliation#\rUnlike operators, nothing watches the ConfigMap for changes. You\u0026rsquo;re polling, not reacting.\n5. Race conditions#\rIf multiple instances run (CronJob overlap, manual trigger + CronJob), they might:\nBoth start restoring simultaneously Overwrite each other\u0026rsquo;s state updates Leave system in inconsistent state 6. Complex orchestration is harder#\rMulti-step workflows with dependencies, rollbacks, conditional logic becomes spaghetti in bash.\nStrategies to Minimize Drawbacks#\rUse a Deployment with Watch Loop Instead of CronJob#\rapiVersion: apps/v1 kind: Deployment metadata: name: restore-controller spec: replicas: 1 # Important: only 1 template: spec: serviceAccountName: restore-controller containers: - name: controller image: your-controller:latest command: [\u0026#34;/bin/controller-loop.sh\u0026#34;]The script does:\nwhile true; do kubectl wait --for=condition=ready configmap/restore-state --timeout=60s check_and_reconcile sleep 30 doneBenefits over CronJob:\nContinuous running, faster reaction time replicas: 1 prevents race conditions Can use kubectl wait for event-driven approach Still simple, no operator framework needed Use Optimistic Locking with Resource Versions#\r# Read current state AND capture resourceVersion CURRENT_RV=$(kubectl get cm restore-state -o jsonpath=\u0026#39;{.metadata.resourceVersion}\u0026#39;) DESIRED=$(kubectl get cm restore-state -o jsonpath=\u0026#39;{.data.desiredBackupTimestamp}\u0026#39;) APPLIED=$(kubectl get cm restore-state -o jsonpath=\u0026#39;{.data.lastAppliedTimestamp}\u0026#39;) if [ \u0026#34;$DESIRED\u0026#34; != \u0026#34;$APPLIED\u0026#34; ]; then # Do the restore work... # Update with resourceVersion check - fails if someone else modified kubectl patch cm restore-state \\ --type=json \\ -p=\u0026#34;[{\\\u0026#34;op\\\u0026#34;: \\\u0026#34;replace\\\u0026#34;, \\\u0026#34;path\\\u0026#34;: \\\u0026#34;/data/lastAppliedTimestamp\\\u0026#34;, \\\u0026#34;value\\\u0026#34;: \\\u0026#34;$DESIRED\\\u0026#34;}]\u0026#34; \\ --resource-version=\u0026#34;$CURRENT_RV\u0026#34; fiBenefits:\nPrevents race conditions via optimistic locking Atomic state updates Clear failure signal if collision occurs Add Lease-Based Locking#\rUse Kubernetes Lease objects for distributed locking:\n# Try to acquire lease kubectl create -f - \u0026lt;\u0026lt;EOF || exit 0 apiVersion: coordination.k8s.io/v1 kind: Lease metadata: name: restore-operation-lock spec: holderIdentity: \u0026#34;$POD_NAME\u0026#34; leaseDurationSeconds: 600 EOF # Do work... # Release lease kubectl delete lease restore-operation-lockBenefits:\nMultiple instances can safely coexist Only one actually does work Automatic timeout if pod crashes State Machine in ConfigMap#\rTrack not just desired/applied, but also intermediate states:\napiVersion: v1 kind: ConfigMap metadata: name: restore-state data: desiredBackupTimestamp: \u0026#34;2024-12-10T00:00:00Z\u0026#34; lastAppliedTimestamp: \u0026#34;2024-12-01T00:00:00Z\u0026#34; operationPhase: \u0026#34;Complete\u0026#34; # Idle|InProgress|Failed|Complete operationStarted: \u0026#34;\u0026#34; operationCompleted: \u0026#34;2024-12-01T00:05:00Z\u0026#34; preStepCompleted: \u0026#34;\u0026#34; # Track granular progress restoreCompleted: \u0026#34;\u0026#34; postStepCompleted: \u0026#34;\u0026#34;Controller logic:\ncase \u0026#34;$PHASE\u0026#34; in \u0026#34;Idle\u0026#34;|\u0026#34;Complete\u0026#34;|\u0026#34;Failed\u0026#34;) if [ \u0026#34;$DESIRED\u0026#34; != \u0026#34;$APPLIED\u0026#34; ]; then start_new_operation fi ;; \u0026#34;InProgress\u0026#34;) # Check if stale (started \u0026gt;1hr ago) -\u0026gt; mark Failed # Otherwise, skip (already running) ;; esacBenefits:\nIdempotent: can resume from any step Visible progress in git/cluster Can detect stuck operations Clear state transitions Use Annotations for Immutability#\rInstead of updating ConfigMap, create new ones:\napiVersion: v1 kind: ConfigMap metadata: name: restore-intent-20241210 # Date in name labels: app: restore-controller applied: \u0026#34;false\u0026#34; data: backupTimestamp: \u0026#34;2024-12-10T00:00:00Z\u0026#34;Controller watches for applied: \u0026quot;false\u0026quot;, processes, then labels it applied: \u0026quot;true\u0026quot;.\nBenefits:\nFull audit history in git No state synchronization issues ArgoCD shows clear diff Easy rollback (just change which ConfigMap exists) Drawback:\nMore ConfigMaps to manage over time (need cleanup policy) Structured Logging and Status Tracking#\rWrite detailed status to a separate ConfigMap or as events:\nkubectl create event restore-operation \\ --type=Normal \\ --reason=RestoreStarted \\ --message=\u0026#34;Starting restore from 2024-12-10\u0026#34; # Or update status ConfigMap kubectl patch cm restore-status --type=merge -p \u0026#39;{ \u0026#34;data\u0026#34;: { \u0026#34;lastOperation\u0026#34;: \u0026#34;\u0026#39;\u0026#34;$(date -Iseconds)\u0026#34;\u0026#39;\u0026#34;, \u0026#34;lastStatus\u0026#34;: \u0026#34;InProgress\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Disabling ingress...\u0026#34; } }\u0026#39;Benefits:\nObservability without logs diving Can integrate with monitoring/alerting Clear operational history Make Scripts Truly Idempotent#\rEach step should check before acting:\ndisable_ingress() { local current=$(kubectl get ingress myapp -o jsonpath=\u0026#39;{.metadata.annotations.disabled}\u0026#39;) if [ \u0026#34;$current\u0026#34; = \u0026#34;true\u0026#34; ]; then echo \u0026#34;Ingress already disabled, skipping\u0026#34; return 0 fi kubectl annotate ingress myapp disabled=true kubectl delete ingress myapp } scale_to_zero() { local replicas=$(kubectl get deploy myapp -o jsonpath=\u0026#39;{.spec.replicas}\u0026#39;) if [ \u0026#34;$replicas\u0026#34; = \u0026#34;0\u0026#34; ]; then echo \u0026#34;Already scaled to zero\u0026#34; return 0 fi # Save original count kubectl annotate deploy myapp original-replicas=\u0026#34;$replicas\u0026#34; kubectl scale deploy myapp --replicas=0 }Benefits:\nSafe to run multiple times Can resume from any point No duplicate work Recommended Hybrid Approach#\rCombine several of these:\nDeployment (not CronJob) - faster reaction, prevents overlaps via replicas: 1 State machine in ConfigMap - track progress, handle failures Lease-based locking - extra safety if you need \u0026gt;1 replica for HA Idempotent steps - each operation checks before acting Status tracking - separate ConfigMap or events for observability This gives you 80% of the operator benefits with 20% of the complexity, works great without ArgoCD, and you can test everything locally with kind/minikube.\nThe main remaining limitation versus a real operator: no automatic watch/event-driven reaction (you\u0026rsquo;re still polling every 30-60s), but that\u0026rsquo;s acceptable for DR operations that aren\u0026rsquo;t time-critical down to the second.\nWould you like me to sketch out a concrete example implementation of this hybrid approach?\n"},{"id":50,"href":"/musings/docs/misc/runbooks/","title":"Runbooks","section":"Miscellaneous","content":"Below is a practical, ops-first guide written for an experienced Unix/PC server architect who is new to operating distributed systems at scale. It assumes you understand the theory‚Äîbut want concrete, repeatable practices that keep Tier 3 asleep and let Tier 2 resolve the majority of incidents safely.\n1. Mental Model Shift (Important Framing)#\rBefore runbooks, align expectations‚Äîthis shapes how you write them.\nTraditional Ops Mindset (what Tier 2 may expect)#\r‚ÄúFind the broken server‚Äù ‚ÄúRestart the service‚Äù ‚ÄúLook at logs on the box‚Äù ‚ÄúFix it so it stays fixed‚Äù Kubernetes / Microservices Reality#\rThere is no single server Restarting things is normal, expected, and safe Many failures are non-actionable symptoms (transient, self-healing) Permanent fixes usually require code or config changes (Tier 3) üëâ Runbooks must explicitly state what not to fix and what ‚Äúnormal‚Äù looks like.\n2. What a Runbook Is (and Is Not)#\rA runbook is a controlled, safe decision tree that:\nGuides Tier 2 from alert ‚Üí diagnosis ‚Üí safe remediation Limits blast radius Makes escalation predictable and justified A runbook is NOT:\nA design doc A tutorial on Kubernetes A dump of kubectl commands without context A place to ‚Äúdebug like a developer‚Äù 3. Anatomy of an Effective Runbook#\rEvery production runbook should follow the same structure, so Tier 2 never has to guess.\n3.1 Standard Runbook Structure (Mandatory)#\r1. Metadata (Top of File)#\rRunbook Name: Service(s): Severity: Last Reviewed: Owner (Tier 3): Applies To: (prod / staging / both)Purpose:\nOwnership clarity Trust that it‚Äôs current Scope awareness 2. Trigger Conditions (Why You Are Here)#\rDescribe exactly how someone ends up reading this runbook.\nGood\nThis runbook is used when:\nAlert: checkout-api High Error Rate Dashboard shows \u0026gt;5% 5xx for 3 minutes Bad\n‚ÄúWhen checkout is broken‚Äù\n3. User Impact (Plain Language)#\rTier 2 must understand business impact immediately.\nExample\nImpact: - Users may be unable to complete purchases - Existing sessions are not affected - No data loss expectedThis helps Tier 2:\nCommunicate with stakeholders Judge urgency Decide escalation timing 4. Known Normal Conditions (Critical)#\rThis prevents false escalations.\nExample\nNormal / Expected: - Short spikes in 5xx during deployments (‚â§2 minutes) - Pod restarts during node maintenance - One replica occasionally restarting under load 5. Diagnosis (Decision Tree, Not Investigation)#\rThis is the most important section.\nRules for Diagnosis Steps#\rBinary questions (yes/no) Observable via dashboards or simple commands No Kubernetes internals unless unavoidable No ‚Äúinterpret this log‚Äù unless explicitly shown Example Diagnosis Flow#\rStep 1: Is traffic reaching the service? ‚Üí Check Dashboard: \u0026#34;Ingress Requests\u0026#34; - YES ‚Üí Go to Step 2 - NO ‚Üí Go to Step 5 Step 2: Are error rates elevated? ‚Üí Check Dashboard: \u0026#34;5xx Rate\u0026#34; - YES ‚Üí Step 3 - NO ‚Üí Likely transient, monitor for 5 minutes Step 3: Is latency elevated? ‚Üí Check Dashboard: \u0026#34;P95 Latency\u0026#34; - YES ‚Üí Step 4 - NO ‚Üí Escalate (unexpected error pattern)üö´ Never say:\n‚ÄúInvestigate‚Äù ‚ÄúDebug‚Äù ‚ÄúLook into‚Äù 6. Resolution Procedures (Safe, Reversible Actions Only)#\rTier 2 actions must be:\nIdempotent Low risk Documented blast radius Structure for Each Action#\rAction Name: When to use: What this does: Steps: Verification: Rollback:Example: Restarting a Deployment#\rAction: Restart checkout-api deployment When to use: - Error rate \u0026gt;10% - At least 2 replicas healthy What this does: - Forces pods to restart one at a time - No user-visible downtime expected Steps: 1. kubectl rollout restart deployment checkout-api 2. Wait for rollout to complete Verification: - Error rate drops below 2% - All replicas ready Rollback: - None required (restart is reversible)üö´ Do not allow:\nScaling to zero Manual pod deletion without guardrails Config changes Database changes 7. Escalation Criteria (Very Explicit)#\rTier 2 must never guess when to escalate.\nGood\nEscalate to Tier 3 if: - Error rate \u0026gt;10% for 10 minutes after restart - Data integrity alerts fire - Symptoms do not match this runbookBad\n‚ÄúIf the issue persists‚Äù\n8. References \u0026amp; Links#\rDashboards Alerts Architecture diagrams Related runbooks 4. Writing for Non-Developers (Key Techniques)#\rUse Concrete Language#\r‚ÄúClick the dashboard named X‚Äù ‚ÄúLook for the red line crossing 5%‚Äù ‚ÄúRun this exact command‚Äù Avoid:#\rKubernetes theory Acronyms without explanation ‚ÄúShould‚Äù, ‚Äúmight‚Äù, ‚Äúprobably‚Äù Provide Expected Outcomes#\rAlways say:\n‚ÄúIf this works, you should see‚Ä¶‚Äù\n5. Identifying What Runbooks You Need#\rStart from failure patterns, not services.\n5.1 Core Runbook Categories#\rInfrastructure / Platform#\rPod crash loops Node not ready Image pull failures Service mesh sidecar failures Traffic \u0026amp; Scaling#\rHigh latency High error rates Autoscaling not triggering Traffic stuck on old version (blue/green) Dependency Failures#\rDatabase unavailable Timeout to external APIs Credential/secret expiration Deployment Issues#\rRollout stuck Readiness probes failing Version mismatch across services üëâ Rule of thumb: If an alert pages Tier 2 ‚Üí it must have a runbook.\n6. Linking Runbooks to Alerts \u0026amp; Dashboards#\rAlert Design Rule#\rNo alert without a runbook link.\nAlert payload should include:\nSummary Impact Link to runbook Link to dashboard Example\nALERT: checkout-api 5xx rate \u0026gt;5% Runbook: https://runbooks/checkout/high-error-rate Dashboard: https://dashboards/checkout/overview 7. Good vs Bad Runbook Examples#\r‚ùå Bad#\r‚ÄúCheck logs and restart pods if needed.‚Äù\nWhy it fails:\nNo criteria No safety No escalation guidance Encourages random actions ‚úÖ Good#\r‚ÄúIf error rate \u0026gt;5% and at least 2 pods are ready, restart deployment. If error rate persists after 10 minutes, escalate.‚Äù\n8. Testing Runbooks with Tier 2 (Mandatory Practice)#\r8.1 Runbook Fire Drills#\rUse staging Inject failure (kill pods, block DB, scale down nodes) Tier 2 follows runbook without help Observe confusion points 8.2 Post-Drill Review#\rAsk:\nWhere did they hesitate? What terms were unclear? What decisions felt unsafe? Update immediately.\n9. Maintaining Runbooks After Incidents#\rAfter Every Tier 3 Escalation:#\rAsk:\nCould Tier 2 have resolved this? Was a runbook missing? Was the escalation criteria unclear? Rule: If Tier 3 touches it twice ‚Üí Tier 2 needs a runbook update.\n10. Operations Handbook (The ‚ÄúFront Door‚Äù)#\rThis is not a runbook‚Äîit‚Äôs orientation.\nContents#\rSystem overview (C4-style diagram) Environments \u0026amp; promotion flow What Tier 2 can and cannot do How incidents are classified Where runbooks live Escalation paths Think of it as ‚ÄúHow to operate this platform‚Äù, not ‚ÄúHow it works.‚Äù\n11. Runbook Templates#\r11.1 Incident Response Runbook Template#\r# Runbook: \u0026lt;Title\u0026gt; ## Trigger ## Impact ## Normal Conditions ## Diagnosis ## Resolution ## Escalation ## References11.2 Deployment Issue Runbook#\r## Trigger ## Expected Deployment Behavior ## Common Failure Modes ## Safe Actions ## Escalation Criteria 12. Runbook Quality Checklist#\rA runbook is acceptable only if:\nTrigger is explicit Impact is described in business terms Diagnosis is step-by-step and observable All actions are safe and reversible Escalation criteria are explicit Linked from alerts Tested by Tier 2 Owned and reviewed regularly Final Advice#\rYour biggest architectural win will not be:\nAutoscaling Service mesh Blue/green deployments It will be turning distributed system chaos into predictable Tier 2 actions.\n"},{"id":51,"href":"/musings/docs/misc/layered-schema/","title":"Schema Validation with Layered Values Files","section":"Miscellaneous","content":"Schema Validation with Layered Values Files#\rGreat question! This is a real-world pattern that many organizations use, and it fundamentally changes how you approach schema validation.\nThe Core Challenge#\rYour setup is:\nvalues.yaml (base defaults) + platform.yaml (environment/platform overrides)\r+ custom-values.yaml (deployment-specific overrides)\r= final merged valuesThe problem: Helm validates the schema against the MERGED result, not individual files.\nThis means:\nIf database.password is required but intentionally omitted from values.yaml Your schema validation will PASS when you provide -f values.yaml -f platform.yaml But will FAIL when someone tests with just values.yaml alone This breaks local development, testing, and chart packaging How Helm Schema Validation Actually Works#\rWhen you run:\nhelm install app . -f values.yaml -f platform.yaml -f custom.yamlHelm does this:\nMerges all value files (left to right, later values override earlier) Validates the final merged result against values.schema.json Then renders templates Key insight: The schema sees ONE combined values object, not individual layers.\nImpact on What I Showed You#\rProblem 1: Required Fields Break Packaging#\rIf your schema has: [EXAMPLE-1: Schema marking database.password as required]\nThen this fails:\nhelm template . -f values.yaml # Error: database.password is requiredEven though you INTEND to provide it via platform.yaml later. This breaks:\nChart Museum publishing (validates with just values.yaml) helm lint (checks values.yaml in isolation) Developer local testing before they have platform.yaml Problem 2: Can\u0026rsquo;t Validate Individual Layers#\rYou can\u0026rsquo;t create separate schemas for each layer because:\nHelm only uses ONE schema file: values.schema.json There\u0026rsquo;s no platform.schema.json or layered validation You can\u0026rsquo;t tell Helm \u0026ldquo;these fields are required in the final merge but optional in base\u0026rdquo; Problem 3: Conditional Requirements Based on Source#\rWhat you want: \u0026ldquo;database.password is optional in values.yaml but required in the final deployment\u0026rdquo;\nWhat JSON Schema can express: \u0026ldquo;database.password is required (period)\u0026rdquo; OR \u0026ldquo;database.password is optional (period)\u0026rdquo;\nThere\u0026rsquo;s no concept of \u0026ldquo;required depending on which file it came from.\u0026rdquo;\nSolution Strategies#\rStrategy 1: Schema for Final State Only (Recommended)#\rApproach: Schema validates what production MUST have, not what values.yaml contains.\nYour values.yaml contains: [EXAMPLE-2: values.yaml with intentionally empty database section]\nYour platform.yaml contains: [EXAMPLE-3: platform.yaml with actual database credentials]\nYour values.schema.json marks database.password as required.\nImplications:\n‚ùå helm template . -f values.yaml fails validation ‚úÖ helm template . -f values.yaml -f platform.yaml passes ‚úÖ Production deployments are validated correctly ‚ùå Chart packaging/publishing requires workarounds Workaround for packaging: [EXAMPLE-4: CI script that provides minimal platform.yaml for validation]\nStrategy 2: Everything Optional in Schema, Validate in Templates#\rApproach: Make everything optional in schema, add validation logic in Helm templates.\nYour values.schema.json: [EXAMPLE-5: Schema with all database fields optional]\nYour template adds validation: [EXAMPLE-6: Deployment template with required field checks]\nImplications:\n‚úÖ Chart packages/lints successfully with just values.yaml ‚úÖ Validation happens at render time with merged values ‚úÖ Clear error messages about missing required fields ‚ùå Less \u0026ldquo;shift-left\u0026rdquo; - errors caught later in process ‚ùå More verbose templates Strategy 3: Hybrid Approach with Different Validation Contexts#\rApproach: Use schema for structure/types, use templates for requirements.\nSchema validates:\nData types (string, integer, boolean) Formats (email, hostname, port ranges) Enums (allowed values) Patterns (regex validation) Templates validate:\nRequired fields (since these depend on deployment context) Cross-field dependencies Environment-specific rules Implications:\n‚úÖ Best of both worlds ‚úÖ Schema catches type errors early ‚úÖ Templates catch missing required fields at deploy time ‚úÖ Works with layered values ‚ö†Ô∏è Split validation logic across two places Strategy 4: Multiple Schema Files with Manual Testing#\rApproach: Create different schema files for different purposes, manually select which to use.\nFiles:\nvalues.schema.json (minimal, for base values.yaml) schemas/production.schema.json (strict, for production) schemas/development.schema.json (relaxed, for dev) You manually test against appropriate schema: [EXAMPLE-7: Script that validates against different schemas]\nImplications:\n‚úÖ Flexible per-environment validation ‚ùå Helm only uses values.schema.json automatically ‚ùå Must remember to run manual validation ‚ùå More maintenance burden ‚ö†Ô∏è Easy to forget which schema to use when Testing with Layered Values#\rUnit Tests (helm-unittest)#\rYour test files can specify multiple value files: [EXAMPLE-8: unittest with layered values]\nKey point: Tests see the merged result, just like Helm does.\nIntegration Tests (chart-testing)#\rThe ct tool follows the same merge behavior: [EXAMPLE-9: ct test with multiple values files]\nManual Validation#\rYou need to test the full merge: [EXAMPLE-10: Manual validation commands]\nWhat You Can\u0026rsquo;t Do (JSON Schema Limitations)#\rCan\u0026rsquo;t mark fields as \u0026ldquo;required in layer N but optional in layer 1\u0026rdquo;\nJSON Schema has no concept of file provenance It only sees the final merged object Can\u0026rsquo;t validate \u0026ldquo;at least one of these files must provide field X\u0026rdquo;\nNo way to track which file provided which value Can only validate the final result Can\u0026rsquo;t have conditional requirements based on deployment environment\nSchema is static, doesn\u0026rsquo;t know if you\u0026rsquo;re deploying to dev/staging/prod Can use conditionals based on VALUES, but not based on CONTEXT Can\u0026rsquo;t enforce \u0026ldquo;this field must come from platform.yaml, not values.yaml\u0026rdquo;\nNo file-source tracking Can\u0026rsquo;t prevent users from putting secrets in wrong file Best Practices for Layered Values#\rPractice 1: Document Intent in Comments#\rIn your values.yaml: [EXAMPLE-11: values.yaml with documentation comments]\nPractice 2: Use Sentinel Values#\rInstead of omitting required fields, use sentinel values: [EXAMPLE-12: Using sentinel values like REQUIRED or OVERRIDE_ME]\nYour schema can then validate:\n[EXAMPLE-13: Schema pattern matching for sentinel values] this example is at the end of the document with some additional explanation\nThis gives early feedback: \u0026ldquo;You forgot to override this required field\u0026rdquo;\nPractice 3: Validation Hooks#\rUse Helm hooks to validate before deployment: [EXAMPLE-14: PreInstall hook that validates required fields]\nPractice 4: Separate Required vs. Optional in Schema Organization#\rStructure your schema to make the layering visible: [EXAMPLE-15: Schema with comments showing which fields expect platform.yaml]\nRecommended Approach for Your Use Case#\rBased on your layered setup, I recommend:\nFor values.schema.json:\nValidate types, formats, enums, patterns Make \u0026ldquo;will be provided by platform.yaml\u0026rdquo; fields optional Use pattern to catch sentinel values if you use them For templates:\nAdd explicit required field checks for production-critical values Provide helpful error messages referencing which layer should provide the value For CI/CD:\nLint/test with merged values (values.yaml + platform.yaml) Run separate validation for each environment\u0026rsquo;s platform file For documentation:\nClearly document which values.yaml fields are \u0026ldquo;placeholders\u0026rdquo; Document which platform.yaml fields are required Provide example platform.yaml for each environment Detection and Warnings#\rYou can add warnings for common mistakes:\nWarn if Required Field Uses Default#\rTemplate logic to detect if production is using a dev default: [EXAMPLE-16: Template warning when production uses development defaults]\nFail Fast on Missing Critical Fields#\rPreInstall hook that checks critical fields: [EXAMPLE-17: Hook that validates before any resources are created]\nThe Uncomfortable Truth#\rHelm\u0026rsquo;s schema validation wasn\u0026rsquo;t designed for layered values files. It assumes:\nOne canonical values.yaml All required fields present in that file Overrides are for customization, not completion Your pattern (intentionally incomplete base + required overrides) is valid and common, but fights against Helm\u0026rsquo;s assumptions.\nThe workaround is accepting that:\nSchema validates final merged state You need template-level validation for context-dependent requirements Your CI/CD must always test with full value layers Chart packaging requires either minimal platform.yaml or template validation only Questions to Clarify Your Needs#\rBefore I provide specific examples, tell me:\nWhat\u0026rsquo;s your primary concern?\nChart packaging/publishing failing validation? Developer experience when testing locally? Catching production deployment errors earlier? All of the above? What fields are intentionally missing from values.yaml?\nJust secrets (passwords, API keys)? Environment-specific config (URLs, database hosts)? Scaling parameters (replicas, resources)? How many layers do you have?\nJust values.yaml + platform.yaml? Or values.yaml + platform.yaml + environment.yaml + custom.yaml? Who controls which layer?\nPlatform team controls platform.yaml? Developers never see/edit platform.yaml? Each team has their own overrides? What\u0026rsquo;s your deployment pattern?\nArgoCD with values files in different repos? Helm directly with -f flags? Helmfile managing the layers? Let me know which examples you want to see numbered EXAMPLE-1 through EXAMPLE-17, and I\u0026rsquo;ll provide them based on your specific context.\nEXAMPLE-13: Schema Pattern Matching for Sentinel Values#\r{ \u0026#34;$schema\u0026#34;: \u0026#34;http://json-schema.org/draft-07/schema#\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;database\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;host\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;not\u0026#34;: { \u0026#34;pattern\u0026#34;: \u0026#34;^(REQUIRED|OVERRIDE_ME|CHANGEME|TODO)$\u0026#34; }, \u0026#34;description\u0026#34;: \u0026#34;Database hostname - must be overridden in platform.yaml\u0026#34; }, \u0026#34;password\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;not\u0026#34;: { \u0026#34;pattern\u0026#34;: \u0026#34;^(REQUIRED|OVERRIDE_ME|CHANGEME|TODO|changeme|password)$\u0026#34; }, \u0026#34;minLength\u0026#34;: 8, \u0026#34;description\u0026#34;: \u0026#34;Database password - must be overridden in platform.yaml\u0026#34; }, \u0026#34;port\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34;, \u0026#34;minimum\u0026#34;: 1, \u0026#34;maximum\u0026#34;: 65535, \u0026#34;not\u0026#34;: { \u0026#34;const\u0026#34;: 0 } } } }, \u0026#34;apiKey\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;not\u0026#34;: { \u0026#34;pattern\u0026#34;: \u0026#34;^(REQUIRED|OVERRIDE_ME|CHANGEME|TODO|your-api-key-here)$\u0026#34; }, \u0026#34;pattern\u0026#34;: \u0026#34;^[a-zA-Z0-9_-]{32,}$\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;API key for external service - must be real value in platform.yaml\u0026#34; }, \u0026#34;externalUrl\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;uri\u0026#34;, \u0026#34;not\u0026#34;: { \u0026#34;pattern\u0026#34;: \u0026#34;^(REQUIRED|OVERRIDE_ME|https://example\\\\.com)$\u0026#34; }, \u0026#34;description\u0026#34;: \u0026#34;External URL - must be overridden for environment\u0026#34; } } }How this works:\nThe not keyword - JSON Schema validates that the value does NOT match the pattern/const inside the not block\nPattern matching - Catches common sentinel values:\nREQUIRED OVERRIDE_ME CHANGEME TODO Common weak passwords like password or changeme Validation behavior:\n# ‚ùå This FAILS validation helm install app . \\ --set database.password=REQUIRED # Error: database.password must not match pattern ^(REQUIRED|OVERRIDE_ME|...)$ # ‚ùå This also FAILS helm install app . \\ --set database.password=changeme # Error: database.password must not match pattern # ‚úÖ This PASSES helm install app . \\ --set database.password=actualSecurePassword123 Additional validations combined:\ndatabase.password must NOT be a sentinel value AND must be at least 8 characters (minLength: 8) Both conditions must be true For URLs:\n\u0026#34;not\u0026#34;: { \u0026#34;pattern\u0026#34;: \u0026#34;^(REQUIRED|OVERRIDE_ME|https://example\\\\.com)$\u0026#34; }This prevents both sentinel values AND the common documentation placeholder https://example.com\nCorresponding values.yaml would look like:\ndatabase: host: OVERRIDE_ME # Will fail validation if not overridden password: REQUIRED # Will fail validation if not overridden port: 5432 # Valid default, can be used as-is apiKey: REQUIRED # Will fail validation if not overridden externalUrl: https://example.com # Will fail validation if not overriddenAnd platform.yaml provides real values:\ndatabase: host: prod-db.internal.company.com password: actualDatabasePassword # port inherits 5432 from values.yaml apiKey: ak_live_abc123def456ghi789jkl012mno345pqr externalUrl: https://api.production.company.comResult:\nChart packages/lints successfully (sentinel values are valid strings) Deployment with just values.yaml FAILS validation (sentinel values detected) Deployment with values.yaml + platform.yaml PASSES (real values provided) This gives you early feedback that required overrides are missing, without making the fields truly \u0026ldquo;required\u0026rdquo; in the schema (which would break packaging).\n"},{"id":52,"href":"/musings/docs/misc/helm-layered-abstraction/","title":"Solution 2: Kustomize with Helm","section":"Miscellaneous","content":"Great question - this is a common challenge with multi-layered architectures. Here are several approaches, from simplest to most sophisticated:\nSolution 1: Values Repository with ArgoCD valueFiles#\rThis is the cleanest approach for your use case.\nRepository Structure:\ngitops-repo/\r‚îú‚îÄ‚îÄ values/\r‚îÇ ‚îú‚îÄ‚îÄ common/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ global.yaml # Shared across all layers\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ networking.yaml # Network configs\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ observability.yaml # Logging, metrics\r‚îÇ ‚îú‚îÄ‚îÄ environments/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ dev.yaml\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ staging.yaml\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ prod.yaml\r‚îÇ ‚îî‚îÄ‚îÄ layers/\r‚îÇ ‚îú‚îÄ‚îÄ data-persistence.yaml\r‚îÇ ‚îú‚îÄ‚îÄ platform-services.yaml\r‚îÇ ‚îú‚îÄ‚îÄ application-services.yaml\r‚îÇ ‚îî‚îÄ‚îÄ ingress.yaml\r‚îú‚îÄ‚îÄ applications/\r‚îÇ ‚îú‚îÄ‚îÄ data-persistence-app.yaml\r‚îÇ ‚îú‚îÄ‚îÄ platform-services-app.yaml\r‚îÇ ‚îú‚îÄ‚îÄ application-services-app.yaml\r‚îÇ ‚îî‚îÄ‚îÄ ingress-app.yaml\r‚îî‚îÄ‚îÄ charts/ # Your actual Helm charts\r‚îú‚îÄ‚îÄ data-persistence/\r‚îú‚îÄ‚îÄ platform-services/\r‚îú‚îÄ‚îÄ application-services/\r‚îî‚îÄ‚îÄ ingress/ArgoCD Application Example:\n# applications/data-persistence-app.yaml apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: data-persistence namespace: argocd spec: project: default source: repoURL: https://github.com/yourorg/gitops-repo targetRevision: main path: charts/data-persistence helm: valueFiles: - ../../values/common/global.yaml - ../../values/common/networking.yaml - ../../values/environments/prod.yaml - ../../values/layers/data-persistence.yaml values: | # Inline overrides if needed specificOverride: value destination: server: https://kubernetes.default.svc namespace: data-persistence syncPolicy: automated: prune: true selfHeal: trueCommon Values Example:\n# values/common/global.yaml global: environment: prod region: us-east-1 domain: example.com labels: app.kubernetes.io/managed-by: argocd app.kubernetes.io/part-of: myapp imageRegistry: registry.example.com imagePullSecrets: - name: registry-creds monitoring: enabled: true serviceMonitor: true security: podSecurityStandard: restricted networkPolicies: true# values/common/networking.yaml networking: serviceMesh: enabled: true type: istio dns: clusterDomain: cluster.local loadBalancer: class: nginx annotations: service.beta.kubernetes.io/aws-load-balancer-type: nlb# values/layers/data-persistence.yaml postgresql: enabled: true replicaCount: 3 storage: size: 100Gi storageClass: fast-ssd redis: enabled: true cluster: enabled: true nodes: 6Pros:\nSimple and obvious - values are in one place ArgoCD natively supports multiple valueFiles Easy to see which values apply to which layer Version controlled with everything else No external dependencies Works perfectly with GitOps Cons:\nValues files can grow large Some duplication between layers if they need similar configs Need to maintain relative paths in Applications Solution 2: Kustomize with Helm#\rUse Kustomize as a wrapper to compose values, then render with Helm.\nRepository Structure:\ngitops-repo/\r‚îú‚îÄ‚îÄ base/\r‚îÇ ‚îú‚îÄ‚îÄ values/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ global.yaml\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ networking.yaml\r‚îÇ ‚îî‚îÄ‚îÄ charts/\r‚îÇ ‚îú‚îÄ‚îÄ data-persistence/\r‚îÇ ‚îú‚îÄ‚îÄ platform-services/\r‚îÇ ‚îú‚îÄ‚îÄ application-services/\r‚îÇ ‚îî‚îÄ‚îÄ ingress/\r‚îú‚îÄ‚îÄ overlays/\r‚îÇ ‚îú‚îÄ‚îÄ prod/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ kustomization.yaml\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ values/\r‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ prod-overrides.yaml\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ layers/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ data-persistence/\r‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ kustomization.yaml\r‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ values.yaml\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ platform-services/\r‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ kustomization.yaml\r‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ values.yaml\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ ...\r‚îÇ ‚îî‚îÄ‚îÄ staging/\r‚îÇ ‚îî‚îÄ‚îÄ ...\r‚îî‚îÄ‚îÄ applications/\r‚îî‚îÄ‚îÄ app-of-apps.yamlKustomization for a Layer:\n# overlays/prod/layers/data-persistence/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization helmCharts: - name: data-persistence repo: oci://registry.example.com/charts version: 1.2.3 releaseName: data-persistence namespace: data-persistence valuesFile: values.yaml valuesInline: global: environment: prod valuesMerge: merge # Merge all values files configMapGenerator: - name: shared-values files: - ../../../../base/values/global.yaml - ../../../../base/values/networking.yamlPros:\nPowerful composition with Kustomize overlays Can use patches for fine-grained control Standard Kubernetes tooling Good for complex multi-environment setups Cons:\nMore complex than pure Helm Kustomize + Helm interaction can be confusing Harder to understand for team members unfamiliar with Kustomize ArgoCD needs to be configured to use Kustomize with Helm Solution 3: Helm Umbrella Chart with Dependencies#\rCreate a parent \u0026ldquo;umbrella\u0026rdquo; chart that includes all layers as dependencies.\nRepository Structure:\ngitops-repo/\r‚îú‚îÄ‚îÄ umbrella-chart/\r‚îÇ ‚îú‚îÄ‚îÄ Chart.yaml\r‚îÇ ‚îú‚îÄ‚îÄ values.yaml # Global defaults\r‚îÇ ‚îú‚îÄ‚îÄ values-prod.yaml\r‚îÇ ‚îú‚îÄ‚îÄ values-staging.yaml\r‚îÇ ‚îî‚îÄ‚îÄ charts/ # Subcharts (can be local or remote)\r‚îÇ ‚îú‚îÄ‚îÄ data-persistence/\r‚îÇ ‚îú‚îÄ‚îÄ platform-services/\r‚îÇ ‚îú‚îÄ‚îÄ application-services/\r‚îÇ ‚îî‚îÄ‚îÄ ingress/\r‚îî‚îÄ‚îÄ applications/\r‚îú‚îÄ‚îÄ data-persistence-app.yaml # Can still deploy independently\r‚îú‚îÄ‚îÄ platform-services-app.yaml\r‚îî‚îÄ‚îÄ umbrella-app.yaml # Or deploy all togetherUmbrella Chart.yaml:\n# umbrella-chart/Chart.yaml apiVersion: v2 name: myapp-stack version: 1.0.0 dependencies: - name: data-persistence version: \u0026#34;1.x.x\u0026#34; repository: \u0026#34;file://./charts/data-persistence\u0026#34; condition: data-persistence.enabled - name: platform-services version: \u0026#34;1.x.x\u0026#34; repository: \u0026#34;file://./charts/platform-services\u0026#34; condition: platform-services.enabled - name: application-services version: \u0026#34;1.x.x\u0026#34; repository: \u0026#34;file://./charts/application-services\u0026#34; condition: application-services.enabled - name: ingress version: \u0026#34;1.x.x\u0026#34; repository: \u0026#34;file://./charts/ingress\u0026#34; condition: ingress.enabledUmbrella values.yaml:\n# umbrella-chart/values.yaml global: environment: prod domain: example.com imageRegistry: registry.example.com # Shared values accessible to all subcharts via .Values.global networking: serviceMesh: enabled: true # Per-layer config data-persistence: enabled: true postgresql: replicaCount: 3 platform-services: enabled: true kafka: replicaCount: 3 application-services: enabled: true replicaCount: 5 ingress: enabled: true className: nginxIndividual ArgoCD Application (for independent deployment):\napiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: data-persistence spec: source: repoURL: https://github.com/yourorg/gitops-repo path: umbrella-chart/charts/data-persistence helm: valueFiles: - ../../values.yaml - ../../values-prod.yaml parameters: - name: global.environment value: prodPros:\nStandard Helm pattern Global values automatically shared via .Values.global Can deploy all together or individually Clear parent-child relationship Cons:\nHarder to deploy layers truly independently Dependency management can be complex Version coordination between umbrella and subcharts Not as flexible as separate charts Solution 4: ConfigMap/Secret with External Values#\rStore common values in a ConfigMap, reference from Applications.\nRepository Structure:\ngitops-repo/\r‚îú‚îÄ‚îÄ shared-config/\r‚îÇ ‚îú‚îÄ‚îÄ common-values-configmap.yaml\r‚îÇ ‚îî‚îÄ‚îÄ prod-values-configmap.yaml\r‚îú‚îÄ‚îÄ charts/\r‚îÇ ‚îú‚îÄ‚îÄ data-persistence/\r‚îÇ ‚îú‚îÄ‚îÄ platform-services/\r‚îÇ ‚îú‚îÄ‚îÄ application-services/\r‚îÇ ‚îî‚îÄ‚îÄ ingress/\r‚îî‚îÄ‚îÄ applications/\r‚îú‚îÄ‚îÄ common-config-app.yaml # Deploys ConfigMaps first\r‚îú‚îÄ‚îÄ data-persistence-app.yaml\r‚îî‚îÄ‚îÄ ...ConfigMap with Common Values:\n# shared-config/common-values-configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: common-helm-values namespace: argocd data: global.yaml: | global: environment: prod domain: example.com imageRegistry: registry.example.com networking.yaml: | networking: serviceMesh: enabled: true loadBalancer: class: nginxArgoCD Application referencing ConfigMap:\napiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: data-persistence spec: source: helm: valuesObject: global: $common-helm-values:global.yaml networking: $common-helm-values:networking.yaml values: | # Layer-specific values postgresql: replicaCount: 3Pros:\nValues live in cluster as ConfigMaps Can update shared values without touching Application manifests Supports secrets for sensitive values Cons:\nNOT fully GitOps (values in cluster, not just git) Complex syntax in ArgoCD Harder to review changes Less obvious what values are being used Not recommended for most use cases My Recommendation: Solution 1 (Values Repository)#\rFor your use case (4 layers, GitOps, ArgoCD), Solution 1 is the cleanest because:\nTruly GitOps - Everything version controlled in git Simple mental model - Values files are just YAML files Native ArgoCD support - Multiple valueFiles work out of the box Easy to review - PRs show exactly what changed Independent deployment - Each layer has its own Application Flexible - Easy to override at any level Detailed Implementation of Solution 1#\rDirectory Structure:\nmyapp-gitops/\r‚îú‚îÄ‚îÄ README.md\r‚îú‚îÄ‚îÄ values/\r‚îÇ ‚îú‚îÄ‚îÄ common/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ README.md\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ global.yaml # Cluster-wide settings\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ networking.yaml # Network policies, service mesh\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ observability.yaml # Monitoring, logging, tracing\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ security.yaml # RBAC, pod security, secrets\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ storage.yaml # Storage classes, PV configs\r‚îÇ ‚îú‚îÄ‚îÄ environments/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ dev.yaml # Dev-specific overrides\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ staging.yaml\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ prod.yaml\r‚îÇ ‚îî‚îÄ‚îÄ layers/\r‚îÇ ‚îú‚îÄ‚îÄ data-persistence/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ base.yaml # Base config for this layer\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ dev.yaml # Dev overrides for this layer\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ staging.yaml\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ prod.yaml\r‚îÇ ‚îú‚îÄ‚îÄ platform-services/\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ ...\r‚îÇ ‚îú‚îÄ‚îÄ application-services/\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ ...\r‚îÇ ‚îî‚îÄ‚îÄ ingress/\r‚îÇ ‚îî‚îÄ‚îÄ ...\r‚îú‚îÄ‚îÄ charts/\r‚îÇ ‚îú‚îÄ‚îÄ data-persistence/ # Your Helm charts\r‚îÇ ‚îú‚îÄ‚îÄ platform-services/\r‚îÇ ‚îú‚îÄ‚îÄ application-services/\r‚îÇ ‚îî‚îÄ‚îÄ ingress/\r‚îî‚îÄ‚îÄ applications/\r‚îú‚îÄ‚îÄ envs/\r‚îÇ ‚îú‚îÄ‚îÄ dev/\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ data-persistence.yaml\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ platform-services.yaml\r‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ application-services.yaml\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ ingress.yaml\r‚îÇ ‚îú‚îÄ‚îÄ staging/\r‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ ...\r‚îÇ ‚îî‚îÄ‚îÄ prod/\r‚îÇ ‚îî‚îÄ‚îÄ ...\r‚îî‚îÄ‚îÄ app-of-apps/\r‚îú‚îÄ‚îÄ dev.yaml\r‚îú‚îÄ‚îÄ staging.yaml\r‚îî‚îÄ‚îÄ prod.yamlLoading Order (Prod Data Persistence Example):\n# applications/envs/prod/data-persistence.yaml apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: data-persistence-prod namespace: argocd finalizers: - resources-finalizer.argocd.argoproj.io spec: project: default source: repoURL: https://github.com/yourorg/myapp-gitops targetRevision: main path: charts/data-persistence helm: # Values are merged in order - later files override earlier valueFiles: # 1. Common values (most generic) - ../../values/common/global.yaml - ../../values/common/networking.yaml - ../../values/common/observability.yaml - ../../values/common/security.yaml - ../../values/common/storage.yaml # 2. Environment-specific (overrides common) - ../../values/environments/prod.yaml # 3. Layer base config (overrides environment) - ../../values/layers/data-persistence/base.yaml # 4. Layer + environment specific (most specific, final override) - ../../values/layers/data-persistence/prod.yaml # 5. Inline values (highest priority) values: | # Emergency overrides or computed values deploymentTimestamp: \u0026#34;2024-12-11T10:00:00Z\u0026#34; destination: server: https://kubernetes.default.svc namespace: data-persistence syncPolicy: automated: prune: true selfHeal: true syncOptions: - CreateNamespace=true # Ensure this layer deploys after dependencies # info: # - name: \u0026#39;Depends on\u0026#39; # value: \u0026#39;none (this is the base layer)\u0026#39;Example Values Files:\n# values/common/global.yaml global: # These are accessible in charts as .Values.global.* environment: \u0026#34;\u0026#34; # Will be overridden by environment file cluster: name: production-us-east-1 region: us-east-1 provider: aws domain: example.com imageRegistry: registry.example.com imagePullPolicy: IfNotPresent imagePullSecrets: - name: registry-credentials labels: app.kubernetes.io/managed-by: argocd app.kubernetes.io/part-of: myapp compliance: sox cost-center: engineering annotations: app.kubernetes.io/documentation: \u0026#34;https://docs.example.com\u0026#34; monitoring: enabled: true prometheus: serviceMonitor: true interval: 30s grafana: dashboards: true backup: enabled: true schedule: \u0026#34;0 2 * * *\u0026#34; retention: 30d# values/environments/prod.yaml global: environment: production resources: # Default resource requests/limits for prod defaults: requests: cpu: 100m memory: 128Mi limits: cpu: 1000m memory: 1Gi replicaCount: # Default replica counts for prod minimum: 3 autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 persistence: storageClass: gp3-encrypted backup: enabled: true security: podSecurityPolicy: restricted networkPolicies: enabled: true# values/layers/data-persistence/base.yaml # Base configuration for data persistence layer across all environments postgresql: enabled: true architecture: replication auth: existingSecret: postgres-credentials primary: persistence: size: 50Gi readReplicas: replicaCount: 1 metrics: enabled: true serviceMonitor: enabled: true redis: enabled: true architecture: replication auth: existingSecret: redis-credentials master: persistence: size: 10Gi replica: replicaCount: 1 metrics: enabled: true serviceMonitor: enabled: true# values/layers/data-persistence/prod.yaml # Production-specific overrides for data persistence postgresql: primary: resources: requests: cpu: 2 memory: 4Gi limits: cpu: 4 memory: 8Gi persistence: size: 500Gi # Much larger in prod readReplicas: replicaCount: 2 # More replicas in prod resources: requests: cpu: 1 memory: 2Gi limits: cpu: 2 memory: 4Gi backup: enabled: true schedule: \u0026#34;0 */6 * * *\u0026#34; # Every 6 hours s3: bucket: myapp-prod-db-backups region: us-east-1 redis: master: resources: requests: cpu: 500m memory: 1Gi limits: cpu: 2 memory: 4Gi persistence: size: 50Gi replica: replicaCount: 2 resources: requests: cpu: 250m memory: 512Mi limits: cpu: 1 memory: 2GiApp-of-Apps Pattern:\n# applications/app-of-apps/prod.yaml apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: myapp-prod namespace: argocd spec: project: default source: repoURL: https://github.com/yourorg/myapp-gitops targetRevision: main path: applications/envs/prod destination: server: https://kubernetes.default.svc namespace: argocd syncPolicy: automated: prune: true selfHeal: trueBenefits of This Structure:#\rClear precedence: common ‚Üí environment ‚Üí layer base ‚Üí layer+environment ‚Üí inline DRY principle: Shared values in one place Easy overrides: Each level can override previous Environment isolation: Easy to see what\u0026rsquo;s different in prod vs dev Layer isolation: Each layer\u0026rsquo;s config is self-contained Reviewable: Git diffs clearly show what changed at which level Testable: Can test individual layers in isolation Tips for Managing This:#\rDocument the loading order in README files Use YAML anchors in common files for reusable snippets Validate with helm template before committing: helm template charts/data-persistence \\ -f values/common/global.yaml \\ -f values/environments/prod.yaml \\ -f values/layers/data-persistence/base.yaml \\ -f values/layers/data-persistence/prod.yaml Use CI to validate all combinations render correctly Keep values files focused - don\u0026rsquo;t put everything in global.yaml Would you like me to elaborate on any specific aspect of this structure?\n"},{"id":53,"href":"/musings/docs/misc/stage2/","title":"Stage 2: Architecture Design","section":"Miscellaneous","content":"Stage 2: Architecture Design#\rReference Network Architecture#\rManagement Cluster VPC Layout#\rVPC CIDR: 10.0.0.0/16 (65,536 IPs)\nSubnet Allocation:\nPublic Subnet: 10.0.0.0/22 (1,024 IPs) - Internet Gateway, NAT Gateway Management Cluster Private Subnet: 10.0.4.0/22 (1,024 IPs) - EKS control plane, worker nodes Workload Cluster 1 Private Subnet: 10.0.8.0/22 (1,024 IPs) Workload Cluster 2 Private Subnet: 10.0.12.0/22 (1,024 IPs) Workload Cluster N\u0026hellip; continues in /22 blocks Key Components:\nSingle Internet Gateway for public subnet Single NAT Gateway in public subnet (for private subnet internet access) Route tables: public (IGW route), private (NAT route) Single AZ deployment (us-east-1a as default) Template Parameters:\nvpc_cidr (default: 10.0.0.0/16) cluster_name (for resource naming) aws_region (default: us-east-1) availability_zone (default: us-east-1a) node_instance_type (default: t3.medium) node_count (default: 2) Management Cluster Resource Stack#\rInfrastructure Resources (All Approaches Need These)#\rVPC - Virtual network Internet Gateway - Public internet access Public Subnet - /22 for NAT and public resources Private Subnet - /22 for management cluster NAT Gateway - Private subnet internet egress Route Tables (2) - Public and private routing Route Table Associations - Link subnets to route tables Security Group - EKS cluster communication IAM Role (Cluster) - EKS control plane permissions IAM Role (NodeGroup) - Worker node permissions IAM Role Policy Attachments - Required AWS managed policies EKS Cluster - Kubernetes control plane EKS NodeGroup - Managed worker nodes Crossplane-Specific Resources#\rProviderConfig - AWS credentials configuration Namespace - Organization (e.g., crossplane-system, platform) Total Base Resources: ~15 distinct AWS resources\nWorkload Cluster Template Structure#\rEach workload cluster follows a similar pattern but in its own /22 subnet:\nRequired Template Parameters:\nworkload_cluster_name - Unique cluster identifier workload_subnet_cidr - The /22 block to use (e.g., 10.0.8.0/22) node_instance_type - Instance type (default: t3.medium) node_count - Number of nodes (default: 2) Resources Per Workload Cluster:\nPrivate Subnet (in management VPC) Route Table Association (to existing private route table) Security Group IAM Role (Cluster) IAM Role (NodeGroup) IAM Role Policy Attachments EKS Cluster EKS NodeGroup Total Per Workload: ~8 resources (reuses VPC, IGW, NAT from management)\nApproach-Specific Architectural Differences#\rDirect MRs:\nAll 15 resources defined as individual manifests Manual dependency management via resource references Explicit configuration of every property XRDs:\nSingle EKSCluster custom resource Composition maps to underlying 15 resources Parameters expose only: cluster name, node count, instance type Opinionated:\nPre-built composition with hardcoded best practices Minimal parameters (often just cluster name) May include additional resources (monitoring, logging) Multi-Cloud:\nCloud-agnostic KubernetesCluster resource AWS-specific composition underneath Same interface could work with GCP/Azure compositions Fargate:\nReplaces NodeGroup resources with Fargate Profile Removes IAM NodeGroup role Adds Fargate Pod Execution Role vCluster:\nManagement cluster is standard EKS Workload \u0026ldquo;clusters\u0026rdquo; become vCluster CRDs, not EKS clusters Dramatically simplified workload template (no VPC/IAM resources) K8s-Native:\nStandard EKS infrastructure Adds: Harbor deployment, Vault deployment, cert-manager, NGINX Ingress controller Removes dependencies on ECR, Secrets Manager, ACM, ALB ECS Control Plane:\nECS Cluster instead of EKS for management plane ECS Task Definition for Crossplane controller ECS Service to run Crossplane Still provisions EKS for workload clusters Resource Dependency Diagrams#\rApproach 1: Direct Managed Resources#\rgraph TD\rsubgraph \u0026#34;Management Cluster - Direct MRs\u0026#34;\rVPC[VPC 10.0.0.0/16]\rIGW[Internet Gateway]\rPubSub[Public Subnet 10.0.0.0/22]\rPrivSub[Private Subnet 10.0.4.0/22]\rNAT[NAT Gateway]\rPubRT[Public Route Table]\rPrivRT[Private Route Table]\rSG[Security Group]\rClusterRole[IAM Role - Cluster]\rClusterPolicies[Policy Attachments x3]\rNodeRole[IAM Role - NodeGroup]\rNodePolicies[Policy Attachments x3]\rEKS[EKS Cluster]\rNodeGroup[Managed NodeGroup]\rVPC --\u0026gt; IGW\rVPC --\u0026gt; PubSub\rVPC --\u0026gt; PrivSub\rIGW --\u0026gt; PubSub\rPubSub --\u0026gt; NAT\rPubSub --\u0026gt; PubRT\rPrivSub --\u0026gt; PrivRT\rPrivRT --\u0026gt; NAT\rPrivSub --\u0026gt; SG\rSG --\u0026gt; ClusterRole\rClusterRole --\u0026gt; ClusterPolicies\rClusterRole --\u0026gt; EKS\rEKS --\u0026gt; NodeRole\rNodeRole --\u0026gt; NodePolicies\rNodeRole --\u0026gt; NodeGroup\rEKS --\u0026gt; NodeGroup\rend\rsubgraph \u0026#34;Crossplane Layer\u0026#34;\rPC[ProviderConfig]\rMR1[VPC Manifest]\rMR2[IGW Manifest]\rMR3[Subnet Manifests]\rMR4[Route Table Manifests]\rMR5[Security Group Manifest]\rMR6[IAM Role Manifests]\rMR7[EKS Cluster Manifest]\rMR8[NodeGroup Manifest]\rPC -.-\u0026gt; MR1\rPC -.-\u0026gt; MR2\rPC -.-\u0026gt; MR3\rPC -.-\u0026gt; MR4\rPC -.-\u0026gt; MR5\rPC -.-\u0026gt; MR6\rPC -.-\u0026gt; MR7\rPC -.-\u0026gt; MR8\rend\rstyle VPC fill:#e1f5ff\rstyle EKS fill:#ff9999\rstyle PC fill:#ffffccKey Characteristics:\n15+ individual manifest files Each manifest represents one AWS resource Dependencies managed through resource references (e.g., subnetIdRef) Complete visibility into every resource property Approach 2: Composite Resources (XRDs)#\rgraph TD\rsubgraph \u0026#34;User Interface\u0026#34;\rXR[EKSCluster Custom Resource]\rParams[Parameters: name, nodeCount, instanceType]\rParams --\u0026gt; XR\rend\rsubgraph \u0026#34;Platform Layer\u0026#34;\rXRD[EKSCluster XRD Definition]\rComp[Composition]\rXR --\u0026gt; XRD\rXRD --\u0026gt; Comp\rend\rsubgraph \u0026#34;Generated Managed Resources\u0026#34;\rVPC[VPC]\rIGW[Internet Gateway]\rSubnets[Subnets]\rNAT[NAT Gateway]\rRT[Route Tables]\rSG[Security Groups]\rIAM[IAM Roles + Policies]\rEKS[EKS Cluster]\rNG[NodeGroup]\rComp --\u0026gt; VPC\rComp --\u0026gt; IGW\rComp --\u0026gt; Subnets\rComp --\u0026gt; NAT\rComp --\u0026gt; RT\rComp --\u0026gt; SG\rComp --\u0026gt; IAM\rComp --\u0026gt; EKS\rComp --\u0026gt; NG\rend\rsubgraph \u0026#34;AWS Infrastructure\u0026#34;\rAWSVPC[AWS VPC]\rAWSEKS[AWS EKS]\rVPC -.-\u0026gt; AWSVPC\rEKS -.-\u0026gt; AWSEKS\rend\rstyle XR fill:#90EE90\rstyle Comp fill:#FFB6C1\rstyle EKS fill:#ff9999Key Characteristics:\nSingle custom resource exposes simplified API Composition template generates all 15 resources Platform team owns the Composition Users only see abstracted interface Approach 3a: Opinionated Platform#\rgraph TD\rsubgraph \u0026#34;User Interface\u0026#34;\rXR[PlatformCluster Resource]\rMinParams[Minimal Parameters: name only]\rMinParams --\u0026gt; XR\rend\rsubgraph \u0026#34;Opinionated Platform Layer\u0026#34;\rPreBuilt[Pre-built Composition]\rDefaults[Hardcoded Best Practices]\rSecurity[Security Policies]\rMonitoring[Observability Stack]\rXR --\u0026gt; PreBuilt\rPreBuilt --\u0026gt; Defaults\rPreBuilt --\u0026gt; Security\rPreBuilt --\u0026gt; Monitoring\rend\rsubgraph \u0026#34;Generated Resources\u0026#34;\rInfra[All AWS Infrastructure]\rCloudWatch[CloudWatch Logs]\rMetricsServer[Metrics Server]\rPSP[Pod Security Policies]\rPreBuilt --\u0026gt; Infra\rPreBuilt --\u0026gt; CloudWatch\rPreBuilt --\u0026gt; MetricsServer\rPreBuilt --\u0026gt; PSP\rend\rstyle XR fill:#90EE90\rstyle PreBuilt fill:#DDA0DD\rstyle Defaults fill:#F0E68CKey Characteristics:\nMinimal user input required Opinionated defaults baked in Additional resources beyond basic infrastructure Limited flexibility, maximum convenience Approach 3b: Multi-Cloud Abstraction#\rgraph TD\rsubgraph \u0026#34;Cloud-Agnostic Layer\u0026#34;\rGeneric[KubernetesCluster Resource]\rCloudParam[Parameter: cloudProvider=aws]\rCloudParam --\u0026gt; Generic\rend\rsubgraph \u0026#34;Provider Router\u0026#34;\rRouter[Cloud Provider Router]\rGeneric --\u0026gt; Router\rend\rsubgraph \u0026#34;AWS Implementation\u0026#34;\rAWSComp[AWS-Specific Composition]\rRouter --\u0026gt;|provider=aws| AWSComp\rVPC[VPC]\rEKS[EKS]\rOthers[Other AWS Resources]\rAWSComp --\u0026gt; VPC\rAWSComp --\u0026gt; EKS\rAWSComp --\u0026gt; Others\rend\rsubgraph \u0026#34;GCP Implementation (Not Used)\u0026#34;\rGCPComp[GCP-Specific Composition]\rRouter -.-\u0026gt;|provider=gcp| GCPComp\rGKE[GKE]\rGCPComp -.-\u0026gt; GKE\rend\rsubgraph \u0026#34;Azure Implementation (Not Used)\u0026#34;\rAzureComp[Azure-Specific Composition]\rRouter -.-\u0026gt;|provider=azure| AzureComp\rAKS[AKS]\rAzureComp -.-\u0026gt; AKS\rend\rstyle Generic fill:#98FB98\rstyle Router fill:#FFD700\rstyle EKS fill:#ff9999Key Characteristics:\nCloud-agnostic API definition Multiple provider-specific implementations Same interface across clouds Lowest common denominator constraints Approach 3c: Fargate-First Architecture#\rgraph TD\rsubgraph \u0026#34;Management Cluster - Fargate\u0026#34;\rVPC[VPC 10.0.0.0/16]\rIGW[Internet Gateway]\rPubSub[Public Subnet]\rPrivSub[Private Subnet]\rNAT[NAT Gateway]\rRT[Route Tables]\rSG[Security Groups]\rClusterRole[IAM Role - Cluster]\rFargateRole[IAM Role - Fargate Pod Execution]\rEKS[EKS Cluster]\rFargateProfile[Fargate Profile]\rVPC --\u0026gt; IGW\rVPC --\u0026gt; PubSub\rVPC --\u0026gt; PrivSub\rPubSub --\u0026gt; NAT\rPrivSub --\u0026gt; RT\rRT --\u0026gt; NAT\rPrivSub --\u0026gt; SG\rSG --\u0026gt; ClusterRole\rClusterRole --\u0026gt; EKS\rEKS --\u0026gt; FargateRole\rFargateRole --\u0026gt; FargateProfile\rEKS --\u0026gt; FargateProfile\rend\rsubgraph \u0026#34;Key Differences from Standard\u0026#34;\rNoNodes[‚ùå No NodeGroup]\rNoNodeRole[‚ùå No NodeGroup IAM Role]\rAddFargate[‚úÖ Fargate Profile]\rAddFargateRole[‚úÖ Fargate Execution Role]\rend\rstyle FargateProfile fill:#9370DB\rstyle NoNodes fill:#FFB6C1\rstyle AddFargate fill:#90EE90Key Characteristics:\nNo EC2 node management Fargate Profile replaces NodeGroup Different IAM role (Pod Execution vs Node) Serverless compute model Approach 3d: vCluster-Based Multi-Tenancy#\rgraph TD\rsubgraph \u0026#34;Management Cluster (Standard EKS)\u0026#34;\rVPC[VPC]\rEKS[EKS Cluster]\rNodeGroup[NodeGroup]\rVPC --\u0026gt; EKS\rEKS --\u0026gt; NodeGroup\rend\rsubgraph \u0026#34;vCluster Operator\u0026#34;\rVCOperator[vCluster Controller]\rNodeGroup --\u0026gt; VCOperator\rend\rsubgraph \u0026#34;Virtual Clusters (Not Real EKS)\u0026#34;\rVC1[vCluster 1]\rVC2[vCluster 2]\rVC3[vCluster 3]\rVCN[vCluster N...]\rVCOperator --\u0026gt; VC1\rVCOperator --\u0026gt; VC2\rVCOperator --\u0026gt; VC3\rVCOperator --\u0026gt; VCN\rend\rsubgraph \u0026#34;Shared Infrastructure\u0026#34;\rSharedNodes[Shared Worker Nodes]\rSharedNetwork[Shared Networking]\rSharedStorage[Shared Storage]\rNodeGroup -.-\u0026gt; SharedNodes\rVPC -.-\u0026gt; SharedNetwork\rSharedNodes -.-\u0026gt; SharedStorage\rVC1 -.-\u0026gt; SharedNodes\rVC2 -.-\u0026gt; SharedNodes\rVC3 -.-\u0026gt; SharedNodes\rend\rstyle EKS fill:#ff9999\rstyle VC1 fill:#87CEEB\rstyle VC2 fill:#87CEEB\rstyle VC3 fill:#87CEEB\rstyle VCN fill:#87CEEBKey Characteristics:\nOne real EKS cluster (management) Multiple virtual clusters running as pods Workload \u0026ldquo;clusters\u0026rdquo; are not separate EKS clusters Massive cost savings, shared infrastructure Workload Cluster Comparison:\ngraph LR\rsubgraph \u0026#34;Standard Approach\u0026#34;\rS1[Workload needs 8 AWS resources]\rS2[Full EKS cluster]\rS3[Dedicated infrastructure]\rS1 --\u0026gt; S2 --\u0026gt; S3\rend\rsubgraph \u0026#34;vCluster Approach\u0026#34;\rV1[Workload needs 1 resource]\rV2[vCluster CRD]\rV3[Runs as pods in host]\rV1 --\u0026gt; V2 --\u0026gt; V3\rend\rstyle S2 fill:#FFB6C1\rstyle V2 fill:#90EE90 Approach 3e: Kubernetes-Native Tooling#\rgraph TD\rsubgraph \u0026#34;Standard EKS Infrastructure\u0026#34;\rVPC[VPC]\rEKS[EKS Cluster]\rNodeGroup[NodeGroup]\rVPC --\u0026gt; EKS\rEKS --\u0026gt; NodeGroup\rend\rsubgraph \u0026#34;K8s-Native Services (Instead of AWS)\u0026#34;\rVault[Vault - Secrets]\rHarbor[Harbor - Container Registry]\rCertManager[cert-manager - Certificates]\rNginx[NGINX Ingress - Load Balancing]\rExternalDNS[external-dns - DNS]\rNodeGroup --\u0026gt; Vault\rNodeGroup --\u0026gt; Harbor\rNodeGroup --\u0026gt; CertManager\rNodeGroup --\u0026gt; Nginx\rNodeGroup --\u0026gt; ExternalDNS\rend\rsubgraph \u0026#34;Minimal AWS Services\u0026#34;\rRoute53[Route53 - DNS only]\rS3[S3 - Harbor backend]\rExternalDNS -.-\u0026gt; Route53\rHarbor -.-\u0026gt; S3\rend\rsubgraph \u0026#34;Replaced AWS Services\u0026#34;\rNoASM[‚ùå Secrets Manager]\rNoECR[‚ùå ECR]\rNoACM[‚ùå Certificate Manager]\rNoALB[‚ùå ALB Controller]\rend\rstyle Vault fill:#90EE90\rstyle Harbor fill:#90EE90\rstyle CertManager fill:#90EE90\rstyle Nginx fill:#90EE90\rstyle NoASM fill:#FFB6C1\rstyle NoECR fill:#FFB6C1Key Characteristics:\nStandard EKS infrastructure Cloud-agnostic tooling for services Additional operational complexity Higher resource requirements Better cloud portability Approach 3f: ECS Control Plane Alternative#\rgraph TD\rsubgraph \u0026#34;Management Control Plane (ECS)\u0026#34;\rVPC[VPC]\rECSCluster[ECS Cluster]\rTaskDef[Task Definition - Crossplane]\rECSService[ECS Service]\rFargateTask[Fargate Task - Running Crossplane]\rVPC --\u0026gt; ECSCluster\rECSCluster --\u0026gt; TaskDef\rTaskDef --\u0026gt; ECSService\rECSService --\u0026gt; FargateTask\rend\rsubgraph \u0026#34;Crossplane in ECS\u0026#34;\rXPController[Crossplane Controller]\rProviders[AWS Providers]\rFargateTask --\u0026gt; XPController\rXPController --\u0026gt; Providers\rend\rsubgraph \u0026#34;Provisioned Workload Clusters (EKS)\u0026#34;\rWL1[EKS Workload Cluster 1]\rWL2[EKS Workload Cluster 2]\rProviders --\u0026gt; WL1\rProviders --\u0026gt; WL2\rend\rsubgraph \u0026#34;Hybrid Model\u0026#34;\rManagementECS[Management = ECS]\rWorkloadEKS[Workloads = EKS]\rend\rstyle ECSCluster fill:#FFA500\rstyle WL1 fill:#ff9999\rstyle WL2 fill:#ff9999\rstyle ManagementECS fill:#FFD700\rstyle WorkloadEKS fill:#87CEEBKey Characteristics:\nManagement plane on ECS (not Kubernetes) Crossplane runs as ECS task Provisions standard EKS workload clusters Unusual architecture, limited community patterns Diagram Comparison Summary#\rComplexity Gradient:\nSimplest Structure: vCluster (fewest resources per workload) Most Transparent: Direct MRs (every resource visible) Most Abstracted: Opinionated (user sees almost nothing) Most Hybrid: ECS Control Plane (mixing orchestrators) Resource Count Comparison:\nApproach Mgmt Cluster Resources Per Workload Resources Direct MRs 15 individual MRs 8 MRs XRDs 1 XR + 1 Composition 1 XR Opinionated 1 XR 1 XR Multi-Cloud 1 Generic XR 1 Generic XR Fargate 14 MRs (no NodeGroup) 7 MRs vCluster 15 MRs + vCluster Op 1 vCluster CRD K8s-Native 15 MRs + 5 apps 8 MRs ECS Control ECS Task + resources 8 EKS MRs Stage 2 Complete#\rThis architecture design provides the foundation for understanding how each approach structures and organizes resources. The diagrams illustrate the fundamental differences in complexity, abstraction levels, and architectural patterns across all approaches.\n"},{"id":54,"href":"/musings/docs/misc/starting-helm-testing/","title":"Testing Complex Helm Charts: A Practical Guide","section":"Miscellaneous","content":"Testing Complex Helm Charts: A Practical Guide#\rYou\u0026rsquo;re asking the right question. Most Helm charts in production have zero tests, and teams discover bugs when they blow up in staging (or worse, production). Let me walk you through this systematically.\nUnderstanding What You\u0026rsquo;re Testing#\rA Helm chart has three distinct failure modes:\nTemplate rendering failures - YAML doesn\u0026rsquo;t compile Kubernetes validation failures - Valid YAML, invalid K8s resources Runtime failures - Deploys successfully but doesn\u0026rsquo;t work Each needs different testing approaches.\nStep 0: Audit Your Current Chart#\rBefore writing tests, understand what you have:\n# 1. How complex is it? find charts/my-app/templates -name \u0026#34;*.yaml\u0026#34; | wc -l # 2. How many values are there? yq eval \u0026#39;..\u0026#39; charts/my-app/values.yaml | grep -c \u0026#39;^\u0026#39; # 3. What conditional logic exists? grep -r \u0026#34;{{- if\u0026#34; charts/my-app/templates/ | wc -l # 4. How many helpers? grep -c \u0026#34;define\u0026#34; charts/my-app/templates/_helpers.tplExample output:\n23 template files\r147 configurable values\r38 conditional blocks\r12 helper functionsTranslation: This chart is complex. You need tests.\nPhase 1: Unit Tests (Testing Template Rendering)#\rTool: helm unittest (Start Here)#\rInstall:\nhelm plugin install https://github.com/helm-unittest/helm-unittestYour First Test (5 minutes)#\r# Create test directory mkdir -p charts/my-app/tests # Create your first test file cat \u0026gt; charts/my-app/tests/deployment_test.yaml \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; suite: test deployment templates: - deployment.yaml tests: - it: should create a deployment asserts: - isKind: of: Deployment - equal: path: metadata.name value: RELEASE-NAME-my-app EOFRun it:\nhelm unittest charts/my-app # Output: ### Chart [ my-app ] charts/my-app PASS test deployment\tcharts/my-app/tests/deployment_test.yaml Charts: 1 passed, 1 total Test Suites: 1 passed, 1 total Tests: 1 passed, 1 totalYou just wrote your first test in 5 minutes. üéâ\nReal-World Example: Testing Conditional Logic#\rYour chart probably has something like this:\n# templates/deployment.yaml {{- if .Values.autoscaling.enabled }} apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: {{ include \u0026#34;my-app.fullname\u0026#34; . }} spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: {{ include \u0026#34;my-app.fullname\u0026#34; . }} minReplicas: {{ .Values.autoscaling.minReplicas }} maxReplicas: {{ .Values.autoscaling.maxReplicas }} {{- end }}The test:\n# tests/hpa_test.yaml suite: test horizontal pod autoscaler templates: - hpa.yaml tests: - it: should not create HPA when autoscaling is disabled set: autoscaling.enabled: false asserts: - hasDocuments: count: 0 - it: should create HPA when autoscaling is enabled set: autoscaling.enabled: true autoscaling.minReplicas: 2 autoscaling.maxReplicas: 10 asserts: - isKind: of: HorizontalPodAutoscaler - equal: path: spec.minReplicas value: 2 - equal: path: spec.maxReplicas value: 10 - it: should target the correct deployment set: autoscaling.enabled: true asserts: - equal: path: spec.scaleTargetRef.kind value: Deployment - equal: path: spec.scaleTargetRef.name value: RELEASE-NAME-my-app Testing Complex Value Combinations#\rCommon bug: Chart works with default values, breaks with custom values.\n# tests/deployment_resources_test.yaml suite: test resource limits templates: - deployment.yaml tests: - it: should use default resources when not specified asserts: - equal: path: spec.template.spec.containers[0].resources.requests.cpu value: 100m - equal: path: spec.template.spec.containers[0].resources.requests.memory value: 128Mi - it: should override resources when specified set: resources.requests.cpu: 500m resources.requests.memory: 512Mi resources.limits.cpu: 1000m resources.limits.memory: 1Gi asserts: - equal: path: spec.template.spec.containers[0].resources.requests.cpu value: 500m - equal: path: spec.template.spec.containers[0].resources.limits.memory value: 1Gi - it: should allow disabling resource limits set: resources: {} asserts: - isNull: path: spec.template.spec.containers[0].resources Testing Helper Functions#\rYour _helpers.tpl probably has something like:\n{{/* Create chart name and version as used by the chart label. */}} {{- define \u0026#34;my-app.chart\u0026#34; -}} {{- printf \u0026#34;%s-%s\u0026#34; .Chart.Name .Chart.Version | replace \u0026#34;+\u0026#34; \u0026#34;_\u0026#34; | trunc 63 | trimSuffix \u0026#34;-\u0026#34; }} {{- end }}Test it:\n# tests/helpers_test.yaml suite: test helper functions templates: - deployment.yaml # Any template that uses the helper tests: - it: should generate correct chart label chart: version: 1.2.3+build.456 # Chart version with + asserts: - equal: path: metadata.labels[\u0026#34;helm.sh/chart\u0026#34;] value: my-app-1.2.3_build.456 # + replaced with _ Testing Schema Validation (Values Validation)#\rCreate a values schema:\n# values.schema.json { \u0026#34;$schema\u0026#34;: \u0026#34;http://json-schema.org/draft-07/schema#\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;replicaCount\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34;, \u0026#34;minimum\u0026#34;: 1, \u0026#34;maximum\u0026#34;: 100 }, \u0026#34;image\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;required\u0026#34;: [\u0026#34;repository\u0026#34;, \u0026#34;tag\u0026#34;], \u0026#34;properties\u0026#34;: { \u0026#34;repository\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;tag\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } } }, \u0026#34;autoscaling\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;enabled\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;boolean\u0026#34; }, \u0026#34;minReplicas\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34;, \u0026#34;minimum\u0026#34;: 1 }, \u0026#34;maxReplicas\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34;, \u0026#34;minimum\u0026#34;: 1 } }, \u0026#34;if\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;enabled\u0026#34;: { \u0026#34;const\u0026#34;: true } } }, \u0026#34;then\u0026#34;: { \u0026#34;required\u0026#34;: [\u0026#34;minReplicas\u0026#34;, \u0026#34;maxReplicas\u0026#34;] } } } }Test schema validation:\n# This should fail helm install my-app charts/my-app \\ --set replicaCount=200 \\ --dry-run # Error: values don\u0026#39;t meet the specifications of the schema Phase 2: Integration Tests (Testing Against Real Kubernetes)#\rTool: ct (Chart Testing)#\rInstall:\n# Install chart-testing brew install chart-testing # macOS # or pip install yamllint yamale # Install kind for local k8s cluster brew install kindSetup#\r# Create ct config cat \u0026gt; ct.yaml \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; chart-dirs: - charts chart-repos: - bitnami=https://charts.bitnami.com/bitnami helm-extra-args: --timeout 600s validate-maintainers: false EOFCreate test values:\n# charts/my-app/ci/default-values.yaml # Minimal viable deployment replicaCount: 1 image: repository: nginx tag: \u0026#34;1.21\u0026#34;# charts/my-app/ci/production-like-values.yaml # Production-like configuration replicaCount: 3 autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 ingress: enabled: true hosts: - host: test.example.com paths: - path: / pathType: PrefixRun integration tests:\n# Create local k8s cluster kind create cluster --name chart-testing # Install and test the chart ct install --charts charts/my-app # Output: # Installing chart \u0026#39;my-app\u0026#39; with values files: # - charts/my-app/ci/default-values.yaml # - charts/my-app/ci/production-like-values.yaml # ‚úì Chart installed successfully # ‚úì All pods became ready Custom Integration Tests with YAML#\r# charts/my-app/ci/test-values.yaml # This will actually deploy and run tests image: repository: nginx tag: \u0026#34;1.21\u0026#34; # Enable test job tests: enabled: trueCreate test pod template:\n# templates/tests/test-connection.yaml apiVersion: v1 kind: Pod metadata: name: \u0026#34;{{ include \u0026#34;my-app.fullname\u0026#34; . }}-test-connection\u0026#34; annotations: \u0026#34;helm.sh/hook\u0026#34;: test \u0026#34;helm.sh/hook-delete-policy\u0026#34;: before-hook-creation,hook-succeeded spec: containers: - name: wget image: busybox command: [\u0026#39;wget\u0026#39;] args: [\u0026#39;{{ include \u0026#34;my-app.fullname\u0026#34; . }}:{{ .Values.service.port }}\u0026#39;] restartPolicy: NeverRun the test:\nhelm install my-app charts/my-app helm test my-app # Output: # NAME: my-app # LAST DEPLOYED: ... # NAMESPACE: default # STATUS: deployed # TEST SUITE: my-app-test-connection # Last Started: ... # Last Completed: ... # Phase: Succeeded Phase 3: Advanced Testing Patterns#\rTesting with Multiple Kubernetes Versions#\r# .github/workflows/chart-test.yaml name: Lint and Test Charts on: pull_request jobs: test: runs-on: ubuntu-latest strategy: matrix: k8s-version: - v1.24.0 - v1.25.0 - v1.26.0 - v1.27.0 steps: - uses: actions/checkout@v3 - name: Set up chart-testing uses: helm/chart-testing-action@v2 - name: Create kind cluster uses: helm/kind-action@v1 with: node_image: kindest/node:${{ matrix.k8s-version }} - name: Run chart-testing (install) run: ct install --charts charts/my-app Snapshot Testing#\rProblem: You want to detect ANY change in rendered output.\n# tests/snapshot_test.yaml suite: snapshot tests templates: - deployment.yaml - service.yaml - ingress.yaml tests: - it: should match snapshot for default values asserts: - matchSnapshot: {} - it: should match snapshot for production config values: - ../ci/production-like-values.yaml asserts: - matchSnapshot: {}First run creates snapshots:\nhelm unittest charts/my-app --update-snapshotFuture runs detect changes:\nhelm unittest charts/my-app # If output changed: # FAIL should match snapshot for default values # - asserts[0] `matchSnapshot` fail # Template:\tcharts/my-app/templates/deployment.yaml # Expected to match snapshot but got diff: # --- Expected # +++ Actual # @@ -15,7 +15,7 @@ # - replicas: 1 # + replicas: 3 Security \u0026amp; Policy Testing#\rUsing Polaris:\n# Install polaris helm repo add fairwinds-stable https://charts.fairwinds.com/stable helm install polaris fairwinds-stable/polaris --namespace polaris --create-namespace # Test your chart against best practices helm template my-app charts/my-app | polaris audit --format=pretty # Output: # Results for kind Deployment: # my-app # ‚úì hostIPCSet: Host IPC is not configured # ‚úì hostPIDSet: Host PID is not configured # ‚úó runAsNonRoot: Should not be allowed to run as root # ‚úó readOnlyRootFilesystem: Root filesystem should be read-onlyUsing Checkov:\n# Install checkov pip install checkov # Scan rendered templates helm template my-app charts/my-app \u0026gt; /tmp/my-app.yaml checkov -f /tmp/my-app.yaml --framework kubernetes # Output: # kubernetes scan results: # Passed checks: 15, Failed checks: 8, Skipped checks: 0 # # Check: CKV_K8S_8: \u0026#34;Liveness Probe Should Be Configured\u0026#34; # FAILED for resource: Deployment.default.my-app # File: /tmp/my-app.yaml:1-50 Quick Start: Test Your Chart in 30 Minutes#\rMinute 0-10: Basic Unit Tests#\r# Install helm unittest helm plugin install https://github.com/helm-unittest/helm-unittest # Create test directory mkdir -p charts/my-app/tests # Test the most critical template (usually deployment) cat \u0026gt; charts/my-app/tests/deployment_test.yaml \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; suite: deployment tests templates: - deployment.yaml tests: - it: should create a Deployment asserts: - isKind: of: Deployment - it: should use correct image set: image.repository: myapp image.tag: \u0026#34;1.0.0\u0026#34; asserts: - equal: path: spec.template.spec.containers[0].image value: myapp:1.0.0 - it: should set replica count set: replicaCount: 5 asserts: - equal: path: spec.replicas value: 5 EOF # Run tests helm unittest charts/my-appMinute 10-20: Test Conditionals#\rcat \u0026gt; charts/my-app/tests/conditionals_test.yaml \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; suite: conditional resource tests templates: - ingress.yaml tests: - it: should not create Ingress when disabled set: ingress.enabled: false asserts: - hasDocuments: count: 0 - it: should create Ingress when enabled set: ingress.enabled: true ingress.hosts[0].host: example.com asserts: - isKind: of: Ingress - contains: path: spec.rules content: host: example.com EOF helm unittest charts/my-appMinute 20-30: Integration Test Setup#\r# Create test values cat \u0026gt; charts/my-app/ci/test-values.yaml \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; replicaCount: 1 image: repository: nginx tag: \u0026#34;1.21\u0026#34; service: type: ClusterIP port: 80 EOF # Create kind cluster kind create cluster --name test # Install chart helm install test-release charts/my-app -f charts/my-app/ci/test-values.yaml # Verify deployment kubectl get all kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=my-app --timeout=60s # Success? You have a working integration test! # Cleanup kind delete cluster --name test Common Patterns to Test#\r1. Security Context#\rtests: - it: should run as non-root when securityContext is set set: securityContext.runAsNonRoot: true securityContext.runAsUser: 1000 asserts: - equal: path: spec.template.spec.securityContext.runAsNonRoot value: true - equal: path: spec.template.spec.securityContext.runAsUser value: 10002. Resource Limits#\rtests: - it: should fail if CPU limits are too low set: resources.limits.cpu: 10m asserts: - failedTemplate: errorMessage: \u0026#34;CPU limit too low, minimum is 100m\u0026#34;3. Volume Mounts#\rtests: - it: should mount config volume when configMap is provided set: configMap.enabled: true configMap.data: config.yaml: \u0026#34;test: data\u0026#34; asserts: - contains: path: spec.template.spec.volumes content: name: config configMap: name: RELEASE-NAME-my-app - contains: path: spec.template.spec.containers[0].volumeMounts content: name: config mountPath: /config CI/CD Integration#\rGitHub Actions#\r# .github/workflows/helm-test.yaml name: Helm Chart Testing on: pull_request: paths: - \u0026#39;charts/**\u0026#39; jobs: unit-test: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - name: Install Helm uses: azure/setup-helm@v3 - name: Install helm-unittest run: helm plugin install https://github.com/helm-unittest/helm-unittest - name: Run unit tests run: helm unittest charts/my-app - name: Upload test results if: always() uses: actions/upload-artifact@v3 with: name: test-results path: charts/my-app/tests/*.xml integration-test: runs-on: ubuntu-latest needs: unit-test steps: - uses: actions/checkout@v3 - name: Create kind cluster uses: helm/kind-action@v1 - name: Install chart-testing uses: helm/chart-testing-action@v2 - name: Run integration tests run: ct install --charts charts/my-app Where to Start with YOUR Chart#\rDay 1 Checklist:\n‚úÖ Install helm unittest ‚úÖ Create charts/my-app/tests/ directory ‚úÖ Write ONE test for your most critical template ‚úÖ Run it: helm unittest charts/my-app Week 1 Goal:\nTest all conditional logic ({{- if }} blocks) Test value overrides for critical settings Achieve \u0026gt;50% template coverage Week 2 Goal:\nAdd values schema validation Create 2-3 CI test value files Set up integration testing with kind Month 1 Goal:\nTests in CI/CD pipeline Snapshot tests for stability Security scanning integrated Start small. One test is infinitely better than zero tests. You got this! üöÄ\n"},{"id":55,"href":"/musings/docs/infrastructure/","title":"Infrastructure","section":"Documentation","content":"Infrastructure#\rBrowse all documents in the Infrastructure category.\n"},{"id":56,"href":"/musings/docs/misc/imperitive-in-declarative/","title":"Imperitive In Declarative","section":"Miscellaneous","content":"This is a great question about bridging declarative GitOps with imperative operational tasks. Here are several approaches to consider:\n1. Custom Kubernetes Operator#\rCreate a CRD like DatabaseRestore that represents the desired state:\napiVersion: ops.example.com/v1 kind: DatabaseRestore metadata: name: production-restore spec: backupTimestamp: \u0026#34;2024-12-10T00:00:00Z\u0026#34; database: postgres-prod preSteps: - disableIngress: true - scaleDeployments: 0 postSteps: - scaleDeployments: auto - enableIngress: truePros:\nMost \u0026ldquo;Kubernetes-native\u0026rdquo; approach True declarative state - ArgoCD sees it as just another resource Built-in reconciliation loop handles idempotency naturally Can watch for changes and react automatically Status conditions show operation progress Perfect integration with GitOps workflows Cons:\nRequires writing and maintaining Go code (or using kubebuilder/operator-sdk) More complex initial setup Need to handle error states, retries, and edge cases Learning curve for operator development 2. Argo Workflows + ApplicationSet#\rUse Argo Workflows with a custom resource that triggers workflow execution:\napiVersion: ops.example.com/v1 kind: RestoreIntent metadata: name: restore-dec-10 spec: backupDate: \u0026#34;2024-12-10\u0026#34; triggered: false # Workflow watches thisA controller or the workflow itself updates triggered: true after execution.\nPros:\nArgo Workflows provides powerful orchestration DAG-based workflow is easy to visualize and understand Can reuse existing workflow templates Good retry and error handling built-in Workflow history preserved Cons:\nNeed coordination between workflow and state tracking Requires additional component (Argo Workflows) Idempotency logic must be built into workflow steps State synchronization can be tricky 3. Job + ConfigMap/Annotation Pattern#\rUse a ConfigMap to track desired state and a CronJob or controller to reconcile:\napiVersion: v1 kind: ConfigMap metadata: name: restore-state data: desiredBackupTimestamp: \u0026#34;2024-12-10T00:00:00Z\u0026#34; lastAppliedTimestamp: \u0026#34;2024-12-01T00:00:00Z\u0026#34;A Job/CronJob compares these values and executes if different.\nPros:\nSimple to implement - just shell scripts in a Job No custom code required Easy to understand for operators Lightweight solution Cons:\nLess robust than operator pattern CronJob timing can be wasteful or delayed State management is manual No built-in reconciliation Harder to handle complex orchestration Race conditions possible 4. Argo CD Sync Hooks + PostSync Jobs#\rUse ArgoCD hooks with a resource that changes when restore is needed:\napiVersion: batch/v1 kind: Job metadata: name: restore-db-20241210 annotations: argocd.argoproj.io/hook: Sync argocd.argoproj.io/hook-delete-policy: BeforeHookCreation spec: template: spec: containers: - name: restore image: restore-tooling env: - name: BACKUP_DATE value: \u0026#34;2024-12-10\u0026#34;Pros:\nIntegrates directly with ArgoCD sync process No additional controllers needed Clear when operation runs (during sync) Hook ordering handles sequencing Cons:\nTightly coupled to ArgoCD sync cycle Can make syncs slow and fragile Not truly idempotent - reruns on every sync unless carefully designed Manual sync required to trigger Hook failures can block entire sync 5. Hybrid: Operator + Argo Workflows#\rCombine a lightweight operator that triggers Argo Workflows:\napiVersion: ops.example.com/v1 kind: DatabaseRestore metadata: name: restore-dec-10 spec: backupTimestamp: \u0026#34;2024-12-10T00:00:00Z\u0026#34; status: workflowName: restore-dec-10-abc123 phase: SucceededPros:\nSeparates state management (operator) from execution (workflow) Best of both worlds for complex operations Clean separation of concerns Workflow provides detailed execution tracking Cons:\nMost complex solution Two systems to maintain Requires both operator and Argo Workflows My Recommendation#\rFor your use case, I\u0026rsquo;d recommend Option 1 (Custom Operator) or Option 5 (Operator + Argo Workflows) depending on complexity:\nStart with Option 1 if your restore logic is straightforward Use Option 5 if you need complex multi-step orchestration with branching logic The operator pattern gives you true declarative state that ArgoCD can track, while the reconciliation loop naturally provides idempotency. The status subresource lets you track \u0026ldquo;this is what we wanted\u0026rdquo; vs \u0026ldquo;this is what we\u0026rsquo;ve done\u0026rdquo;, which is exactly what you\u0026rsquo;re looking for.\nWould you like me to sketch out a more detailed example of any of these approaches?\n"},{"id":57,"href":"/musings/docs/workflows/","title":"Workflows","section":"Documentation","content":"Workflows#\rBrowse all documents in the Workflows category.\n"},{"id":58,"href":"/musings/docs/misc/","title":"Miscellaneous","section":"Documentation","content":"Miscellaneous#\rBrowse all documents in the Miscellaneous category.\n"},{"id":59,"href":"/musings/docs/mermaid-test/","title":"Mermaid Diagram Test","section":"Documentation","content":"Mermaid Diagram Test#\rThis document tests various Mermaid diagram types to ensure they render correctly.\nFlowchart#\rgraph TD\rA[Start] --\u0026gt; B{Decision}\rB --\u0026gt;|Yes| C[Action 1]\rB --\u0026gt;|No| D[Action 2]\rC --\u0026gt; E[End]\rD --\u0026gt; ESequence Diagram#\rsequenceDiagram\rparticipant User\rparticipant Hugo\rparticipant Theme\rUser-\u0026gt;\u0026gt;Hugo: Build Site\rHugo-\u0026gt;\u0026gt;Theme: Apply Books Theme\rTheme-\u0026gt;\u0026gt;Hugo: Rendered HTML\rHugo-\u0026gt;\u0026gt;User: Static SiteClass Diagram#\rclassDiagram\rclass Document {\r+String title\r+String summary\r+String[] keywords\r+String category\r+render()\r}\rclass Site {\r+Document[] documents\r+build()\r+serve()\r}\rSite --\u0026gt; Document : containsIf all three diagrams above render as interactive SVG graphics instead of code blocks, then Mermaid is working correctly!\n"},{"id":60,"href":"/musings/docs/test-document/","title":"Test Document","section":"Documentation","content":"Test Document#\rThis is a test document to verify that Hugo and the Books theme are working correctly.\nFeatures to Test#\rTable of Contents#\rThis document should show a table of contents on the right side with the following sections:\nFeatures to Test Table of Contents Mermaid Diagrams Code Blocks Lists and Formatting Mermaid Diagrams#\rHere\u0026rsquo;s a test Mermaid diagram:\ngraph TD\rA[Hugo Site] --\u0026gt; B[Books Theme]\rB --\u0026gt; C[Test Document]\rC --\u0026gt; D[Rendered Page]\rD --\u0026gt; E[Success!]Code Blocks#\rHere\u0026rsquo;s a code block to test syntax highlighting:\nfunction testHugo() { console.log(\u0026#34;Hugo and Books theme are working!\u0026#34;); return true; }Lists and Formatting#\rBold text and italic text should render correctly.\nNumbered list item 1 Numbered list item 2 Numbered list item 3 Bullet point 1 Bullet point 2 Bullet point 3 This is a blockquote to test formatting.\nIf you can see this document rendered properly with:\nTable of contents on the right Proper navigation in the sidebar Formatted text and code blocks The Mermaid diagram (if configured) Then Hugo and the Books theme are working correctly!\n"}]